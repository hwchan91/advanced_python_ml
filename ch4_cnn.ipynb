{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config theano to use GPU, must be done before theano is imported\n",
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda0,floatX=float32\"#,optimizer=None,exception_verbosity=high\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://deeplearning.net/tutorial/lenet.html\n",
    "    \n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "\n",
    "class LeNetConvPoolLayer():\n",
    "    def __init__(self, \n",
    "        rng, \n",
    "        input, # with 4 dimensions: rows of input(i.e. batch) * num of input maps per input * height per map * width per map; see: https://www.quora.com/Why-are-there-4-dimensions-to-convolve-over-the-Stanford-UFDL-example-in-convolutional-neural-networks\n",
    "        image_shape, # tuple/list of len 4 representing the 4 dims of input: (batch size, num input feature maps, image height, image width)\n",
    "        filter_shape, # tuple/list of len 4 representing: (number of filters, num input feature maps, filter height, filter width)\n",
    "        poolsize=(2, 2), # eg. downsample every 2x2 bits to 1 bit\n",
    "        activation=T.nnet.relu,\n",
    "        border_mode='valid', # no padding; see doc for more options\n",
    "        subsample=(1, 1) # unit stride        \n",
    "    ):\n",
    "        assert image_shape[1] == filter_shape[1] # num input feataure maps should be same for both arrs\n",
    "        \n",
    "        self.input = input\n",
    "        fan_in = numpy.prod(filter_shape[1:]) # (num input feature maps * filter height * filter width) input nodes to each hidden node/unit(?) \n",
    "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) / numpy.prod(poolsize)) # (num output feature maps * filter height * filter width /  pooling size) output nodes per layer\n",
    "        \n",
    "        # initialize weights with random weights\n",
    "        W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
    "        self.W = theano.shared(\n",
    "            numpy.asarray(\n",
    "                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX) # a 1D tensor(i.e. array) of len(num of filters) for adding a bias per output feature map\n",
    "        self.b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "        # convolve input feature maps with filters\n",
    "        # doc: http://deeplearning.net/software/theano/library/tensor/nnet/conv.html#theano.tensor.nnet.conv2d\n",
    "        # simply explained; conv2d iterates over every sample, apply the SAME weights on each 'cropped' input(map) to create each output map\n",
    "        # taking an analogy with MLP, each output feature map can be considered as one hidden node in an MLP, where each value in the map represents a different cropped position of the input image\n",
    "        # (programmatically, I believe it flattens the num of maps * input height * input width for every sample; apply the transformtion; and then reshape it back to the expected output dimensions)\n",
    "        conv_out = conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            input_shape=image_shape,\n",
    "            border_mode=border_mode,\n",
    "            subsample=subsample\n",
    "        )\n",
    "\n",
    "        # pool each feature map individually, using maxpooling\n",
    "        # doc: http://deeplearning.net/software/theano/library/tensor/signal/pool.html\n",
    "        # note: 'Max pooling will be done over the 2 last dimensions', i.e. height * width\n",
    "        pooled_out = pool.pool_2d(\n",
    "            input=conv_out,\n",
    "            ws=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "        # add the bias term. Since the bias is a vector (1D array), we reshape it to a tensor of shape (1, n_filters, 1, 1). \n",
    "        # since every output map uses one shared weight, only 1 bias is needed for each output map\n",
    "        self.output = activation(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x')) # explanation of dimshuffle: https://stackoverflow.com/questions/42401420/how-the-function-dimshuffle-works-in-theano\n",
    "\n",
    "        # shared_params\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        rng, \n",
    "        input, \n",
    "        n_in, \n",
    "        n_out, \n",
    "        p_dropout=0.0, \n",
    "        W=None, \n",
    "        b=None, \n",
    "        activation=T.tanh # if set actication=T.nnet.sigmoid, becomes logistic regresssion layer\n",
    "    ): \n",
    "        self.input = input\n",
    "        self.theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "        \n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        dropout_input = self.get_corrupted_input(input, p_dropout)\n",
    "        lin_output = T.dot(dropout_input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    # add noise by setting corruption_level% of data to 0s\n",
    "    def get_corrupted_input(self, input, corruption_level):   \n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.input = input\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # predict_proba\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(self.input, self.W) + self.b) # softmax=normalized sigmoid\n",
    "        # predict\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "    # this is akin to cost = -1/m * sigma(ylog(wx) + (1-y)log(1-wx)) when y is binomial\n",
    "    # in the current case y has n-labels, and only the prediction of the right label is picked out\n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    # perc of wrong predictions\n",
    "    def errors(self, y):\n",
    "        return T.mean(T.neq(self.y_pred, y)) # T.neq(a,b) checks a != b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: output size calculated by formula:\n",
    "# (Width - Filter_size + 2*Padding / Stride) + 1\n",
    "# stride is usually 1 in implementations such as conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "\n",
    "class CNN():\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng=numpy.random.RandomState(1234),\n",
    "        n_in_maps=3, \n",
    "        in_width=32,\n",
    "        in_height=32,\n",
    "        n_out=10,\n",
    "        conv_layer_sizes=[40,80],\n",
    "        conv_filter_sizes=[(5, 5), (5, 5)],\n",
    "        conv_padding_sizes=[(0, 0), (0, 0)],\n",
    "        conv_subsample_sizes=[(1, 1), (1, 1)],\n",
    "        conv_pooling_sizes=[(2, 2), (2, 2)],\n",
    "        fully_connected_layer_sizes=[2000,2000],\n",
    "        dropout_levels=[0.1,0.1],\n",
    "        batch_size=100,\n",
    "        lmbda=0.1\n",
    "    ):        \n",
    "        # init var\n",
    "        self.rng = numpy_rng or numpy.random.RandomState(23455)\n",
    "        self.x = T.matrix('x')\n",
    "        self.y = T.ivector('y')\n",
    "        self.index = T.lscalar('index')\n",
    "        self.x_batch = self.x[self.index * batch_size: (self.index + 1) * batch_size]\n",
    "        self.y_batch = self.y[self.index * batch_size: (self.index + 1) * batch_size]\n",
    "        self.conv_layers = []\n",
    "        self.fully_connected_layers = []\n",
    "        self.params = [] # holds the shared/updatable vars\n",
    "        weights = [] # just store the weights for calc regularization\n",
    "        \n",
    "        input_shape = (batch_size, n_in_maps, in_width, in_height)\n",
    "        layer0_input = self.x_batch.reshape(input_shape) # cifar data comes in RGB(3) channels in 32x32 size\n",
    "\n",
    "        # conv layers\n",
    "        out_dims = [] # store (width, height) of each conv layers output\n",
    "        for i in range(len(conv_layer_sizes)):\n",
    "            out_n_maps = conv_layer_sizes[i]\n",
    "            filter_size = conv_filter_sizes[i]\n",
    "            border_mode = conv_padding_sizes[i]\n",
    "            subsample = conv_subsample_sizes[i]\n",
    "            poolsize = conv_pooling_sizes[i]\n",
    "            \n",
    "            if i == 0:\n",
    "                layer_input = layer0_input\n",
    "            else:\n",
    "                # the input is the prev layer\n",
    "                n_in_maps, in_width, in_height = out_dims[i-1]             \n",
    "                layer_input = self.conv_layers[i-1].output # output is the final activation\n",
    "            \n",
    "            out_width  = ((in_width - filter_size[0] + 2 * border_mode[0]) / subsample[0] + 1 ) / poolsize[0]\n",
    "            out_height = ((in_height - filter_size[1] + 2 * border_mode[1]) / subsample[1] + 1) / poolsize[1]\n",
    "            out_dims.append((out_n_maps, out_width, out_height))\n",
    "\n",
    "            conv_layer = LeNetConvPoolLayer(\n",
    "                self.rng,\n",
    "                input=layer_input,\n",
    "                image_shape=(batch_size, n_in_maps, in_width, in_height), # apparently, theano conv2d accepts a defined list/tuple as arg only, therefore this arg cannot be simplified in to input.shape (which would be a tensor vector)\n",
    "                filter_shape=(out_n_maps, n_in_maps, filter_size[0], filter_size[1]),\n",
    "                poolsize=poolsize,\n",
    "                border_mode=border_mode,\n",
    "                subsample=subsample\n",
    "            )\n",
    "            weights.append(conv_layer.W)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            self.params.extend(conv_layer.params)\n",
    "        \n",
    "        final_conv_output = self.conv_layers[-1].output.flatten(2)\n",
    "        final_conv_output_features = int(numpy.prod(out_dims[-1]))\n",
    "        \n",
    "        # fully connected layers\n",
    "        for i in range(len(fully_connected_layer_sizes)):\n",
    "            if i == 0:\n",
    "                fully_connected_layer_input = self.conv_layers[-1].output.flatten(2)\n",
    "                fully_connected_layer_n_in = int(numpy.prod(out_dims[-1]))\n",
    "            else:\n",
    "                fully_connected_layer_input = self.fully_connected_layers[i-1].output\n",
    "                fully_connected_layer_n_in = fully_connected_layer_sizes[i-1]\n",
    "\n",
    "            hidden_layer = HiddenLayer(\n",
    "                self.rng,\n",
    "                input=fully_connected_layer_input,\n",
    "                n_in=fully_connected_layer_n_in,\n",
    "                n_out=fully_connected_layer_sizes[i],\n",
    "                p_dropout=dropout_levels[i],\n",
    "            )\n",
    "            weights.append(hidden_layer.W)\n",
    "            self.fully_connected_layers.append(hidden_layer)\n",
    "            self.params.extend(hidden_layer.params)\n",
    "\n",
    "        # sigmoidal LR output layer\n",
    "        log_layer = LogisticRegression(\n",
    "            input=self.fully_connected_layers[-1].output, \n",
    "            n_in=fully_connected_layer_sizes[-1], \n",
    "            n_out=n_out\n",
    "        )\n",
    "        weights.append(log_layer.W)\n",
    "        self.params.extend(log_layer.params)\n",
    "    \n",
    "        # L2 regularization\n",
    "        l2_norm_squared = sum([(weight**2).sum() for weight in weights])\n",
    "        self.cost = log_layer.negative_log_likelihood(self.y_batch) +  0.5 * lmbda * l2_norm_squared/batch_size\n",
    "        self.errors = log_layer.errors(self.y_batch)\n",
    "\n",
    "    def build_train_fn(self, train_x, train_y, learning_rate):\n",
    "        # create a list of gradients for all model parameters\n",
    "        grads = T.grad(self.cost, self.params)\n",
    "        \n",
    "        updates = OrderedDict()\n",
    "        for grad, param in zip(grads, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - grad * T.cast(learning_rate, dtype=theano.config.floatX)\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[self.index],\n",
    "            outputs=self.cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_x,\n",
    "                self.y: train_y\n",
    "            }\n",
    "        )\n",
    "        return train_fn\n",
    "\n",
    "    def get_errors(self, test_x, test_y, batch_size):  \n",
    "        n_batches = int(test_x.shape[0] / batch_size)\n",
    "        get_batch_error = theano.function(\n",
    "            inputs=[self.index],\n",
    "            outputs = self.errors, # perc of wrong preds\n",
    "            givens={\n",
    "                self.x: test_x,\n",
    "                self.y: test_y\n",
    "            }\n",
    "        )\n",
    "\n",
    "        def score_func():\n",
    "            return [get_batch_error(i) for i in range(n_batches)]\n",
    "\n",
    "        return score_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_train():\n",
    "    X = numpy.empty((0,3072), dtype=theano.config.floatX)\n",
    "    y = []\n",
    "    for i in range(5):\n",
    "        i += 1\n",
    "        unpacked = unpickle(f'cifar-10-batches-py/data_batch_{i}')\n",
    "        X = numpy.concatenate((X, unpacked[b'data']))\n",
    "        y = numpy.concatenate((y, unpacked[b'labels']))\n",
    "    return X,y.astype('int32')\n",
    "\n",
    "def load_cifar10_test():\n",
    "    unpacked = unpickle(f'cifar-10-batches-py/test_batch')\n",
    "    return numpy.array(unpacked[b'data'], dtype=theano.config.floatX), numpy.array(unpacked[b'labels'], dtype='int32')\n",
    "\n",
    "X, y = load_cifar10_train()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set_x, valid_set_x, train_set_y, valid_set_y = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "test_set_x, test_set_y = load_cifar10_test()\n",
    "\n",
    "# for test\n",
    "train_set_x = train_set_x[0:10000,:]\n",
    "valid_set_x = valid_set_x[0:1000,:]\n",
    "test_set_x = test_set_x[0:1000,:]\n",
    "\n",
    "train_set_y = train_set_y[0:10000]\n",
    "valid_set_y = valid_set_y[0:1000]\n",
    "test_set_y = test_set_y[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "cnn = CNN(conv_layer_sizes=[64,128], fully_connected_layer_sizes=[2048,2048], batch_size = batch_size)\n",
    "train_fn = cnn.build_train_fn(train_set_x, train_set_y, 0.03)\n",
    "get_validate_errors = cnn.get_errors(valid_set_x, valid_set_y, batch_size)\n",
    "get_test_errors = cnn.get_errors(test_set_x, test_set_y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def train_cnn(training_epochs=1000):\n",
    "    print('... training the model')\n",
    "    n_train_batches = int(train_set_x.shape[0] / batch_size)\n",
    "    \n",
    "    # early-stopping parameters\n",
    "    patience = 10000 # look as this many examples regardless\n",
    "    patience_increase = 2. # loop for n times more when a new best is found\n",
    "    improvement_threshold = 0.995 # a relative improvement of this much is considered significant\n",
    "\n",
    "    # go through this many minibatches before checking the network on\n",
    "    # the validation set; in this case we check every epoch\n",
    "    validation_frequency = min(n_train_batches, patience / 2) # = n_train_batches\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            train_fn(minibatch_index)\n",
    "            iter = epoch * n_train_batches + minibatch_index\n",
    "\n",
    "            # for every 'validation_frequency' iters\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = get_validate_errors()\n",
    "                curr_mean_validation_loss = numpy.mean(validation_losses, dtype='float64')\n",
    "                print(f'epoch {epoch}, minibatch {minibatch_index + 1}/{n_train_batches}, validation error {curr_mean_validation_loss * 100.}%')\n",
    "\n",
    "                # if we got the least validation errors until now\n",
    "                if curr_mean_validation_loss < best_validation_loss:\n",
    "                    # improve patience if loss improvement is good enough; which will allow more training = double of the curr loop count\n",
    "                    if (curr_mean_validation_loss < best_validation_loss * improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = curr_mean_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = get_test_errors()\n",
    "                    test_score = numpy.mean(test_losses, dtype='float64')\n",
    "                    print(f'     epoch {epoch}, minibatch minibatch {minibatch_index + 1}/{n_train_batches}, test error of best model {test_score * 100.}%')\n",
    "\n",
    "            # if no improvement in validation score for the last 50% iters\n",
    "            if patience <= iter:\n",
    "                return best_validation_loss, best_iter, test_score\n",
    "    return best_validation_loss, best_iter, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training the model\n",
      "epoch 0, minibatch 200/200, validation error 75.6%\n",
      "     epoch 0, minibatch minibatch 200/200, test error of best model 73.70000000000002%\n",
      "epoch 1, minibatch 200/200, validation error 72.7%\n",
      "     epoch 1, minibatch minibatch 200/200, test error of best model 73.10000000000001%\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "best_validation_loss, best_iter, test_score = train_cnn()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f'training time: {end_time - start_time}s.')\n",
    "print(f'Optimization complete with best validation score of {best_validation_loss * 100.}%,\\n'\n",
    "    f'obtained at iteration {best_iter + 1},\\n'\n",
    "    f'with test performance {test_score * 100.}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST dataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data(dataset):\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "datasets = load_data('mnist.pkl.gz')\n",
    "# datasets = load_data('mnist_expanded.pkl.gz')\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x,  test_set_y  = datasets[2]\n",
    "\n",
    "train_set_x = np.array(train_set_x, dtype=theano.config.floatX)\n",
    "valid_set_x = np.array(valid_set_x, dtype=theano.config.floatX)\n",
    "test_set_x = np.array(test_set_x, dtype=theano.config.floatX)\n",
    "\n",
    "train_set_y = np.array(train_set_y).astype('int32')\n",
    "valid_set_y = np.array(valid_set_y).astype('int32')\n",
    "test_set_y = np.array(test_set_y).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(\n",
    "    n_in_maps=1, \n",
    "    in_width=28,\n",
    "    in_height=28,\n",
    "    n_out=10,\n",
    "    conv_layer_sizes=[20,40],\n",
    "    fully_connected_layer_sizes=[100,100],\n",
    "    dropout_levels=[0.3,0.3]\n",
    ")\n",
    "train_fn = cnn.build_train_fn(train_set_x, train_set_y, 0.1)\n",
    "get_validate_errors = cnn.get_errors(valid_set_x, valid_set_y, 500)\n",
    "get_test_errors = cnn.get_errors(test_set_x, test_set_y, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training the model\n",
      "epoch 0, minibatch 100/100, validation error 9.590000000000002%\n",
      "     epoch 0, minibatch minibatch 100/100, test error of best model 9.82%\n",
      "epoch 1, minibatch 100/100, validation error 5.510000000000001%\n",
      "     epoch 1, minibatch minibatch 100/100, test error of best model 5.49%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-275dfe54a245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest_validation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'training time: {end_time - start_time}s.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-6bcd5a9db452>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[0;34m(training_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "best_validation_loss, best_iter, test_score = train_cnn()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f'training time: {end_time - start_time}s.')\n",
    "print(f'Optimization complete with best validation score of {best_validation_loss * 100.}%,\\n'\n",
    "    f'obtained at iteration {best_iter + 1},\\n'\n",
    "    f'with test performance {test_score * 100.}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding the MNIST training set\n",
      "Expanding image number 1000\n",
      "Expanding image number 2000\n",
      "Expanding image number 3000\n",
      "Expanding image number 4000\n",
      "Expanding image number 5000\n",
      "Expanding image number 6000\n",
      "Expanding image number 7000\n",
      "Expanding image number 8000\n",
      "Expanding image number 9000\n",
      "Expanding image number 10000\n",
      "Expanding image number 11000\n",
      "Expanding image number 12000\n",
      "Expanding image number 13000\n",
      "Expanding image number 14000\n",
      "Expanding image number 15000\n",
      "Expanding image number 16000\n",
      "Expanding image number 17000\n",
      "Expanding image number 18000\n",
      "Expanding image number 19000\n",
      "Expanding image number 20000\n",
      "Expanding image number 21000\n",
      "Expanding image number 22000\n",
      "Expanding image number 23000\n",
      "Expanding image number 24000\n",
      "Expanding image number 25000\n",
      "Expanding image number 26000\n",
      "Expanding image number 27000\n",
      "Expanding image number 28000\n",
      "Expanding image number 29000\n",
      "Expanding image number 30000\n",
      "Expanding image number 31000\n",
      "Expanding image number 32000\n",
      "Expanding image number 33000\n",
      "Expanding image number 34000\n",
      "Expanding image number 35000\n",
      "Expanding image number 36000\n",
      "Expanding image number 37000\n",
      "Expanding image number 38000\n",
      "Expanding image number 39000\n",
      "Expanding image number 40000\n",
      "Expanding image number 41000\n",
      "Expanding image number 42000\n",
      "Expanding image number 43000\n",
      "Expanding image number 44000\n",
      "Expanding image number 45000\n",
      "Expanding image number 46000\n",
      "Expanding image number 47000\n",
      "Expanding image number 48000\n",
      "Expanding image number 49000\n",
      "Expanding image number 50000\n",
      "Saving expanded data. This may take a few minutes.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Take the 50,000 MNIST training images, and create an expanded set of\n",
    "250,000 images, by displacing each training image up, down, left and\n",
    "right, by one pixel.  Save the resulting file to\n",
    "../data/mnist_expanded.pkl.gz.\n",
    "Note that this program is memory intensive, and may not run on small\n",
    "systems.\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import gzip\n",
    "import os.path\n",
    "import random\n",
    "import numpy as np\n",
    "    \n",
    "print(\"Expanding the MNIST training set\")\n",
    "\n",
    "if os.path.exists(\"mnist_expanded.pkl.gz\"):\n",
    "    print(\"The expanded training set already exists.  Exiting.\")\n",
    "else:\n",
    "    with gzip.open(\"mnist.pkl.gz\", 'rb') as f:\n",
    "        u = pickle._Unpickler(f)\n",
    "        u.encoding = 'latin1'\n",
    "        training_data, validation_data, test_data = u.load()\n",
    "\n",
    "    expanded_training_pairs = []\n",
    "    j = 0 # counter\n",
    "    for x, y in zip(training_data[0], training_data[1]):\n",
    "        expanded_training_pairs.append((x, y))\n",
    "        image = np.reshape(x, (-1, 28))\n",
    "        j += 1\n",
    "        if j % 1000 == 0: print(\"Expanding image number\", j)\n",
    "        # iterate over data telling us the details of how to\n",
    "        # do the displacement\n",
    "        for d, axis, index_position, index in [\n",
    "                (1,  0, \"first\", 0),\n",
    "                (-1, 0, \"first\", 27),\n",
    "                (1,  1, \"last\",  0),\n",
    "                (-1, 1, \"last\",  27)]:\n",
    "            new_img = np.roll(image, d, axis)\n",
    "            if index_position == \"first\": \n",
    "                new_img[index, :] = np.zeros(28)\n",
    "            else: \n",
    "                new_img[:, index] = np.zeros(28)\n",
    "            expanded_training_pairs.append((np.reshape(new_img, 784), y))\n",
    "    random.shuffle(expanded_training_pairs)\n",
    "    expanded_training_data = [list(d) for d in zip(*expanded_training_pairs)]\n",
    "    print(\"Saving expanded data. This may take a few minutes.\")\n",
    "    f = gzip.open(\"mnist_expanded.pkl.gz\", \"w\")\n",
    "    pickle.dump((expanded_training_data, validation_data, test_data), f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 784)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_set_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
