{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config theano to use GPU, must be done before theano is imported\n",
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=cpu,floatX=float32,optimizer=None,exception_verbosity=high\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "\n",
    "class dA:\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng=numpy.random.RandomState(1234),\n",
    "        theano_rng=RandomStreams(numpy.random.RandomState(1234).randint(2 ** 30)), # to generate random numbers in theano, a RandomStream need to initialized with a numpy rng\n",
    "        input=T.dmatrix(name='input'),\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        bhid=None,\n",
    "        bvis=None\n",
    "    ):\n",
    "        self.x = input\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        self.numpy_rng = numpy_rng \n",
    "        self.theano_rng = theano_rng\n",
    "        \n",
    "        self.W = W or self.initial_W(rng=self.numpy_rng, n_hidden=n_hidden, n_visible=n_visible) # weights of visible layer to hidden layer\n",
    "        self.W_prime = self.W.T # weights of hidden layer to recontruction/visible layer = shared weights\n",
    "        self.b = bhid or self.bias_obj(n=n_hidden, name='b') # bias of hidden layer\n",
    "        self.b_prime = bvis or self.bias_obj(n=n_visible, name='b_prime') # bias of reconstruction/visible layer\n",
    "        \n",
    "        # shared variables\n",
    "        self.params = [self.W, self.b, self.b_prime]\n",
    "    \n",
    "    def initial_W(self, rng=None, n_hidden=None, n_visible=None):\n",
    "        W = numpy.asarray(\n",
    "            rng.uniform(\n",
    "                low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                size=(n_visible, n_hidden)\n",
    "            ),\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "        return theano.shared(\n",
    "            value= W, \n",
    "            name='W', \n",
    "            borrow=True\n",
    "        )\n",
    "    \n",
    "    def bias_obj(self, n=None, name=None):\n",
    "        return theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                n,\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name=name,\n",
    "            borrow=True\n",
    "        )\n",
    "    \n",
    "    # add noise by setting corruption_level% of data to 0s\n",
    "    def get_corrupted_input(self, input, corruption_level):   \n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input\n",
    "    \n",
    "    def get_hidden_values(self, input):\n",
    "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
    "    \n",
    "    def get_reconstructed_input(self, hidden):\n",
    "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "    \n",
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "        y = self.get_hidden_values(tilde_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        # it was originally: L = T.mean((0.5 * (z â€“ self.x)) ** 2), ie. 1/2 ||z-x)||^2\n",
    "        # now it's the similar to logistic regression, i.e. cross-entropy ( suitable when the input are bit vectors (either 1 or 0))\n",
    "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1) \n",
    "        cost = T.mean(L)\n",
    "\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = OrderedDict()\n",
    "        for gparam, param in zip(gparams, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - gparam * T.cast(learning_rate, dtype=theano.config.floatX)\n",
    "\n",
    "        return (cost, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to plot hidden layer\n",
    "\n",
    "import numpy\n",
    "\n",
    "\n",
    "def scale_to_unit_interval(ndar, eps=1e-8):\n",
    "    \"\"\" Scales all values in the ndarray ndar to be between 0 and 1 \"\"\"\n",
    "    ndar = ndar.copy()\n",
    "    ndar -= ndar.min()\n",
    "    ndar *= 1.0 / (ndar.max() + eps)\n",
    "    return ndar\n",
    "\n",
    "\n",
    "def tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0),\n",
    "                       scale_rows_to_unit_interval=True,\n",
    "                       output_pixel_vals=True):\n",
    "    \"\"\"\n",
    "    Transform an array with one flattened image per row, into an array in\n",
    "    which images are reshaped and layed out like tiles on a floor.\n",
    "\n",
    "    This function is useful for visualizing datasets whose rows are images,\n",
    "    and also columns of matrices for transforming those rows\n",
    "    (such as the first layer of a neural net).\n",
    "\n",
    "    :type X: a 2-D ndarray or a tuple of 4 channels, elements of which can\n",
    "    be 2-D ndarrays or None;\n",
    "    :param X: a 2-D array in which every row is a flattened image.\n",
    "\n",
    "    :type img_shape: tuple; (height, width)\n",
    "    :param img_shape: the original shape of each image\n",
    "\n",
    "    :type tile_shape: tuple; (rows, cols)\n",
    "    :param tile_shape: the number of images to tile (rows, cols)\n",
    "\n",
    "    :param output_pixel_vals: if output should be pixel values (i.e. int8\n",
    "    values) or floats\n",
    "\n",
    "    :param scale_rows_to_unit_interval: if the values need to be scaled before\n",
    "    being plotted to [0,1] or not\n",
    "\n",
    "\n",
    "    :returns: array suitable for viewing as an image.\n",
    "    (See:`Image.fromarray`.)\n",
    "    :rtype: a 2-d array with same dtype as X.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(img_shape) == 2\n",
    "    assert len(tile_shape) == 2\n",
    "    assert len(tile_spacing) == 2\n",
    "\n",
    "    # The expression below can be re-written in a more C style as\n",
    "    # follows :\n",
    "    #\n",
    "    # out_shape    = [0,0]\n",
    "    # out_shape[0] = (img_shape[0]+tile_spacing[0])*tile_shape[0] -\n",
    "    #                tile_spacing[0]\n",
    "    # out_shape[1] = (img_shape[1]+tile_spacing[1])*tile_shape[1] -\n",
    "    #                tile_spacing[1]\n",
    "    out_shape = [\n",
    "        (ishp + tsp) * tshp - tsp\n",
    "        for ishp, tshp, tsp in zip(img_shape, tile_shape, tile_spacing)\n",
    "    ]\n",
    "\n",
    "    if isinstance(X, tuple):\n",
    "        assert len(X) == 4\n",
    "        # Create an output numpy ndarray to store the image\n",
    "        if output_pixel_vals:\n",
    "            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n",
    "                                    dtype='uint8')\n",
    "        else:\n",
    "            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n",
    "                                    dtype=X.dtype)\n",
    "\n",
    "        #colors default to 0, alpha defaults to 1 (opaque)\n",
    "        if output_pixel_vals:\n",
    "            channel_defaults = [0, 0, 0, 255]\n",
    "        else:\n",
    "            channel_defaults = [0., 0., 0., 1.]\n",
    "\n",
    "        for i in range(4):\n",
    "            if X[i] is None:\n",
    "                # if channel is None, fill it with zeros of the correct\n",
    "                # dtype\n",
    "                dt = out_array.dtype\n",
    "                if output_pixel_vals:\n",
    "                    dt = 'uint8'\n",
    "                out_array[:, :, i] = numpy.zeros(\n",
    "                    out_shape,\n",
    "                    dtype=dt\n",
    "                ) + channel_defaults[i]\n",
    "            else:\n",
    "                # use a recurrent call to compute the channel and store it\n",
    "                # in the output\n",
    "                out_array[:, :, i] = tile_raster_images(\n",
    "                    X[i], img_shape, tile_shape, tile_spacing,\n",
    "                    scale_rows_to_unit_interval, output_pixel_vals)\n",
    "        return out_array\n",
    "\n",
    "    else:\n",
    "        # if we are dealing with only one channel\n",
    "        H, W = img_shape\n",
    "        Hs, Ws = tile_spacing\n",
    "\n",
    "        # generate a matrix to store the output\n",
    "        dt = X.dtype\n",
    "        if output_pixel_vals:\n",
    "            dt = 'uint8'\n",
    "        out_array = numpy.zeros(out_shape, dtype=dt)\n",
    "\n",
    "        for tile_row in range(tile_shape[0]):\n",
    "            for tile_col in range(tile_shape[1]):\n",
    "                if tile_row * tile_shape[1] + tile_col < X.shape[0]:\n",
    "                    this_x = X[tile_row * tile_shape[1] + tile_col]\n",
    "                    if scale_rows_to_unit_interval:\n",
    "                        # if we should scale values to be between 0 and 1\n",
    "                        # do this by calling the `scale_to_unit_interval`\n",
    "                        # function\n",
    "                        this_img = scale_to_unit_interval(\n",
    "                            this_x.reshape(img_shape))\n",
    "                    else:\n",
    "                        this_img = this_x.reshape(img_shape)\n",
    "                    # add the slice to the corresponding position in the\n",
    "                    # output array\n",
    "                    c = 1\n",
    "                    if output_pixel_vals:\n",
    "                        c = 255\n",
    "                    out_array[\n",
    "                        tile_row * (H + Hs): tile_row * (H + Hs) + H,\n",
    "                        tile_col * (W + Ws): tile_col * (W + Ws) + W\n",
    "                    ] = this_img * c\n",
    "        return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data(dataset):\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "datasets = load_data('mnist.pkl.gz')\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x,  test_set_y  = datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "\n",
    "def test_da(\n",
    "    learning_rate=0.1,\n",
    "    training_epochs=15,\n",
    "    batch_size=20,\n",
    "    output_folder='dA_plots',\n",
    "    X = train_set_x,\n",
    "    corruption_level = 0.0\n",
    "):\n",
    "    index = T.lscalar()\n",
    "    train = T.matrix('x')\n",
    "    x = train[index * batch_size: (index + 1) * batch_size]\n",
    "\n",
    "    numpy_rng = numpy.random.RandomState(123)\n",
    "    theano_rng=RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "    # if not os.path.isdir(output_folder):\n",
    "    #     os.makedirs(output_folder)\n",
    "    # os.chdir(output_folder)\n",
    "\n",
    "    da = dA(\n",
    "        numpy_rng=numpy_rng,\n",
    "        theano_rng=theano_rng,\n",
    "        input=x,\n",
    "        n_visible=28 * 28,\n",
    "        n_hidden=500\n",
    "    )\n",
    "\n",
    "    cost, updates = da.get_cost_updates(\n",
    "        corruption_level=corruption_level,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_da = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            train: X\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # start training\n",
    "    plotting_time = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "    n_train_batches = int(X.shape[0] / batch_size)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        mean_cost = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            mean_cost.append(train_da(batch_index))\n",
    "        print('Training epoch %d, cost is ' % epoch, numpy.mean(mean_cost))\n",
    "    end_time = timeit.default_timer()\n",
    "    pretraining_time = (end_time - start_time) \n",
    "    print(f'The no corruption code training an for {pretraining_time / 60.} m')\n",
    "\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, cost is  63.28917\n",
      "Training epoch 1, cost is  55.786655\n",
      "Training epoch 2, cost is  54.76311\n",
      "Training epoch 3, cost is  54.24205\n",
      "Training epoch 4, cost is  53.888668\n",
      "Training epoch 5, cost is  53.62035\n",
      "Training epoch 6, cost is  53.40375\n",
      "Training epoch 7, cost is  53.221992\n",
      "Training epoch 8, cost is  53.0658\n",
      "Training epoch 9, cost is  52.92956\n",
      "Training epoch 10, cost is  52.809414\n",
      "Training epoch 11, cost is  52.70244\n",
      "Training epoch 12, cost is  52.60631\n",
      "Training epoch 13, cost is  52.51917\n",
      "Training epoch 14, cost is  52.439526\n",
      "The no corruption code training an for 1.8524673257833153 m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.dA at 0x1c567e7390>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da = test_da()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "image = Image.fromarray(\n",
    "    tile_raster_images(\n",
    "        X=da.W.get_value(borrow=True).T,\n",
    "        img_shape=(28, 28), \n",
    "        tile_shape=(10, 10),\n",
    "        tile_spacing=(1, 1)\n",
    "    )\n",
    ")\n",
    "image.save('filters_corruption_0.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0, cost is  81.771416\n",
      "Training epoch 1, cost is  73.42857\n",
      "Training epoch 2, cost is  70.86327\n",
      "Training epoch 3, cost is  69.33966\n",
      "Training epoch 4, cost is  68.41347\n",
      "Training epoch 5, cost is  67.72368\n",
      "Training epoch 6, cost is  67.240135\n",
      "Training epoch 7, cost is  66.8493\n",
      "Training epoch 8, cost is  66.56639\n",
      "Training epoch 9, cost is  66.35912\n",
      "Training epoch 10, cost is  66.13366\n",
      "Training epoch 11, cost is  65.989395\n",
      "Training epoch 12, cost is  65.83441\n",
      "Training epoch 13, cost is  65.71854\n",
      "Training epoch 14, cost is  65.601074\n",
      "The no corruption code training an for 2.0588454288666376 m\n"
     ]
    }
   ],
   "source": [
    "da = test_da(corruption_level=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "image = Image.fromarray(\n",
    "    tile_raster_images(\n",
    "        X=da.W.get_value(borrow=True).T,\n",
    "        img_shape=(28, 28), \n",
    "        tile_shape=(10, 10),\n",
    "        tile_spacing=(1, 1)\n",
    "    )\n",
    ")\n",
    "image.save('filters_corruption_0_3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "########## Stacked dA ##########\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,activation=T.tanh): # if set actication=T.nnet.sigmoid, becomes logistic regresssion layer\n",
    "        self.input = input\n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.input = input\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # predict_proba\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(self.input, self.W) + self.b) # softmax=normalized sigmoid\n",
    "        # predict\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "    # this is akin to cost = -1/m * sigma(ylog(wx) + (1-y)log(1-wx)) when y is binomial\n",
    "    # in the current case y has n-labels, and only the prediction of the right label is picked out\n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    # perc of wrong predictions\n",
    "    def errors(self, y):\n",
    "        return T.mean(T.neq(self.y_pred, y)) # T.neq(a,b) checks a != b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "class SdA:\n",
    "    def __init__(\n",
    "        self, \n",
    "        numpy_rng=numpy.random.RandomState(1234),\n",
    "        theano_rng=None, \n",
    "        n_ins=784,\n",
    "        n_outs=10,\n",
    "        hidden_layers_sizes=[500, 500],  # each elem represents on layer with n(value of elem) nodes\n",
    "        corruption_levels=[0.1, 0.1]\n",
    "    ):\n",
    "        \n",
    "        self.sigmoid_layers = []\n",
    "        self.da_layers = []\n",
    "        self.params = [] # holds the shared/updatable vars\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "        assert self.n_layers > 0\n",
    "        \n",
    "        self.numpy_rng = numpy_rng\n",
    "        self.theano_rng = theano_rng or RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        self.index = T.lscalar('index')\n",
    "        self.batch_size = T.lscalar('batch_size')\n",
    "        self.x = T.matrix('x')\n",
    "        self.x_batch = self.x[self.index * self.batch_size : (self.index + 1) * self.batch_size]\n",
    "        self.y = T.ivector('y') # the labels are presented as 1D vector of [int] labels\n",
    "        self.y_batch = self.y[self.index * self.batch_size : (self.index + 1) * self.batch_size]\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            output_size = hidden_layers_sizes[i]\n",
    "            if i == 0:\n",
    "                # first layer is to input\n",
    "                input_size = n_ins\n",
    "                layer_input = self.x_batch\n",
    "            else:\n",
    "                # subseq layers are dAs, and the input is the prev layer\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "                layer_input = self.sigmoid_layers[-1].output # output is the final activation\n",
    "            \n",
    "            # logistic regression layer\n",
    "            sigmoid_layer = HiddenLayer(\n",
    "                rng=self.numpy_rng,\n",
    "                input=layer_input,\n",
    "                n_in=input_size,\n",
    "                n_out=output_size,\n",
    "                activation=T.nnet.sigmoid\n",
    "            )\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "            self.params.extend(sigmoid_layer.params) # sigmoid_layer.params is [W,b]; Note: extend: [a] + [b]; append [a] << [b]\n",
    "            \n",
    "            # DA layer, which pretrains the W and b that will be used by the SdA\n",
    "            da_layer = dA(\n",
    "                numpy_rng=self.numpy_rng,\n",
    "                theano_rng=self.theano_rng,\n",
    "                input=layer_input,\n",
    "                n_visible=input_size,\n",
    "                n_hidden=output_size,\n",
    "                W=sigmoid_layer.W,\n",
    "                bhid=sigmoid_layer.b\n",
    "            )\n",
    "            self.da_layers.append(da_layer)\n",
    "            # note for this implementation, vbias of the RBMs are not treated as a param of the DBN (whereas W and hbias is already included in the DBN params)\n",
    "            \n",
    "        # note the sigmoid_layers do not generate a prediction or return the error of the model\n",
    "        # thus a LogisticRegression class that has those functions is added to the end of the sigmoid_layers\n",
    "        # the input is the activation of the final sigmoid_layer\n",
    "        # output is the actual prediction\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs)\n",
    "        self.params.extend(self.logLayer.params)\n",
    "        \n",
    "        # finetune_cost = cost of regression model\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y_batch)\n",
    "        # perc of wrong preds\n",
    "        self.errors = self.logLayer.errors(self.y_batch)\n",
    "        \n",
    "    def pretraining_functions(self, X, batch_size): \n",
    "        corruption_level = T.scalar('corruption')\n",
    "        learning_rate = T.scalar('lr')\n",
    "        \n",
    "        pretrain_fns = []\n",
    "        for da in self.da_layers:\n",
    "            cost, updates = da.get_cost_updates(corruption_level, learning_rate)\n",
    "\n",
    "            fn = theano.function(\n",
    "                inputs=[self.index, corruption_level, learning_rate],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.batch_size: batch_size,\n",
    "                    self.x: X\n",
    "                }\n",
    "            )\n",
    "            # append theano function for each layer to output\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "    \n",
    "    ### the rest below is the same as DBN's implementation\n",
    "    def build_train_function(\n",
    "        self, \n",
    "        train_x, \n",
    "        train_y, \n",
    "        batch_size, \n",
    "        learning_rate\n",
    "    ):\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # gradients of MLP, computed by theano automatically\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # create updates list\n",
    "        updates = {}\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates[param] = param - gparam * T.cast(learning_rate, dtype=theano.config.floatX)\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[self.index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.batch_size: batch_size,\n",
    "                self.x: train_x,\n",
    "                self.y: train_y\n",
    "            }\n",
    "        )\n",
    "        return train_fn\n",
    "\n",
    "    def get_errors(self, X, y, batch_size):  \n",
    "        n_batches = int(X.shape[0] / batch_size)\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "        get_batch_error = theano.function(\n",
    "            inputs=[self.index],\n",
    "            outputs =self.errors, # perc of wrong preds\n",
    "            givens={\n",
    "                self.batch_size: batch_size,\n",
    "                self.x: X,\n",
    "                self.y: y\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        def score_func():\n",
    "            return [get_batch_error(i) for i in range(n_batches)]\n",
    "        \n",
    "        return score_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "... pre-training the model\n",
      "Pre-training layer 0, epoch 0, mean cost: \n",
      "114.97539083404541\n",
      "Pre-training layer 0, epoch 1, mean cost: \n",
      "89.29495334701538\n",
      "Pre-training layer 0, epoch 2, mean cost: \n",
      "82.76587084350587\n",
      "Pre-training layer 0, epoch 3, mean cost: \n",
      "79.09152413101197\n",
      "Pre-training layer 0, epoch 4, mean cost: \n",
      "76.63940047073365\n",
      "Pre-training layer 0, epoch 5, mean cost: \n",
      "74.84518805465699\n",
      "Pre-training layer 0, epoch 6, mean cost: \n",
      "73.42404341583251\n",
      "Pre-training layer 0, epoch 7, mean cost: \n",
      "72.28049770202637\n",
      "Pre-training layer 0, epoch 8, mean cost: \n",
      "71.30632259674073\n",
      "Pre-training layer 0, epoch 9, mean cost: \n",
      "70.50293253479003\n",
      "Pre-training layer 0, epoch 10, mean cost: \n",
      "69.81221449508666\n",
      "Pre-training layer 0, epoch 11, mean cost: \n",
      "69.18325417785644\n",
      "Pre-training layer 0, epoch 12, mean cost: \n",
      "68.61638901062011\n",
      "Pre-training layer 0, epoch 13, mean cost: \n",
      "68.16601371383668\n",
      "Pre-training layer 0, epoch 14, mean cost: \n",
      "67.71901187667847\n",
      "Pre-training layer 1, epoch 0, mean cost: \n",
      "283.3581629852295\n",
      "Pre-training layer 1, epoch 1, mean cost: \n",
      "252.4028918914795\n",
      "Pre-training layer 1, epoch 2, mean cost: \n",
      "242.04841062927247\n",
      "Pre-training layer 1, epoch 3, mean cost: \n",
      "236.09005106201172\n",
      "Pre-training layer 1, epoch 4, mean cost: \n",
      "232.0444986907959\n",
      "Pre-training layer 1, epoch 5, mean cost: \n",
      "229.10166868286132\n",
      "Pre-training layer 1, epoch 6, mean cost: \n",
      "226.89653882751466\n",
      "Pre-training layer 1, epoch 7, mean cost: \n",
      "225.14307133789063\n",
      "Pre-training layer 1, epoch 8, mean cost: \n",
      "223.7245842590332\n",
      "Pre-training layer 1, epoch 9, mean cost: \n",
      "222.55837623901368\n",
      "Pre-training layer 1, epoch 10, mean cost: \n",
      "221.55293600158691\n",
      "Pre-training layer 1, epoch 11, mean cost: \n",
      "220.721877041626\n",
      "Pre-training layer 1, epoch 12, mean cost: \n",
      "219.99100878601075\n",
      "Pre-training layer 1, epoch 13, mean cost: \n",
      "219.3636971862793\n",
      "Pre-training layer 1, epoch 14, mean cost: \n",
      "218.8046351989746\n",
      "Pre-training layer 2, epoch 0, mean cost: \n",
      "62.24118619766235\n",
      "Pre-training layer 2, epoch 1, mean cost: \n",
      "55.01858622970581\n",
      "Pre-training layer 2, epoch 2, mean cost: \n",
      "51.03798367385864\n",
      "Pre-training layer 2, epoch 3, mean cost: \n",
      "48.633719610595705\n",
      "Pre-training layer 2, epoch 4, mean cost: \n",
      "47.06071958847046\n",
      "Pre-training layer 2, epoch 5, mean cost: \n",
      "45.91331293258667\n",
      "Pre-training layer 2, epoch 6, mean cost: \n",
      "45.03613892974853\n",
      "Pre-training layer 2, epoch 7, mean cost: \n",
      "44.32457494354248\n",
      "Pre-training layer 2, epoch 8, mean cost: \n",
      "43.758230238342286\n",
      "Pre-training layer 2, epoch 9, mean cost: \n",
      "43.25196535263061\n",
      "Pre-training layer 2, epoch 10, mean cost: \n",
      "42.865840319442746\n",
      "Pre-training layer 2, epoch 11, mean cost: \n",
      "42.52773520355225\n",
      "Pre-training layer 2, epoch 12, mean cost: \n",
      "42.236961212921145\n",
      "Pre-training layer 2, epoch 13, mean cost: \n",
      "41.979930360794064\n",
      "Pre-training layer 2, epoch 14, mean cost: \n",
      "41.75345576095581\n",
      "Preraining time: 287.7615342429999 s.\n"
     ]
    }
   ],
   "source": [
    "batch_size=10\n",
    "pretrain_lr=0.001\n",
    "pretraining_epochs=15\n",
    "corruption_levels=[0.1, 0.2, 0.3]\n",
    "\n",
    "# # for testing\n",
    "# batch_size=10\n",
    "# pretrain_lr=0.01\n",
    "# pretraining_epochs=1\n",
    "# corruption_levels=[0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "train_set_y = train_set_y.astype('int32')\n",
    "valid_set_y = valid_set_y.astype('int32')\n",
    "test_set_y = test_set_y.astype('int32')\n",
    "\n",
    "numpy_rng = numpy.random.RandomState(89677)\n",
    "n_train_batches = int(train_set_x.shape[0] / batch_size)\n",
    "\n",
    "print('... building the model')\n",
    "sda = SdA(\n",
    "    numpy_rng=numpy_rng, \n",
    "    n_ins=28 * 28,\n",
    "    hidden_layers_sizes=[500, 100, 20],\n",
    "    n_outs=10\n",
    ")\n",
    "\n",
    "pretraining_fns = sda.pretraining_functions(\n",
    "    X=train_set_x,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print('... pre-training the model')\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Pre-train layer-wise\n",
    "for i in range(sda.n_layers):\n",
    "    for epoch in range(pretraining_epochs):\n",
    "        costs = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            costs.append(pretraining_fns[i](batch_index, corruption_levels[i], pretrain_lr))\n",
    "        print(f'Pre-training layer {i}, epoch {epoch}, mean cost: ')\n",
    "        print(numpy.mean(costs, dtype='float64'))\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f'Preraining time: {end_time - start_time} s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... getting the finetuning functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:129: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <class 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... finetuning the model\n",
      "epoch 0, minibatch 5000/5000, validation error 7.290000000000001%\n",
      "     epoch 0, minibatch minibatch 5000/5000, test error of best model 7.870000000000001%\n",
      "epoch 1, minibatch 5000/5000, validation error 5.5200000000000005%\n",
      "     epoch 1, minibatch minibatch 5000/5000, test error of best model 6.09%\n",
      "epoch 2, minibatch 5000/5000, validation error 4.5%\n",
      "     epoch 2, minibatch minibatch 5000/5000, test error of best model 4.9799999999999995%\n",
      "epoch 3, minibatch 5000/5000, validation error 4.13%\n",
      "     epoch 3, minibatch minibatch 5000/5000, test error of best model 4.38%\n",
      "epoch 4, minibatch 5000/5000, validation error 3.8600000000000003%\n",
      "     epoch 4, minibatch minibatch 5000/5000, test error of best model 4.140000000000001%\n",
      "epoch 5, minibatch 5000/5000, validation error 3.5700000000000003%\n",
      "     epoch 5, minibatch minibatch 5000/5000, test error of best model 3.91%\n",
      "epoch 6, minibatch 5000/5000, validation error 3.55%\n",
      "     epoch 6, minibatch minibatch 5000/5000, test error of best model 3.7800000000000002%\n",
      "epoch 7, minibatch 5000/5000, validation error 3.4900000000000007%\n",
      "     epoch 7, minibatch minibatch 5000/5000, test error of best model 3.61%\n",
      "epoch 8, minibatch 5000/5000, validation error 3.34%\n",
      "     epoch 8, minibatch minibatch 5000/5000, test error of best model 3.5900000000000003%\n",
      "epoch 9, minibatch 5000/5000, validation error 3.2399999999999998%\n",
      "     epoch 9, minibatch minibatch 5000/5000, test error of best model 3.55%\n",
      "epoch 10, minibatch 5000/5000, validation error 3.12%\n",
      "     epoch 10, minibatch minibatch 5000/5000, test error of best model 3.3600000000000003%\n",
      "epoch 11, minibatch 5000/5000, validation error 3.11%\n",
      "     epoch 11, minibatch minibatch 5000/5000, test error of best model 3.39%\n",
      "epoch 12, minibatch 5000/5000, validation error 3.0900000000000003%\n",
      "     epoch 12, minibatch minibatch 5000/5000, test error of best model 3.2799999999999994%\n",
      "epoch 13, minibatch 5000/5000, validation error 3.0%\n",
      "     epoch 13, minibatch minibatch 5000/5000, test error of best model 3.17%\n",
      "epoch 14, minibatch 5000/5000, validation error 3.0%\n",
      "epoch 15, minibatch 5000/5000, validation error 2.9400000000000004%\n",
      "     epoch 15, minibatch minibatch 5000/5000, test error of best model 3.04%\n",
      "epoch 16, minibatch 5000/5000, validation error 2.88%\n",
      "     epoch 16, minibatch minibatch 5000/5000, test error of best model 3.02%\n",
      "epoch 17, minibatch 5000/5000, validation error 2.87%\n",
      "     epoch 17, minibatch minibatch 5000/5000, test error of best model 2.9499999999999997%\n",
      "epoch 18, minibatch 5000/5000, validation error 2.9000000000000004%\n",
      "epoch 19, minibatch 5000/5000, validation error 2.88%\n",
      "epoch 20, minibatch 5000/5000, validation error 2.87%\n",
      "epoch 21, minibatch 5000/5000, validation error 2.84%\n",
      "     epoch 21, minibatch minibatch 5000/5000, test error of best model 2.9000000000000004%\n",
      "epoch 22, minibatch 5000/5000, validation error 2.82%\n",
      "     epoch 22, minibatch minibatch 5000/5000, test error of best model 2.85%\n",
      "epoch 23, minibatch 5000/5000, validation error 2.8000000000000003%\n",
      "     epoch 23, minibatch minibatch 5000/5000, test error of best model 2.87%\n",
      "epoch 24, minibatch 5000/5000, validation error 2.7800000000000002%\n",
      "     epoch 24, minibatch minibatch 5000/5000, test error of best model 2.85%\n",
      "epoch 25, minibatch 5000/5000, validation error 2.75%\n",
      "     epoch 25, minibatch minibatch 5000/5000, test error of best model 2.82%\n",
      "epoch 26, minibatch 5000/5000, validation error 2.72%\n",
      "     epoch 26, minibatch minibatch 5000/5000, test error of best model 2.8099999999999996%\n",
      "epoch 27, minibatch 5000/5000, validation error 2.69%\n",
      "     epoch 27, minibatch minibatch 5000/5000, test error of best model 2.78%\n",
      "epoch 28, minibatch 5000/5000, validation error 2.64%\n",
      "     epoch 28, minibatch minibatch 5000/5000, test error of best model 2.74%\n",
      "epoch 29, minibatch 5000/5000, validation error 2.65%\n",
      "epoch 30, minibatch 5000/5000, validation error 2.66%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-3633730fc9b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_validation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mbest_validation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-65-3633730fc9b4>\u001b[0m in \u001b[0;36mtrain_mlp\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/tensor/basic.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inp, out)\u001b[0m\n\u001b[1;32m   5968\u001b[0m         \u001b[0;31m# gives a numpy float object but we need to return a 0d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5969\u001b[0m         \u001b[0;31m# ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5970\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5972\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "finetune_lr = 0.1\n",
    "training_epochs=1000\n",
    "\n",
    "print('... getting the finetuning functions')\n",
    "train_fn = sda.build_train_function(\n",
    "    train_x = train_set_x,\n",
    "    train_y = train_set_y,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=finetune_lr\n",
    ")\n",
    "get_validate_errors = sda.get_errors(valid_set_x, valid_set_y, batch_size)\n",
    "get_test_errors = sda.get_errors(test_set_x, test_set_y, batch_size)\n",
    "\n",
    "def train_mlp():\n",
    "    print('... finetuning the model')\n",
    "    \n",
    "    # early-stopping parameters\n",
    "    patience = 4 * n_train_batches # look as this many examples regardless, i.e. 4 epochs\n",
    "    patience_increase = 2. # loop for n times more when a new best is found\n",
    "    improvement_threshold = 0.995 # a relative improvement of this much is considered significant\n",
    "\n",
    "    # go through this many minibatches before checking the network on\n",
    "    # the validation set; in this case we check every epoch\n",
    "    validation_frequency = min(n_train_batches, patience / 2) # = n_train_batches\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            train_fn(minibatch_index)\n",
    "            iter = epoch * n_train_batches + minibatch_index\n",
    "\n",
    "            # for every 'validation_frequency' iters\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = get_validate_errors()\n",
    "                curr_mean_validation_loss = numpy.mean(validation_losses, dtype='float64')\n",
    "                print(f'epoch {epoch}, minibatch {minibatch_index + 1}/{n_train_batches}, validation error {curr_mean_validation_loss * 100.}%')\n",
    "\n",
    "                # if we got the least validation errors until now\n",
    "                if curr_mean_validation_loss < best_validation_loss:\n",
    "                    # improve patience if loss improvement is good enough; which will allow more training = double of the curr loop count\n",
    "                    if (curr_mean_validation_loss < best_validation_loss * improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = curr_mean_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = get_test_errors()\n",
    "                    test_score = numpy.mean(test_losses, dtype='float64')\n",
    "                    print(f'     epoch {epoch}, minibatch minibatch {minibatch_index + 1}/{n_train_batches}, test error of best model {test_score * 100.}%')\n",
    "\n",
    "            # if no improvement in validation score for the last 50% iters\n",
    "            if patience <= iter:\n",
    "                return best_validation_loss, best_iter, test_score\n",
    "    return best_validation_loss, best_iter, test_score\n",
    "\n",
    "best_validation_loss, best_iter, test_score = train_mlp()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f'training time: {end_time - start_time}s.')\n",
    "print(f'Optimization complete with best validation score of {best_validation_loss * 100.}%,\\n'\n",
    "    f'obtained at iteration {best_iter + 1},\\n'\n",
    "    f'with test performance {test_score * 100.}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
