{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "test = pd.read_csv('testtrolls.csv')\n",
    "training = pd.read_csv('trainingtrolls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20120529113543Z</td>\n",
       "      <td>\"GALLUP DAILY\\nMay 24-26, 2012 \\u2013 Updates ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120612120924Z</td>\n",
       "      <td>\"This is someone whose self-importance got the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20120530113019Z</td>\n",
       "      <td>\"Don't stand on your porch.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20120528204226Z</td>\n",
       "      <td>\"camp has got to come in here and get this guy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"You could not be more wrong. The American Tea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment\n",
       "0       0  20120529113543Z  \"GALLUP DAILY\\nMay 24-26, 2012 \\u2013 Updates ...\n",
       "1       0  20120612120924Z  \"This is someone whose self-importance got the...\n",
       "2       0  20120530113019Z                       \"Don't stand on your porch.\"\n",
       "3       0  20120528204226Z  \"camp has got to come in here and get this guy...\n",
       "4       0              NaN  \"You could not be more wrong. The American Tea..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse 'some degree' of html text, like removing quotes, double \\\\ etc.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "training['Comment'] = training['Comment'].apply(lambda text: BeautifulSoup(text, 'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test _EM 123'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "EMAIL_REGEX = r'[\\w\\-\\.\\+]+\\@[a-zA-Z0-9\\.\\-]+\\.[a-zA-z0-9]{2,4}'\n",
    "re.sub(EMAIL_REGEX, '_EM', \"test anc@mail.com 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test _U _U 123'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL_REGEX = r'http(s)?:\\/\\/\\S+' # only works for ones prepended by http(s)://\n",
    "re.sub(URL_REGEX, '_U', \"test http://mail.com https://mail.com 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format whitespaces, line breaks and quotes\n",
    "\n",
    "line = \" \\\" \\n \\\\n test - _ \\ '\"\n",
    "\n",
    "line = line.replace('\"', ' ')\n",
    "line = line.replace('_', ' ')\n",
    "line = line.replace('-', ' ')\n",
    "line = line.replace('\\n', ' ')\n",
    "line = line.replace('\\\\n', ' ')\n",
    "line = line.replace('\\'', ' ')\n",
    "line = line.replace('\\\\', ' ')\n",
    "line = re.sub(' +',' ', line)\n",
    "\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'... test what _BX\\n you _BQ\\n   _SS\\n test aagh _EL abcdef help _Q\\n me _X\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manage punctuation\n",
    "# ([^!\\?]) matches not ! or ?\n",
    "# (\\Z|[^!\\?]) matches end of line or not (! or ?)\n",
    "# r'\\1 _BQ\\n\\3' replaces the middle matching part with _BQ\n",
    "line = \"... test what?!? you????  ... test aaaaaaagh abc.def help? me!\"\n",
    "\n",
    "line = re.sub(r'([^!\\?])(\\?{2,})(\\Z|[^!\\?])', r'\\1 _BQ\\n\\3', line) # replace what??? to what _BQ\\n\n",
    "line = re.sub(r'([^\\.])(\\.{2,})', r'\\1 _SS\\n', line) # replace  uh... to uh _SS\\n\n",
    "line = re.sub(r'([^!\\?])(\\?|!){2,}(\\Z|[^!\\?])', r'\\1 _BX\\n\\3', line) # replace what!? to what _BX\\n \n",
    "line = re.sub(r'([^!\\?])\\?(\\Z|[^!\\?])', r'\\1 _Q\\n\\2', line) # replace is it? with is it _Q\\n\n",
    "line = re.sub(r'([^!\\?])!(\\Z|[^!\\?])', r'\\1 _X\\n\\2', line) # replace great! with great _X\\n\n",
    "line = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2 _EL', line) # replace aaaaaaagh(at least 3 repeating letters) to aagh _EL\n",
    "line = re.sub(r'(\\w+)\\.(\\w+)', r'\\1\\2', line) # replace abc.def with abcdef\n",
    "\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey #%**&$ _SW you'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hey #%**&$ you\"\n",
    "\n",
    "# swearing\n",
    "text = re.sub(r'([#%&\\*\\$]{2,})(\\w*)', r'\\1\\2 _SW', text) # add _SW after sequence of #%$&* (assuming ! and ? are already removed, and not including @)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi _BS _S  _S _BF _F'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi x-}} => <3 =(( :(\"\n",
    "\n",
    "# emotes\n",
    "# note: (?:xxxx){n,} meaning any of which with count n+\n",
    "# head: any of 8 x ; : =\n",
    "# nose(optional): -\n",
    "# mouth(happy): any of ) ] } >\n",
    "# mouth(sad):   any of ( [ | \\ / { < \n",
    "\n",
    "text = re.sub(r' [8x;:=]-?(?:\\)|\\}|\\]|>){2,}', r' _BS', text) # with 2+ mouth symbols\n",
    "text = re.sub(r' ([8x;:=]-?[\\)\\}\\]|>])|(?:<3)', r' _S', text) # with 1 mouth OR heart symbol, ie.<3\n",
    "text = re.sub(r' [8x;:=]-?(?:\\(|\\[|\\||\\\\|/|\\{|<){2,}', r' _BF', text) # wih 2+ mouth symbols\n",
    "text = re.sub(r' [8x;:=]-?[\\(\\[\\(|\\[|\\||\\\\|/|\\{|<]', r' _F', text) # with 1 mouth\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is '"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove number and percentages\n",
    "line = \"it is 100%\"\n",
    "\n",
    "line = re.sub('[1|2|3|4|5|6|7|8|9|0]', '', line)\n",
    "line = re.sub('[%]', '', line)\n",
    "\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi'],\n",
       " ['welcome'],\n",
       " ['hello'],\n",
       " ['See', 'this'],\n",
       " ['amazing', 'sentence'],\n",
       " ['don', 't', 'you', 'agree'],\n",
       " ['100%', 'wholesome&healthy']]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into phrases and words\n",
    "line = \"Hi; welcome \\n hello. See this: amazing sentence (don't you agree) 100% wholesome&healthy! .  . \"\n",
    "\n",
    "phrases = re.split(r'[;:\\.()\\n]', line)\n",
    "phrases = [re.findall(r'[\\w%\\*&#]+', ph) for ph in phrases] # select words (words may include symbols except ! and ?)\n",
    "phrases = [ph for ph in phrases if ph] # remove empty arrays inside phrases; note: empty array is falsey\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'welcome',\n",
       " 'hello',\n",
       " 'See',\n",
       " 'this',\n",
       " 'amazing',\n",
       " 'sentence',\n",
       " 'don',\n",
       " 't',\n",
       " 'you',\n",
       " 'agree',\n",
       " '100%',\n",
       " 'wholesome&healthy']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten phrases into a single list\n",
    "\n",
    "words = []\n",
    "[words.extend(ph) for ph in phrases]\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stringing consecutive single letters together\n",
    "words = [\"h\", \"e\", \"l\", \"l\", \"o\", \"world\"]\n",
    "\n",
    "tmp = words\n",
    "words = []\n",
    "new_word = ''\n",
    "for word in tmp:\n",
    "    if len(word) == 1: # keep adding consecutive single letters to new_word until the next word is not a single letter\n",
    "        new_word = new_word + word\n",
    "    else:\n",
    "        if new_word:\n",
    "            words.append(new_word)\n",
    "            new_word = ''\n",
    "        words.append(word)\n",
    "        \n",
    "words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain', 'Madrid', 'Spain', 'makes', 'elated', 'boy']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "words = re.findall(r'[\\w%\\*&#]+', \"the rain in Madrid of Spain makes him a elated boy\")\n",
    "\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram tagger of word's part of speech\n",
    "import nltk\n",
    "from nltk import NgramTagger\n",
    "# nltk.download('brown')\n",
    "\n",
    "# Backoff tagging\n",
    "brown_a = nltk.corpus.brown.tagged_sents()\n",
    "tagger = None\n",
    "# backoff sets the fallback if fails to tag; \n",
    "# since this is inside a loop, \n",
    "# the 4-gram will fall back on the 3-gram; \n",
    "# 3 on 2 and so on, until 1-gram will fallback to None\n",
    "for n in range(1,4):\n",
    "    tagger = NgramTagger(n, brown_a, backoff = tagger) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('silly', 'JJ'),\n",
       " ('dogs', 'NNS'),\n",
       " ('happily', 'RB'),\n",
       " ('jumped', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('reddish', 'JJ'),\n",
       " ('foxes', None)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['silly', 'dogs', 'happily', 'jumped', 'over', 'the', 'reddish', 'foxes']\n",
    "words = tagger.tag(words)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('silly', 'r'),\n",
       " ('dogs', 'n'),\n",
       " ('happily', 'r'),\n",
       " ('jumped', 'v'),\n",
       " ('over', None),\n",
       " ('the', None),\n",
       " ('reddish', 'a'),\n",
       " ('foxes', 'n')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "words = ['silly', 'dogs', 'happily', 'jumped', 'over', 'the', 'reddish', 'foxes']\n",
    "\n",
    "words = pos_tag(words)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag == None:\n",
    "        return None\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "words = list(map(lambda word_pos: (word_pos[0], get_wordnet_pos(word_pos[1])), words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['silli', 'dog', 'jump', 'over', 'the', 'reddish', 'fox']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = ['silly', 'dogs', 'jumped', 'over', 'the', 'reddish', 'foxes']\n",
    "list(map(lambda word: stemmer.stem(word), words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['silly', 'dog', 'happily', 'jump', 'over', 'the', 'reddish', 'fox']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = []\n",
    "for word, pos in words: \n",
    "    if pos:\n",
    "        word = lemmatizer.lemmatize(word, pos = pos)\n",
    "    lemmatized.append(word)\n",
    "        \n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# together\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_line(line):\n",
    "    line = str(BeautifulSoup(str(line), \"html.parser\"))\n",
    "\n",
    "    EMAIL_REGEX = r'[\\w\\-\\.\\+]+\\@[a-zA-Z0-9\\.\\-]+\\.[a-zA-z0-9]{2,4}'\n",
    "    line = re.sub(EMAIL_REGEX, '_EM', line)\n",
    "\n",
    "    URL_REGEX = r'http(s)?:\\/\\/\\S+' # only works for ones prepended by http(s)://\n",
    "    line = re.sub(URL_REGEX, '_U', line)\n",
    "\n",
    "    # format whitespaces, line breaks and quotes\n",
    "    line = line.replace('\"', ' ')\n",
    "    line = line.replace('_', ' ')\n",
    "    line = line.replace('-', ' ')\n",
    "    line = line.replace('\\n', ' ')\n",
    "    line = line.replace('\\\\n', ' ')\n",
    "    line = line.replace('\\'', ' ')\n",
    "    line = re.sub(' +',' ', line)\n",
    "    \n",
    "    print(line)\n",
    "    # manage punctuation\n",
    "    line = re.sub(r'([^!\\?])(\\?{2,})(\\Z|[^!\\?])', r'\\1 _BQ\\n\\3', line) # replace what??? to what _BQ\\n\n",
    "    line = re.sub(r'([^\\.])(\\.{2,})', r'\\1 _SS\\n', line) # replace  uh... to uh _SS\\n\n",
    "    line = re.sub(r'([^!\\?])(\\?|!){2,}(\\Z|[^!\\?])', r'\\1 _BX\\n\\3', line) # replace what!? to what _BX\\n \n",
    "    line = re.sub(r'([^!\\?])\\?(\\Z|[^!\\?])', r'\\1 _Q\\n\\2', line) # replace is it? with is it _Q\\n\n",
    "    line = re.sub(r'([^!\\?])!(\\Z|[^!\\?])', r'\\1 _X\\n\\2', line) # replace great! with great _X\\n\n",
    "    line = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2 _EL', line) # replace aaaaaaagh(at least 3 repeating letters) to aagh _EL\n",
    "    line = re.sub(r'(\\w+)\\.(\\w+)', r'\\1\\2', line) # replace abc.def with abcdef\n",
    "\n",
    "    # swearing\n",
    "    line = re.sub(r'([#%&\\*\\$]{2,})(\\w*)', r'\\1\\2 _SW', line) # add _SW after sequence of #%$&* (assuming ! and ? are already removed, and not including @)\n",
    "\n",
    "    # emotes\n",
    "    # note: (?:xxxx){n,} meaning any of which with count n+\n",
    "    # head: any of 8 x ; : =\n",
    "    # nose(optional): -\n",
    "    # mouth(happy): any of ) ] } >\n",
    "    # mouth(sad):   any of ( [ | \\ / { < \n",
    "    line = re.sub(r' [8x;:=]-?(?:\\)|\\}|\\]|>){2,}', r' _BS', line) # with 2+ mouth symbols\n",
    "    line = re.sub(r' ([8x;:=]-?[\\)\\}\\]|>])|(?:<3)', r' _S', line) # with 1 mouth OR heart symbol, ie.<3\n",
    "    line = re.sub(r' [8x;:=]-?(?:\\(|\\[|\\||\\\\|/|\\{|<){2,}', r' _BF', line) # wih 2+ mouth symbols\n",
    "    line = re.sub(r' [8x;:=]-?[\\(\\[\\(|\\[|\\||\\\\|/|\\{|<]', r' _F', line) # with 1 mouth\n",
    "\n",
    "    # remove number and percentages; and '\\'\n",
    "    line = re.sub('[1|2|3|4|5|6|7|8|9|0]', '', line)\n",
    "    line = re.sub('[%]', '', line)\n",
    "    line = line.replace('\\\\', ' ')\n",
    "\n",
    "    # split into phrases and words\n",
    "    phrases = re.split(r'[;:\\.()\\n]', line)\n",
    "    phrases = [re.findall(r'[\\w%\\*&#]+', ph) for ph in phrases] # select words (words may include symbols except ! and ?)\n",
    "    phrases = [ph for ph in phrases if ph] # remove empty arrays inside phrases; note: empty array is falsey\n",
    "\n",
    "    # flatten phrases into a single list\n",
    "    words = []\n",
    "    [words.extend(ph) for ph in phrases]\n",
    "\n",
    "    # stringing consecutive single letters together\n",
    "    tmp = words\n",
    "    words = []\n",
    "    new_word = ''\n",
    "    for word in tmp:\n",
    "        if len(word) == 1: # keep adding consecutive single letters to new_word until the next word is not a single letter\n",
    "            new_word = new_word + word\n",
    "        else:\n",
    "            if new_word:\n",
    "                words.append(new_word)\n",
    "                new_word = ''\n",
    "            words.append(word)\n",
    "\n",
    "    # remove common words, defined in stopwords by NLTK\n",
    "    words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "    \n",
    "    # tag part-of-speech\n",
    "    words = pos_tag(words)\n",
    "    words = list(map(lambda word_pos: (word_pos[0], get_wordnet_pos(word_pos[1])), words))\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    for word, pos in words: \n",
    "        if pos:\n",
    "            word = lemmatizer.lemmatize(word, pos = pos)\n",
    "        lemmatized.append(word)\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag == None:\n",
    "        return None\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i am 100% sure that this is a scam!!!! ;( \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sure', 'scam', '_BX', '_F']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_line(\"'i am 100% sure that this is a scam!!!! ;('\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dogs lazily lay on the green fields :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dog', 'lazily', 'lay', 'green', 'field', '_S']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_line(\"the dogs lazily lay on the green fields :)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
