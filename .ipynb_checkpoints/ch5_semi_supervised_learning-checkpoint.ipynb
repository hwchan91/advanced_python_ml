{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following implementation only work with binary labels\n",
    "class SelfLearningModel():\n",
    "    # pass in evaluator for labeled data as basemodel\n",
    "    def __init__(self, basemodel, max_iter = 200, prob_threshold = 0.8):\n",
    "        self.model = basemodel\n",
    "        self.max_iter = max_iter\n",
    "        self.prob_threshold = prob_threshold \n",
    "        \n",
    "    # unlabeled data is marked as -1 in y    \n",
    "    def fit(self, X, y):\n",
    "        unlabeledX = X[y==-1, :]\n",
    "        labeledX = X[y!=-1, :]\n",
    "        labeledy = y[y!=-1]\n",
    "        \n",
    "        self.model.fit(labeledX, labeledy)\n",
    "        \n",
    "        # to get an estimate of proba if basemodel does not have #predict_proba (with Platt scaling)\n",
    "        if not getattr(self.model, \"predict_proba\", None):\n",
    "            self.plattlr = LR()\n",
    "            preds = self.model.predict(labeledX)\n",
    "            self.plattlr.fit( preds.reshape( -1, 1 ), labeledy ) # essentially drawing a boundary between the labelledy labels to estimate the proba\n",
    "            \n",
    "        unlabeledy = self.model.predict(unlabeledX)\n",
    "        unlabeledprob = self.predict_proba(unlabeledX)\n",
    "        \n",
    "        unlabeledy_old = []\n",
    "        #re-train, labeling unlabeled instances with model predictions, until convergence\n",
    "        i = 0\n",
    "        while (len(unlabeledy_old) == 0 or numpy.any(unlabeledy!=unlabeledy_old)) and i < self.max_iter:\n",
    "            unlabeledy_old = numpy.copy(unlabeledy)\n",
    "            # only works if there are only 2 labels\n",
    "            uidx = numpy.where((unlabeledprob[:, 0] > self.prob_threshold) | (unlabeledprob[:, 1] > self.prob_threshold))[0]\n",
    "            \n",
    "            self.model.fit(numpy.vstack((labeledX, unlabeledX[uidx, :])), numpy.hstack((labeledy, unlabeledy_old[uidx])))\n",
    "            unlabeledy = self.model.predict(unlabeledX) # does unlabeledX auto update?\n",
    "            unlabeledprob = self.predict_proba(unlabeledX)\n",
    "            i += 1\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if getattr(self.model, \"predict_proba\", None):\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            preds = self.model.predict(X)\n",
    "            return self.plattlr.predict_proba(preds.reshape( -1, 1 ))\n",
    "        \n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return sklearn.metrics.accuracy_score(y, self.model.predict(X), sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ds = pd.read_csv('heart_disease_weka_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = ds['num'].values # actual labels\n",
    "X = ds.drop(['num'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# create unlabelled data - set all as unlabelled first\n",
    "ys = np.array([-1]*len(ytrue)) # -1 denotes unlabeled point\n",
    "\n",
    "labeled_N = int(2)\n",
    "# get N/2 points where y == 0 and N/2 points wherre y == 1\n",
    "random_labeled_points = random.sample(list(np.where(ytrue == 0)[0]), labeled_N//2) + random.sample(list(np.where(ytrue == 1)[0]), labeled_N//2)\n",
    "\n",
    "# add N points of labeled data into unlabelled data\n",
    "ys[random_labeled_points] = ytrue[random_labeled_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised log.reg. score 0.4612794612794613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "basemodel = SGDClassifier(loss='log', penalty='l1', max_iter=5, tol=-np.infty)\n",
    "\n",
    "# (traditional) supervised training\n",
    "basemodel.fit(X[random_labeled_points, :], ys[random_labeled_points])\n",
    "print(\"supervised log.reg. score\", basemodel.score(X, ytrue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self-learning log.reg. score 0.5387205387205387\n"
     ]
    }
   ],
   "source": [
    "ssmodel = SelfLearningModel(basemodel)\n",
    "ssmodel.fit(X, ys)\n",
    "print(\"self-learning log.reg. score\", ssmodel.score(X, ytrue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive Pessimistic Likelihood Estimation - the following implementation only work with binary labels\n",
    "\n",
    "import numpy\n",
    "import sklearn.metrics\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import nlopt # for optimization, i.e. minimization of cost\n",
    "import scipy.stats # for t-test\n",
    "import sys\n",
    "\n",
    "class CPLELearningModel():\n",
    "    def __init__(\n",
    "        self, \n",
    "        basemodel, \n",
    "        pessimistic=True, \n",
    "        predict_from_probabilities = False, \n",
    "        use_sample_weighting = True, \n",
    "        max_iter=3000, \n",
    "        verbose = 1\n",
    "    ):\n",
    "        self.model = basemodel\n",
    "        self.pessimistic = pessimistic\n",
    "        self.predict_from_probabilities = predict_from_probabilities\n",
    "        self.use_sample_weighting = use_sample_weighting\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.it = 0 # iteration counter\n",
    "        self.noimprovementsince = 0 # log likelihood hasn't improved since this number of iterations\n",
    "        self.maxnoimprovementsince = 3 # threshold for iterations without improvements (convergence is assumed when this is reached)\n",
    "        \n",
    "        self.buffersize =  200\n",
    "        # buffer for the last few discriminative likelihoods (used to check for convergence)\n",
    "        self.lastdls = [0]*self.buffersize # same as np.zeros, except returns int 0 and list obj instead of array obj\n",
    "        \n",
    "        # best discriminative likelihood and corresponding soft labels; updated during training\n",
    "        self.bestdl = numpy.infty\n",
    "        self.bestlbls = []\n",
    "\n",
    "    # random probs are assigned to the unlabelled data, which is subseq. optimized through an optimization procedure   \n",
    "    # Explanation: converting softlabels(floats) into hard labels(binary), and then fit all the data (preferably with weights based on softlabels) \n",
    "    # the cost is then calculated by the accuracy of the labelled data + comparing the accuracy of the prediction of the fitted model with the assigned (random) labels\n",
    "    def discriminative_likelihood(\n",
    "        self, \n",
    "        model, \n",
    "        labeledData, \n",
    "        labeledy = None, \n",
    "        unlabeledData = None, \n",
    "        unlabeledWeights = None, # predicted probs of unlabeled y\n",
    "        unlabeledlambda = 1\n",
    "    ):\n",
    "        unlabeledy = (unlabeledWeights[:, 0]<0.5)*1 # taking prob, those below 0.5 are assiged 0; above 0.5 are assigned 1 (*1 converts boolean to binary)\n",
    "        uweights = numpy.copy(unlabeledWeights[:, 0])\n",
    "        uweights[unlabeledy==1] = 1-uweights[unlabeledy==1] # subtract from 1 for k=1 instances to reflect confidence; confidence is larger if prediction is closer to 0, i.e. pessimistic??\n",
    "        weights = numpy.hstack((numpy.ones(len(labeledy)), uweights)) # labeled y have 100% confidence; merged with that of the predicted unlabeled data\n",
    "        labels = numpy.hstack((labeledy, unlabeledy))\n",
    "        \n",
    "        # fit model on supervised(labeled+unlabeled) data\n",
    "        if self.use_sample_weighting: # \n",
    "            model.fit(numpy.vstack((labeledData, unlabeledData)), labels, sample_weight=weights) # use confidence level obtained above\n",
    "        else:\n",
    "            model.fit(numpy.vstack((labeledData, unlabeledData)), labels)\n",
    "        \n",
    "        # probability of labeled data\n",
    "        P = model.predict_proba(labeledData)\n",
    "        labeledDL = -sklearn.metrics.log_loss(labeledy, P) # cost, i.e. negative log-likelihood\n",
    "\n",
    "        # probability of unlabeled data\n",
    "        unlabeledP = model.predict_proba(unlabeledData)  \n",
    "        eps = 1e-15\n",
    "        unlabeledP = numpy.clip(unlabeledP, eps, 1 - eps) # so that wouldn't take log(0)\n",
    "        # separate weights into those predicting 0 and those 1; multiplied with predictions; summed, then take the mean; i.e. negative log llikelihood with each sample multiplied by respective weights\n",
    "        unlabeledDL = numpy.average((unlabeledWeights * numpy.vstack((1-unlabeledy, unlabeledy)).T * numpy.log(unlabeledP)).sum(axis=1))\n",
    "        \n",
    "        # calc cost\n",
    "        if self.pessimistic:\n",
    "            # pessimistic: minimize the difference between unlabeled and labeled discriminative likelihood (assume worst case for unknown true labels)\n",
    "            dl = unlabeledlambda * unlabeledDL - labeledDL\n",
    "        else: \n",
    "            # optimistic: minimize negative total discriminative likelihood (i.e. maximize likelihood) \n",
    "            dl = - unlabeledlambda * unlabeledDL - labeledDL\n",
    "        \n",
    "        return dl\n",
    "    \n",
    "    # allow breaking out of nlopt if cost function converged\n",
    "    def discriminative_likelihood_objective(\n",
    "        self, \n",
    "        model, \n",
    "        labeledData, \n",
    "        labeledy = None, \n",
    "        unlabeledData = None, \n",
    "        unlabeledWeights = None, \n",
    "        unlabeledlambda = 1\n",
    "    ):\n",
    "        dl = self.discriminative_likelihood(\n",
    "            model, \n",
    "            labeledData, \n",
    "            labeledy, \n",
    "            unlabeledData, \n",
    "            unlabeledWeights, \n",
    "            unlabeledlambda\n",
    "        )\n",
    "        \n",
    "        self.it += 1\n",
    "        self.lastdls[numpy.mod(self.it, len(self.lastdls))] = dl # cost result stored in lastdls in position mod(iter) index (instead of a rolling update)\n",
    "        \n",
    "        if numpy.mod(self.it, self.buffersize) == 0: # or True:\n",
    "            # improvement of last half to first half\n",
    "            improvement = numpy.mean((self.lastdls[(len(self.lastdls)//2):])) - numpy.mean((self.lastdls[:(len(self.lastdls)//2)]))\n",
    "            # ttest - test for hypothesis that the likelihoods have not changed (i.e. there has been no improvement, and we are close to convergence) \n",
    "            _, prob = scipy.stats.ttest_ind(self.lastdls[int((len(self.lastdls)/2)):], self.lastdls[:int((len(self.lastdls)/2))])\n",
    "            \n",
    "            # take confidence level at 90%, i.e. alpha=0.1\n",
    "            # if improvement is not certain accoring to t-test...\n",
    "            noimprovement = prob > 0.1 and improvement < 0\n",
    "            if noimprovement:\n",
    "                self.noimprovementsince += 1\n",
    "                if self.noimprovementsince >= self.maxnoimprovementsince:\n",
    "                    # no improvement since a while - converged; exit\n",
    "                    self.noimprovementsince = 0\n",
    "                    raise Exception(\" converged.\") # we need to raise an exception to get NLopt to stop before exceeding the iteration budget\n",
    "            else:\n",
    "                self.noimprovementsince = 0\n",
    "            \n",
    "            if self.verbose == 2:\n",
    "                print(self.it, dl, numpy.mean(self.lastdls), improvement, round(prob, 3), (prob < 0.1))\n",
    "            elif self.verbose:\n",
    "                sys.stdout.write(('.') if not noimprovement else 'n')\n",
    "                      \n",
    "        if dl < self.bestdl:\n",
    "            self.bestdl = dl\n",
    "            self.bestlbls = numpy.copy(unlabeledWeights[:, 0])\n",
    "                        \n",
    "        return dl\n",
    "    \n",
    "    def fit(self, X, y): # -1 for unlabeled\n",
    "        unlabeledX = X[y==-1, :]\n",
    "        labeledX = X[y!=-1, :]\n",
    "        labeledy = y[y!=-1]\n",
    "        \n",
    "        M = unlabeledX.shape[0]\n",
    "         \n",
    "        # train on labeled data\n",
    "        self.model.fit(labeledX, labeledy)\n",
    "        \n",
    "        # to get an estimate of proba if basemodel does not have #predict_proba (with Platt scaling)\n",
    "        if not getattr(self.model, \"predict_proba\", None):\n",
    "            # Platt scaling\n",
    "            self.plattlr = LR()\n",
    "            preds = self.model.predict(labeledX)\n",
    "            self.plattlr.fit( preds.reshape( -1, 1 ), labeledy )\n",
    "\n",
    "        # re-train, labeling unlabeled instances pessimistically\n",
    "        # use random labels initially, optimize by minimizing the cost function\n",
    "        # note: grads in the arg is necessary for nlopt\n",
    "        f = lambda softlabels, grads=[]: self.discriminative_likelihood_objective(\n",
    "                self.model, \n",
    "                labeledX, \n",
    "                labeledy=labeledy, \n",
    "                unlabeledData=unlabeledX, \n",
    "                unlabeledWeights=numpy.vstack((softlabels, 1-softlabels)).T, \n",
    "            )\n",
    "        lblinit = numpy.random.random(len(unlabeledX))\n",
    "\n",
    "        try:\n",
    "            self.it = 0\n",
    "            # optimization for minimizing cost(dl)\n",
    "            opt = nlopt.opt(nlopt.GN_DIRECT_L_RAND, M) # for algorithms, see https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/\n",
    "            opt.set_lower_bounds(numpy.zeros(M))\n",
    "            opt.set_upper_bounds(numpy.ones(M))\n",
    "            opt.set_min_objective(f)\n",
    "            opt.set_maxeval(self.max_iter)\n",
    "            self.bestsoftlbl = opt.optimize(lblinit) # if function converged, #discriminative_likelihood_objective will raise and break out of nlopt\n",
    "            print(\" max_iter exceeded.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        self.bestsoftlbl = self.bestlbls # we rely on the self.bestlbls (in case nlopt is broken out of due to convergence)\n",
    "        ll = f(self.bestsoftlbl) # the minimized cost\n",
    "\n",
    "        # same as the middle section in #discriminative_likelihood, can be refactored\n",
    "        unlabeledy = (self.bestsoftlbl<0.5)*1\n",
    "        uweights = numpy.copy(self.bestsoftlbl) # large prob. for k=0 instances, small prob. for k=1 instances \n",
    "        uweights[unlabeledy==1] = 1-uweights[unlabeledy==1] # subtract from 1 for k=1 instances to reflect confidence\n",
    "        weights = numpy.hstack((numpy.ones(len(labeledy)), uweights))\n",
    "        labels = numpy.hstack((labeledy, unlabeledy))\n",
    "        if self.use_sample_weighting:\n",
    "            self.model.fit(numpy.vstack((labeledX, unlabeledX)), labels, sample_weight=weights)\n",
    "        else:\n",
    "            self.model.fit(numpy.vstack((labeledX, unlabeledX)), labels)\n",
    "        \n",
    "        if self.verbose > 1:\n",
    "            print(\"number of non-one soft labels: \", numpy.sum(self.bestsoftlbl != 1), \", balance:\", numpy.sum(self.bestsoftlbl<0.5), \" / \", len(self.bestsoftlbl))\n",
    "            print(\"current likelihood: \", ll)\n",
    "                        \n",
    "        return self\n",
    "    \n",
    "    # same as SelfLearning\n",
    "    def predict_proba(self, X):\n",
    "        if getattr(self.model, \"predict_proba\", None):\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            preds = self.model.predict(X)\n",
    "            return self.plattlr.predict_proba(preds.reshape( -1, 1 ))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        if self.predict_from_probabilities:\n",
    "            P = self.predict_proba(X)\n",
    "            return (P[:, 0]<numpy.average(P[:, 0]))\n",
    "        else:\n",
    "            return self.model.predict(X)\n",
    "    \n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return sklearn.metrics.accuracy_score(y, self.predict(X), sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unlabelled data - set all as unlabelled first\n",
    "ys = np.array([-1]*len(ytrue)) # -1 denotes unlabeled point\n",
    "\n",
    "labeled_N = int(20)\n",
    "# get N/2 points where y == 0 and N/2 points wherre y == 1\n",
    "random_labeled_points = random.sample(list(np.where(ytrue == 0)[0]), labeled_N//2) + random.sample(list(np.where(ytrue == 1)[0]), labeled_N//2)\n",
    "\n",
    "# add N points of labeled data into unlabelled data\n",
    "ys[random_labeled_points] = ytrue[random_labeled_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supervised log.reg. score 0.6835016835016835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "basemodel = SGDClassifier(loss='log', penalty='l1', max_iter=3000, tol=-np.infty)\n",
    "\n",
    "# (traditional) supervised training\n",
    "basemodel.fit(X[random_labeled_points, :], ys[random_labeled_points])\n",
    "print(\"supervised log.reg. score\", basemodel.score(X, ytrue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self-learning log.reg. score 0.7912457912457912\n"
     ]
    }
   ],
   "source": [
    "ssmodel = SelfLearningModel(basemodel)\n",
    "ssmodel.fit(X, ys)\n",
    "print(\"self-learning log.reg. score\", ssmodel.score(X, ytrue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 16.729070395850236 11.900967018887636 1.5601950233013433 0.053 True\n",
      "400 11.00189844671482 12.27011901260958 -0.9250046350379044 0.252 False\n",
      "600 16.569745915889758 11.751856281960618 0.07627891252135299 0.929 False\n",
      "800 14.983910424420595 12.538101681982004 0.2467096489438827 0.759 False\n",
      "1000 16.756779001060757 11.478232649010442 -0.44352213616023484 0.613 False\n",
      "1200 16.749851849758127 12.213713400222879 -0.14929121353569563 0.867 False\n",
      "1400 16.562818764587128 12.634333317882378 0.6790332566670365 0.406 False\n",
      "1600 16.071686006148294 12.508382352362128 0.9012125291777835 0.286 False\n",
      "1800 12.392096574400572 13.015612339906392 -0.620910611114855 0.396 False\n",
      "2000 16.583600218495018 12.75894476155293 -0.006108754162227115 0.994 False\n",
      " converged.\n",
      "number of non-one soft labels:  277 , balance: 4  /  277\n",
      "current likelihood:  16.853682263830954\n",
      "CPLE semi-supevised log.reg. score 0.6767676767676768\n"
     ]
    }
   ],
   "source": [
    "ssmodel = CPLELearningModel(basemodel, verbose=2)\n",
    "ssmodel.fit(X, ys)\n",
    "print(\"CPLE semi-supevised log.reg. score\", ssmodel.score(X, ytrue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPLE does not offer improvement over supervised on this dataset over this amount of iterations!! \n",
    "# (not to mention it's doesn't give t-verified improvements most of the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
