{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda build conda=4.6.8=py36_0; python v3.6 is needed for theano1.0.3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config theano to use GPU, must be done before theano is imported\n",
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda,mode=FAST_RUN,floatX=float32\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.gpuarray): Could not initialize pygpu, support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/theano/gpuarray/__init__.py\", line 227, in <module>\n",
      "    use(config.device)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/theano/gpuarray/__init__.py\", line 214, in use\n",
      "    init_dev(device, preallocate=preallocate)\n",
      "  File \"/anaconda3/lib/python3.6/site-packages/theano/gpuarray/__init__.py\", line 99, in init_dev\n",
      "    **args)\n",
      "  File \"pygpu/gpuarray.pyx\", line 658, in pygpu.gpuarray.init\n",
      "  File \"pygpu/gpuarray.pyx\", line 587, in pygpu.gpuarray.pygpu_init\n",
      "pygpu.gpuarray.GpuArrayException: b'Could not load \"/Library/Frameworks/CUDA.framework/CUDA\": dlopen(/Library/Frameworks/CUDA.framework/CUDA, 5): image not found'\n"
     ]
    }
   ],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2, 12, 22, 32, 42, 52, 62, 72, 82, 92]), array([ 11,  21,  31,  41,  51,  61,  71,  81,  91, 101])] 1 100\n",
      "[ 2  3  4  5  6  7  8  9 10 11] 1 100\n",
      "[] 1 1100\n",
      "[ 2  3  4  5  6  7  8  9 10 11] 1 1100\n",
      "[ 2  3  4  5  6  7  8  9 10 11] 2 100\n"
     ]
    }
   ],
   "source": [
    "# explanation of the theano.scan function\n",
    "# #scan(fn=func, output_infos=args of func, n_steps=n_iters)\n",
    "\n",
    "# for output_infos, it is either a single elem, or the length must equal to the output of func\n",
    "# output_infos stand for 1. the args pass into the func for the first iter; 2. the args that get returned from the func, which are used for subsequent iters\n",
    "\n",
    "# if the func takes in 1 arg but returns 2 args, output_infos=[pass_in, None] would pass the first output back into the func; output_infos=[None, pass_in] would pass the second output back into the func\n",
    "# outputs are all the outputs of the func for each iter, however note it groups each output elem into its own array\n",
    "# e.g. func returns [a,b]; outputs = [[n_steps of a], [n_steps of b]]\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "def func(x):\n",
    "    return [x+1, x+10]\n",
    "outputs, updates = theano.scan(func, outputs_info=[None, a], n_steps=10)\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())\n",
    "\n",
    "# scan expects the output of the function to be: \n",
    "# 1. the outputs, or \n",
    "# 2. a dict of the updates (each key of the dict must be a shared var, with the value instructions for how to update the var), or\n",
    "# 3. a tuple: (outputs, updates), note in this case, the outputs must be parenthesized (see example below)\n",
    "\n",
    "# in func1, a is passed as an arg into the func, but since the func does not return the updates dicts, 'a' itself is not updated\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "def func1(x):\n",
    "    return x+1\n",
    "outputs, updates = theano.scan(func1, outputs_info=a, n_steps=10)\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())\n",
    "\n",
    "# in func2, the updates dict is returned, and b is updated, but outputs is nil\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "def func2():\n",
    "    return {b: b+100}\n",
    "outputs, updates = theano.scan(func2, outputs_info=None, n_steps=10)\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())\n",
    "\n",
    "# in func3, both outputs and updates are returned, and b is updated as instructed by the updates dict\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "def func3(x):\n",
    "    return (x+1), {b: b+100} # the first elem must be within parenthesis\n",
    "outputs, updates = theano.scan(func3, outputs_info=a, n_steps=10)\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())\n",
    "\n",
    "# back to func1, #scan returns nil updates, but a is manually added into updates, so a is updated\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "outputs, updates = theano.scan(func1, outputs_info=a, n_steps=10)\n",
    "updates[a] = a+1\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "class RBM(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        hbias=None,\n",
    "        vbias=None,\n",
    "        numpy_rng=None,\n",
    "        theano_rng=None\n",
    "    ):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        # to generate random numbers in theano, a RandomStream need to initialized with a numpy rng\n",
    "        self.numpy_rng = numpy_rng or numpy.random.RandomState(1234)\n",
    "        self.theano_rng = theano_rng or RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        self.W = theano.shared(\n",
    "            value= W or self.initial_W(rng=self.numpy_rng, n_hidden=n_hidden, n_visible=n_visible), \n",
    "            name='W', \n",
    "            borrow=True\n",
    "        )\n",
    "        # hbias: an array of length n_hidden, for positive phase (forward prop)\n",
    "        self.hbias = hbias or self.bias_obj(n=n_hidden, name='hbias')\n",
    "        # vbias, an array of length n_visible, for negative phase (backward prop)\n",
    "        self.vbias = vbias or self.bias_obj(n=n_visible, name='vbias')\n",
    "        # initialize input layer for standalone RBM or layer0 of DBN\n",
    "        self.input = input or T.matrix('input')\n",
    "        # shared variables\n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "\n",
    "    def initial_W(self, rng=None, n_hidden=None, n_visible=None):\n",
    "        return numpy.asarray(\n",
    "            rng.uniform(\n",
    "                low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                size=(n_visible, n_hidden)\n",
    "            ),\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "    \n",
    "    def bias_obj(self, n=None, name=None):\n",
    "        return theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                n,\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name=name,\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "    # forward prop/positive phase: sigmoid(input * w + h_bias)\n",
    "    def propup(self, vis):\n",
    "        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    # backward prop/negative phase: sigmoid(hidden * w + v_bias)\n",
    "    def propdown(self, hid):\n",
    "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    # force propup to return a binomial layer\n",
    "    def sample_h_given_v(self, v0_sample):\n",
    "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
    "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
    "                                             n=1, p=h1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    # force propdown to return a binomial layer\n",
    "    def sample_v_given_h(self, h0_sample):\n",
    "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
    "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
    "                                             n=1, p=v1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    # gibbs sampling, using h0 (initial layer) to generate h1 (new layer)\n",
    "    def gibbs_hvh(self, h0_sample):\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
    "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    # gibbs sampling, using v0 to generate v1\n",
    "    def gibbs_vhv(self, v0_sample):\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample,\n",
    "                pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "    \n",
    "    # free energy is defined as: -v_bias*input - h_bias*hidden - hidden * weight * input\n",
    "    # I don't think there's a reasoning for this other than this is how it's defined\n",
    "    # which can be rewritten as -v_bias*input -sigma(log(1+ e^(h_bias + weight*input))) if both input & hidden consist of binomial nodes(either 1 or 0) only\n",
    "    # http://deeplearning.net/tutorial/rbm.html\n",
    "    def free_energy(self, v_sample):\n",
    "        wx_b = T.dot(v_sample, self.W) + self.hbias\n",
    "        vbias_term = T.dot(v_sample, self.vbias)\n",
    "        hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
    "        return -hidden_term - vbias_term\n",
    "\n",
    "    # persistent=None for Contrastive Divergence(CD) (default) to start gibbs sampling using the (hidden layer generated from the) input\n",
    "    # persistent=given sample for Persistant Contrastive Divergence(PCD) to start gibbs sampling using the given sample\n",
    "    # I believe hvh is used in this method instead of vhv (it is enough to calc cost by comparing just v0 to v1 o), because if persistent is used, hvh allows the chain to continue using the new h layer\n",
    "    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n",
    "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
    "        chain_start = persistent or ph_sample\n",
    "        (\n",
    "            [\n",
    "                pre_sigmoid_nvs,\n",
    "                nv_means,\n",
    "                nv_samples,\n",
    "                pre_sigmoid_nhs,\n",
    "                nh_means,\n",
    "                nh_samples\n",
    "            ],\n",
    "            updates\n",
    "        ) = theano.scan(\n",
    "            self.gibbs_hvh,\n",
    "            outputs_info=[None, None, None, None, None, chain_start],\n",
    "            n_steps=k\n",
    "        )\n",
    "\n",
    "        chain_end = nv_samples[-1]\n",
    "\n",
    "        cost = T.mean(self.free_energy(self.input)) - T.mean(self.free_energy(chain_end))\n",
    "        # We must not compute the gradient through the gibbs sampling\n",
    "        # gparams is an array of the differential of each of the shared varaibles(in self.params), ie. W, vbias, hbias\n",
    "        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n",
    "      \n",
    "        for gparam, param in zip(gparams, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - gparam * T.cast(lr, dtype=theano.config.floatX)\n",
    "       \n",
    "        if persistent:\n",
    "            updates[persistent] = nh_samples[-1] # this allows persistent(must be a shared var) to be updated\n",
    "            # pseudo-likelihood is a better proxy for PCD\n",
    "            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n",
    "        else:\n",
    "            # reconstruction cross-entropy is a better proxy for CD\n",
    "            monitoring_cost = self.get_reconstruction_cost(pre_sigmoid_nvs[-1])\n",
    "\n",
    "        return monitoring_cost, updates\n",
    "\n",
    "    # cost =(close to) N_bits * cost of one bit = N * e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i}))), where x_{\\i} is x_i with bit_i_idx flipped (1 to 0, or 0 to 1)\n",
    "    # bit_i_idx is randomly sampled, in this implementation, it simply loops through each bit per call\n",
    "    # note: bit_i_idx is added to the \n",
    "    def get_pseudo_likelihood_cost(self, updates):\n",
    "        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
    "\n",
    "        # binarize the input image by rounding to nearest integer\n",
    "        xi = T.round(self.input)\n",
    "\n",
    "        # calculate free energy for the given bit configuration\n",
    "        fe_xi = self.free_energy(xi)\n",
    "\n",
    "        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns to a new theano var\n",
    "        # this allows xi_flip to auto update when bit_i_idx updates\n",
    "        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
    "\n",
    "        # calculate free energy with bit flipped\n",
    "        fe_xi_flip = self.free_energy(xi_flip)\n",
    "\n",
    "        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n",
    "        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip - fe_xi)))\n",
    "\n",
    "        # increment bit_i_idx % number as part of updates\n",
    "        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def get_reconstruction_cost(self, pre_sigmoid_nv):\n",
    "        cross_entropy = T.mean(\n",
    "            T.sum(\n",
    "                self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n",
    "                (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to plot hidden layer\n",
    "\n",
    "import numpy\n",
    "\n",
    "\n",
    "def scale_to_unit_interval(ndar, eps=1e-8):\n",
    "    \"\"\" Scales all values in the ndarray ndar to be between 0 and 1 \"\"\"\n",
    "    ndar = ndar.copy()\n",
    "    ndar -= ndar.min()\n",
    "    ndar *= 1.0 / (ndar.max() + eps)\n",
    "    return ndar\n",
    "\n",
    "\n",
    "def tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0),\n",
    "                       scale_rows_to_unit_interval=True,\n",
    "                       output_pixel_vals=True):\n",
    "    \"\"\"\n",
    "    Transform an array with one flattened image per row, into an array in\n",
    "    which images are reshaped and layed out like tiles on a floor.\n",
    "\n",
    "    This function is useful for visualizing datasets whose rows are images,\n",
    "    and also columns of matrices for transforming those rows\n",
    "    (such as the first layer of a neural net).\n",
    "\n",
    "    :type X: a 2-D ndarray or a tuple of 4 channels, elements of which can\n",
    "    be 2-D ndarrays or None;\n",
    "    :param X: a 2-D array in which every row is a flattened image.\n",
    "\n",
    "    :type img_shape: tuple; (height, width)\n",
    "    :param img_shape: the original shape of each image\n",
    "\n",
    "    :type tile_shape: tuple; (rows, cols)\n",
    "    :param tile_shape: the number of images to tile (rows, cols)\n",
    "\n",
    "    :param output_pixel_vals: if output should be pixel values (i.e. int8\n",
    "    values) or floats\n",
    "\n",
    "    :param scale_rows_to_unit_interval: if the values need to be scaled before\n",
    "    being plotted to [0,1] or not\n",
    "\n",
    "\n",
    "    :returns: array suitable for viewing as an image.\n",
    "    (See:`Image.fromarray`.)\n",
    "    :rtype: a 2-d array with same dtype as X.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(img_shape) == 2\n",
    "    assert len(tile_shape) == 2\n",
    "    assert len(tile_spacing) == 2\n",
    "\n",
    "    # The expression below can be re-written in a more C style as\n",
    "    # follows :\n",
    "    #\n",
    "    # out_shape    = [0,0]\n",
    "    # out_shape[0] = (img_shape[0]+tile_spacing[0])*tile_shape[0] -\n",
    "    #                tile_spacing[0]\n",
    "    # out_shape[1] = (img_shape[1]+tile_spacing[1])*tile_shape[1] -\n",
    "    #                tile_spacing[1]\n",
    "    out_shape = [\n",
    "        (ishp + tsp) * tshp - tsp\n",
    "        for ishp, tshp, tsp in zip(img_shape, tile_shape, tile_spacing)\n",
    "    ]\n",
    "\n",
    "    if isinstance(X, tuple):\n",
    "        assert len(X) == 4\n",
    "        # Create an output numpy ndarray to store the image\n",
    "        if output_pixel_vals:\n",
    "            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n",
    "                                    dtype='uint8')\n",
    "        else:\n",
    "            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n",
    "                                    dtype=X.dtype)\n",
    "\n",
    "        #colors default to 0, alpha defaults to 1 (opaque)\n",
    "        if output_pixel_vals:\n",
    "            channel_defaults = [0, 0, 0, 255]\n",
    "        else:\n",
    "            channel_defaults = [0., 0., 0., 1.]\n",
    "\n",
    "        for i in range(4):\n",
    "            if X[i] is None:\n",
    "                # if channel is None, fill it with zeros of the correct\n",
    "                # dtype\n",
    "                dt = out_array.dtype\n",
    "                if output_pixel_vals:\n",
    "                    dt = 'uint8'\n",
    "                out_array[:, :, i] = numpy.zeros(\n",
    "                    out_shape,\n",
    "                    dtype=dt\n",
    "                ) + channel_defaults[i]\n",
    "            else:\n",
    "                # use a recurrent call to compute the channel and store it\n",
    "                # in the output\n",
    "                out_array[:, :, i] = tile_raster_images(\n",
    "                    X[i], img_shape, tile_shape, tile_spacing,\n",
    "                    scale_rows_to_unit_interval, output_pixel_vals)\n",
    "        return out_array\n",
    "\n",
    "    else:\n",
    "        # if we are dealing with only one channel\n",
    "        H, W = img_shape\n",
    "        Hs, Ws = tile_spacing\n",
    "\n",
    "        # generate a matrix to store the output\n",
    "        dt = X.dtype\n",
    "        if output_pixel_vals:\n",
    "            dt = 'uint8'\n",
    "        out_array = numpy.zeros(out_shape, dtype=dt)\n",
    "\n",
    "        for tile_row in range(tile_shape[0]):\n",
    "            for tile_col in range(tile_shape[1]):\n",
    "                if tile_row * tile_shape[1] + tile_col < X.shape[0]:\n",
    "                    this_x = X[tile_row * tile_shape[1] + tile_col]\n",
    "                    if scale_rows_to_unit_interval:\n",
    "                        # if we should scale values to be between 0 and 1\n",
    "                        # do this by calling the `scale_to_unit_interval`\n",
    "                        # function\n",
    "                        this_img = scale_to_unit_interval(\n",
    "                            this_x.reshape(img_shape))\n",
    "                    else:\n",
    "                        this_img = this_x.reshape(img_shape)\n",
    "                    # add the slice to the corresponding position in the\n",
    "                    # output array\n",
    "                    c = 1\n",
    "                    if output_pixel_vals:\n",
    "                        c = 255\n",
    "                    out_array[\n",
    "                        tile_row * (H + Hs): tile_row * (H + Hs) + H,\n",
    "                        tile_col * (W + Ws): tile_col * (W + Ws) + W\n",
    "                    ] = this_img * c\n",
    "        return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data(dataset):\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "datasets = load_data('mnist.pkl.gz')\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "test_set_x, test_set_y = datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "import PIL.Image as Image\n",
    "\n",
    "# specific to training mnist from a zip file of the data\n",
    "def test_rbm(\n",
    "    X=None,\n",
    "    learning_rate=0.1, \n",
    "    training_epochs=15,\n",
    "    batch_size=20,\n",
    "    output_folder='rbm_plots',\n",
    "    n_hidden=500\n",
    "):\n",
    "    # init var\n",
    "    index = T.lscalar()    # index of [mini]batch\n",
    "    train = T.matrix('x')\n",
    "    x = train[index * batch_size : (index + 1) * batch_size] # batch, where each batch has batch_size number of rows\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "    # each row of the chain will store a hidden sample(layer)\n",
    "    persistent_chain = theano.shared(\n",
    "        numpy.zeros(\n",
    "            (batch_size, n_hidden),\n",
    "            dtype=theano.config.floatX\n",
    "        ),\n",
    "        borrow=True\n",
    "    )\n",
    "    rbm = RBM(\n",
    "        input=x, \n",
    "        n_visible=28 * 28, # dimensions of the mnist data\n",
    "        n_hidden=n_hidden, \n",
    "        numpy_rng=rng, \n",
    "        theano_rng=theano_rng\n",
    "    )\n",
    "    cost, updates = rbm.get_cost_updates(\n",
    "        lr=learning_rate,\n",
    "        persistent=persistent_chain, \n",
    "#         persistent=None, \n",
    "        k=15\n",
    "    )\n",
    "    \n",
    "    # go into folder to save plots\n",
    "#     if not os.path.isdir(output_folder):\n",
    "#         os.makedirs(output_folder)\n",
    "#     os.chdir(output_folder)\n",
    "    \n",
    "    # define theano function\n",
    "    train_rbm = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            train: X # defined outside function\n",
    "        },\n",
    "        name='train_rbm'\n",
    "    )\n",
    "    \n",
    "    # start training\n",
    "    plotting_time = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    n_train_batches = int(X.shape[0] / batch_size)\n",
    "    for epoch in range(training_epochs):\n",
    "        # go through the training set\n",
    "        mean_cost = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            mean_cost += [train_rbm(batch_index)]\n",
    "\n",
    "        print('Training epoch %d, cost is ' % epoch, numpy.mean(mean_cost))\n",
    "\n",
    "        # Plot filters after each training epoch\n",
    "        plotting_start = timeit.default_timer()\n",
    "#         # Construct image from the weight matrix\n",
    "#         image = Image.fromarray(\n",
    "#             tile_raster_images(\n",
    "#                 X=rbm.W.get_value(borrow=True).T,\n",
    "#                 img_shape=(28, 28),\n",
    "#                 tile_shape=(10, 10),\n",
    "#                 tile_spacing=(1, 1)\n",
    "#             )\n",
    "#         )\n",
    "#         image.save('filters_at_epoch_%i.png' % epoch)\n",
    "        plotting_stop = timeit.default_timer()\n",
    "        plotting_time += (plotting_stop - plotting_start)\n",
    "\n",
    "    # calculate time of execution\n",
    "    end_time = timeit.default_timer()\n",
    "    pretraining_time = (end_time - start_time) - plotting_time\n",
    "    print ('Training took %f minutes' % (pretraining_time / 60.))\n",
    "    \n",
    "    return rbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-5a4d18a06cfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# with (persistant) CD-k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_rbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-2353c2b9f0fc>\u001b[0m in \u001b[0;36mtest_rbm\u001b[0;34m(X, learning_rate, training_epochs, batch_size, output_folder, n_hidden)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmean_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mmean_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_rbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training epoch %d, cost is '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/tensor/raw_random.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inputs, out_)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mrout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         if (not isinstance(rval, np.ndarray) or\n\u001b[1;32m    262\u001b[0m                 str(rval.dtype) != node.outputs[1].type.dtype):\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.binomial\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36many\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2083\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0marray_function_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_any_dispatcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2084\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with (persistant) CD-k\n",
    "rbm = test_rbm(train_set_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(rbm, n_chains=20, n_samples=10):\n",
    "    #### sampling from the trained rbm\n",
    "    n_test = test_set_x.shape[0]\n",
    "    rng = numpy.random.RandomState(123)\n",
    "\n",
    "    # pick random test examples, with which to initialize the persistent chain\n",
    "    test_idx = rng.randint(n_test - n_chains)\n",
    "    persistent_vis_chain = theano.shared(\n",
    "        numpy.asarray(\n",
    "            test_set_x[test_idx:test_idx + n_chains],\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "    )\n",
    "    print(test_set_y[test_idx:test_idx + n_chains])\n",
    "\n",
    "    # pass back 1000 times\n",
    "    plot_every = 1000\n",
    "    (\n",
    "        [\n",
    "            presig_hids,\n",
    "            hid_mfs,\n",
    "            hid_samples,\n",
    "            presig_vis,\n",
    "            vis_mfs,\n",
    "            vis_samples\n",
    "        ],\n",
    "        updates\n",
    "    ) = theano.scan(\n",
    "        rbm.gibbs_vhv,\n",
    "        outputs_info=[None, None, None, None, None, persistent_vis_chain],\n",
    "        n_steps=plot_every\n",
    "    )\n",
    "\n",
    "    updates.update({persistent_vis_chain: vis_samples[-1]}) # so in the loop below, when scan is called, persistent_vis_chain will be updated to vis_samples[-1]\n",
    "    \n",
    "    # execute\n",
    "    sample_fn = theano.function(\n",
    "        [],\n",
    "        [\n",
    "            vis_mfs[-1],\n",
    "            vis_samples[-1]\n",
    "        ],\n",
    "        updates=updates,\n",
    "        name='sample_fn'\n",
    "    )\n",
    "\n",
    "    # create a space to store the image for plotting; 29 because x:(28,28) + 1 for separation\n",
    "    image_data = numpy.zeros(\n",
    "        (29 * n_samples + 1, 29 * n_chains - 1),\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    for idx in range(n_samples):\n",
    "        # for every loop, sample_fn is called, passing the data through the rbm for 1000 more times\n",
    "        # only the last sample generated is plot\n",
    "        vis_mf, vis_sample = sample_fn()\n",
    "        print(' ... plotting sample ', idx) # note: each sample is a layer, not a row; only 20 rows are plotted\n",
    "        image_data[29 * idx:29 * idx + 28, :] = tile_raster_images(\n",
    "            X=vis_mf,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(1, n_chains),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "\n",
    "    image = Image.fromarray(image_data)\n",
    "    image.save('samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 8 0 7 1 9 8 7 5 5 9 1 7 5 4 9 1 2 2 1]\n",
      " ... plotting sample  0\n",
      " ... plotting sample  1\n",
      " ... plotting sample  2\n",
      " ... plotting sample  3\n",
      " ... plotting sample  4\n",
      " ... plotting sample  5\n",
      " ... plotting sample  6\n",
      " ... plotting sample  7\n",
      " ... plotting sample  8\n",
      " ... plotting sample  9\n"
     ]
    }
   ],
   "source": [
    "sample(rbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
