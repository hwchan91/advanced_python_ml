{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda build conda=4.6.8=py36_0; python v3.6 is needed for theano1.0.3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config theano to use GPU, must be done before theano is imported\n",
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda,mode=FAST_RUN,floatX=float32\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2, 12, 22, 32, 42, 52, 62, 72, 82, 92]), array([ 11,  21,  31,  41,  51,  61,  71,  81,  91, 101])] 1 100\n",
      "[ 2  3  4  5  6  7  8  9 10 11] 1 100\n",
      "[] 1 1100\n",
      "[ 2  3  4  5  6  7  8  9 10 11] 1 1100\n",
      "[ 2  3  4  5  6  7  8  9 10 11] 1 200\n"
     ]
    }
   ],
   "source": [
    "# explanation of the theano.scan function\n",
    "# #scan(fn=func, output_infos=args of func, n_steps=n_iters)\n",
    "\n",
    "# for output_infos, it is either a single elem, or the length must equal to the output of func\n",
    "# output_infos stand for 1. the args pass into the func for the first iter; 2. the args that get returned from the func, which are used for subsequent iters\n",
    "\n",
    "# if the func takes in 1 arg but returns 2 args, output_infos=[pass_in, None] would pass the first output back into the func; output_infos=[None, pass_in] would pass the second output back into the func\n",
    "# outputs are all the outputs of the func for each iter, however note it groups each output elem into its own array\n",
    "# e.g. func returns [a,b]; outputs = [[n_steps of a], [n_steps of b]]\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "def func(x):\n",
    "    return [x+1, x+10]\n",
    "outputs, updates = theano.scan(func, outputs_info=[None, a], n_steps=10)\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())\n",
    "\n",
    "# scan expects the output of the function to be: \n",
    "# 1. the outputs, or \n",
    "# 2. a dict of the updates (each key of the dict must be a shared var, with the value instructions for how to update the var), or\n",
    "# 3. a tuple: (outputs, updates), note in this case, the outputs must be parenthesized (see example below)\n",
    "\n",
    "# in func1, a is passed as an arg into the func, but since the func does not return the updates dicts, 'a' itself is not updated\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "def func1(x):\n",
    "    return x+1\n",
    "outputs, updates = theano.scan(func1, outputs_info=a, n_steps=10)\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())\n",
    "\n",
    "# in func2, the updates dict is returned, and b is updated, but outputs is nil\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "def func2():\n",
    "    return {b: b+100}\n",
    "outputs, updates = theano.scan(func2, outputs_info=None, n_steps=10)\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())\n",
    "\n",
    "# in func3, both outputs and updates are returned, and b is updated as instructed by the updates dict\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "def func3(x):\n",
    "    return (x+1), {b: b+100} # the first elem must be within parenthesis\n",
    "outputs, updates = theano.scan(func3, outputs_info=a, n_steps=10)\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())\n",
    "\n",
    "# back to func1, #scan returns nil updates, but b is manually added into updates, so b is updated\n",
    "# also note that when updates is returned natively from the scan func, the update is pefromed n_steps times per function call; \n",
    "# but when updates is added outside the func, it is only performed once per function call\n",
    "a = theano.shared(1)\n",
    "b = theano.shared(100)\n",
    "outputs, updates = theano.scan(func1, outputs_info=a, n_steps=10)\n",
    "updates[b] = b+100\n",
    "f = theano.function([], outputs=outputs, updates=updates)\n",
    "print(f(), a.get_value(), b.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "class RBM(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        hbias=None,\n",
    "        vbias=None,\n",
    "        numpy_rng=None,\n",
    "        theano_rng=None\n",
    "    ):\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "        # to generate random numbers in theano, a RandomStream need to initialized with a numpy rng\n",
    "        self.numpy_rng = numpy_rng or numpy.random.RandomState(1234)\n",
    "        self.theano_rng = theano_rng or RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        self.W = W or self.initial_W(rng=self.numpy_rng, n_hidden=n_hidden, n_visible=n_visible)\n",
    "        # hbias: an array of length n_hidden, for positive phase (forward prop)\n",
    "        self.hbias = hbias or self.bias_obj(n=n_hidden, name='hbias')\n",
    "        # vbias, an array of length n_visible, for negative phase (backward prop)\n",
    "        self.vbias = vbias or self.bias_obj(n=n_visible, name='vbias')\n",
    "        # initialize input layer for standalone RBM or layer0 of DBN\n",
    "        self.input = input or T.matrix('input')\n",
    "        # shared variables\n",
    "        self.params = [self.W, self.hbias, self.vbias]\n",
    "\n",
    "    def initial_W(self, rng=None, n_hidden=None, n_visible=None):\n",
    "        W = numpy.asarray(\n",
    "            rng.uniform(\n",
    "                low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                size=(n_visible, n_hidden)\n",
    "            ),\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "        return theano.shared(\n",
    "            value= W, \n",
    "            name='W', \n",
    "            borrow=True\n",
    "        )\n",
    "    \n",
    "    def bias_obj(self, n=None, name=None):\n",
    "        return theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                n,\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name=name,\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "    # forward prop/positive phase: sigmoid(input * w + h_bias)\n",
    "    def propup(self, vis):\n",
    "        pre_sigmoid_activation = T.dot(vis, self.W) + self.hbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    # backward prop/negative phase: sigmoid(hidden * w + v_bias)\n",
    "    def propdown(self, hid):\n",
    "        pre_sigmoid_activation = T.dot(hid, self.W.T) + self.vbias\n",
    "        return [pre_sigmoid_activation, T.nnet.sigmoid(pre_sigmoid_activation)]\n",
    "\n",
    "    # force propup to return a binomial layer\n",
    "    def sample_h_given_v(self, v0_sample):\n",
    "        pre_sigmoid_h1, h1_mean = self.propup(v0_sample)\n",
    "        h1_sample = self.theano_rng.binomial(size=h1_mean.shape,\n",
    "                                             n=1, p=h1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    # force propdown to return a binomial layer\n",
    "    def sample_v_given_h(self, h0_sample):\n",
    "        pre_sigmoid_v1, v1_mean = self.propdown(h0_sample)\n",
    "        v1_sample = self.theano_rng.binomial(size=v1_mean.shape,\n",
    "                                             n=1, p=v1_mean,\n",
    "                                             dtype=theano.config.floatX)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "\n",
    "    # gibbs sampling, using h0 (initial layer) to generate h1 (new layer)\n",
    "    def gibbs_hvh(self, h0_sample):\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h0_sample)\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v1_sample)\n",
    "        return [pre_sigmoid_v1, v1_mean, v1_sample,\n",
    "                pre_sigmoid_h1, h1_mean, h1_sample]\n",
    "\n",
    "    # gibbs sampling, using v0 to generate v1\n",
    "    def gibbs_vhv(self, v0_sample):\n",
    "        pre_sigmoid_h1, h1_mean, h1_sample = self.sample_h_given_v(v0_sample)\n",
    "        pre_sigmoid_v1, v1_mean, v1_sample = self.sample_v_given_h(h1_sample)\n",
    "        return [pre_sigmoid_h1, h1_mean, h1_sample,\n",
    "                pre_sigmoid_v1, v1_mean, v1_sample]\n",
    "    \n",
    "    # free energy is defined as: -v_bias*input - h_bias*hidden - hidden * weight * input\n",
    "    # I don't think there's a reasoning for this other than this is how it's defined\n",
    "    # which can be rewritten as -v_bias*input -sigma(log(1+ e^(h_bias + weight*input))) if both input & hidden consist of binomial nodes(either 1 or 0) only\n",
    "    # http://deeplearning.net/tutorial/rbm.html\n",
    "    def free_energy(self, v_sample):\n",
    "        wx_b = T.dot(v_sample, self.W) + self.hbias\n",
    "        vbias_term = T.dot(v_sample, self.vbias)\n",
    "        hidden_term = T.sum(T.log(1 + T.exp(wx_b)), axis=1)\n",
    "        return -hidden_term - vbias_term\n",
    "\n",
    "    # persistent=None for Contrastive Divergence(CD) (default) to start gibbs sampling using the (hidden layer generated from the) input\n",
    "    # persistent=given sample for Persistant Contrastive Divergence(PCD) to start gibbs sampling using the previous data point in the persistant chain\n",
    "    def get_cost_updates(self, lr=0.1, persistent=None, k=1):\n",
    "        pre_sigmoid_ph, ph_mean, ph_sample = self.sample_h_given_v(self.input)\n",
    "        chain_start = persistent or ph_sample\n",
    "        (\n",
    "            [\n",
    "                pre_sigmoid_nvs,\n",
    "                nv_means,\n",
    "                nv_samples,\n",
    "                pre_sigmoid_nhs,\n",
    "                nh_means,\n",
    "                nh_samples\n",
    "            ],\n",
    "            updates\n",
    "        ) = theano.scan(\n",
    "            self.gibbs_hvh,\n",
    "            outputs_info=[None, None, None, None, None, chain_start],\n",
    "            n_steps=k\n",
    "        )\n",
    "\n",
    "        chain_end = nv_samples[-1]\n",
    "\n",
    "        cost = T.mean(self.free_energy(self.input)) - T.mean(self.free_energy(chain_end))\n",
    "        # We must not compute the gradient through the gibbs sampling\n",
    "        # gparams is an array of the differential of each of the shared varaibles(in self.params), ie. W, vbias, hbias\n",
    "        gparams = T.grad(cost, self.params, consider_constant=[chain_end])\n",
    "      \n",
    "        # since update vars are added outside of the scan func, ie. hvh, the gradient is updated after every CD-k (not k times every CD-1)\n",
    "        for gparam, param in zip(gparams, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - gparam * T.cast(lr, dtype=theano.config.floatX)\n",
    "       \n",
    "        if persistent:\n",
    "            updates[persistent] = nh_samples[-1] # this allows persistent(must be a shared var) to be updated\n",
    "            # pseudo-likelihood is a better proxy for PCD\n",
    "            monitoring_cost = self.get_pseudo_likelihood_cost(updates)\n",
    "        else:\n",
    "            # reconstruction cross-entropy is a better proxy for CD\n",
    "            monitoring_cost = self.get_reconstruction_cost(pre_sigmoid_nvs[-1])\n",
    "\n",
    "        return monitoring_cost, updates\n",
    "\n",
    "    # cost =(close to) N_bits * cost of one bit = N * e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i}))), where x_{\\i} is x_i with bit_i_idx flipped (1 to 0, or 0 to 1)\n",
    "    # bit_i_idx is randomly sampled, in this implementation, it simply loops through each bit per call\n",
    "    # note: bit_i_idx is added to the \n",
    "    def get_pseudo_likelihood_cost(self, updates):\n",
    "        bit_i_idx = theano.shared(value=0, name='bit_i_idx')\n",
    "\n",
    "        # binarize the input image by rounding to nearest integer\n",
    "        xi = T.round(self.input)\n",
    "\n",
    "        # calculate free energy for the given bit configuration\n",
    "        fe_xi = self.free_energy(xi)\n",
    "\n",
    "        # Equivalent to xi[:,bit_i_idx] = 1-xi[:, bit_i_idx], but assigns to a new theano var\n",
    "        # this allows xi_flip to auto update when bit_i_idx updates\n",
    "        xi_flip = T.set_subtensor(xi[:, bit_i_idx], 1 - xi[:, bit_i_idx])\n",
    "\n",
    "        # calculate free energy with bit flipped\n",
    "        fe_xi_flip = self.free_energy(xi_flip)\n",
    "\n",
    "        # equivalent to e^(-FE(x_i)) / (e^(-FE(x_i)) + e^(-FE(x_{\\i})))\n",
    "        cost = T.mean(self.n_visible * T.log(T.nnet.sigmoid(fe_xi_flip - fe_xi)))\n",
    "\n",
    "        # increment bit_i_idx % number as part of updates\n",
    "        updates[bit_i_idx] = (bit_i_idx + 1) % self.n_visible\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def get_reconstruction_cost(self, pre_sigmoid_nv):\n",
    "        cross_entropy = T.mean(\n",
    "            T.sum(\n",
    "                self.input * T.log(T.nnet.sigmoid(pre_sigmoid_nv)) +\n",
    "                (1 - self.input) * T.log(1 - T.nnet.sigmoid(pre_sigmoid_nv)),\n",
    "                axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method to plot hidden layer\n",
    "\n",
    "import numpy\n",
    "\n",
    "\n",
    "def scale_to_unit_interval(ndar, eps=1e-8):\n",
    "    \"\"\" Scales all values in the ndarray ndar to be between 0 and 1 \"\"\"\n",
    "    ndar = ndar.copy()\n",
    "    ndar -= ndar.min()\n",
    "    ndar *= 1.0 / (ndar.max() + eps)\n",
    "    return ndar\n",
    "\n",
    "\n",
    "def tile_raster_images(X, img_shape, tile_shape, tile_spacing=(0, 0),\n",
    "                       scale_rows_to_unit_interval=True,\n",
    "                       output_pixel_vals=True):\n",
    "    \"\"\"\n",
    "    Transform an array with one flattened image per row, into an array in\n",
    "    which images are reshaped and layed out like tiles on a floor.\n",
    "\n",
    "    This function is useful for visualizing datasets whose rows are images,\n",
    "    and also columns of matrices for transforming those rows\n",
    "    (such as the first layer of a neural net).\n",
    "\n",
    "    :type X: a 2-D ndarray or a tuple of 4 channels, elements of which can\n",
    "    be 2-D ndarrays or None;\n",
    "    :param X: a 2-D array in which every row is a flattened image.\n",
    "\n",
    "    :type img_shape: tuple; (height, width)\n",
    "    :param img_shape: the original shape of each image\n",
    "\n",
    "    :type tile_shape: tuple; (rows, cols)\n",
    "    :param tile_shape: the number of images to tile (rows, cols)\n",
    "\n",
    "    :param output_pixel_vals: if output should be pixel values (i.e. int8\n",
    "    values) or floats\n",
    "\n",
    "    :param scale_rows_to_unit_interval: if the values need to be scaled before\n",
    "    being plotted to [0,1] or not\n",
    "\n",
    "\n",
    "    :returns: array suitable for viewing as an image.\n",
    "    (See:`Image.fromarray`.)\n",
    "    :rtype: a 2-d array with same dtype as X.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(img_shape) == 2\n",
    "    assert len(tile_shape) == 2\n",
    "    assert len(tile_spacing) == 2\n",
    "\n",
    "    # The expression below can be re-written in a more C style as\n",
    "    # follows :\n",
    "    #\n",
    "    # out_shape    = [0,0]\n",
    "    # out_shape[0] = (img_shape[0]+tile_spacing[0])*tile_shape[0] -\n",
    "    #                tile_spacing[0]\n",
    "    # out_shape[1] = (img_shape[1]+tile_spacing[1])*tile_shape[1] -\n",
    "    #                tile_spacing[1]\n",
    "    out_shape = [\n",
    "        (ishp + tsp) * tshp - tsp\n",
    "        for ishp, tshp, tsp in zip(img_shape, tile_shape, tile_spacing)\n",
    "    ]\n",
    "\n",
    "    if isinstance(X, tuple):\n",
    "        assert len(X) == 4\n",
    "        # Create an output numpy ndarray to store the image\n",
    "        if output_pixel_vals:\n",
    "            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n",
    "                                    dtype='uint8')\n",
    "        else:\n",
    "            out_array = numpy.zeros((out_shape[0], out_shape[1], 4),\n",
    "                                    dtype=X.dtype)\n",
    "\n",
    "        #colors default to 0, alpha defaults to 1 (opaque)\n",
    "        if output_pixel_vals:\n",
    "            channel_defaults = [0, 0, 0, 255]\n",
    "        else:\n",
    "            channel_defaults = [0., 0., 0., 1.]\n",
    "\n",
    "        for i in range(4):\n",
    "            if X[i] is None:\n",
    "                # if channel is None, fill it with zeros of the correct\n",
    "                # dtype\n",
    "                dt = out_array.dtype\n",
    "                if output_pixel_vals:\n",
    "                    dt = 'uint8'\n",
    "                out_array[:, :, i] = numpy.zeros(\n",
    "                    out_shape,\n",
    "                    dtype=dt\n",
    "                ) + channel_defaults[i]\n",
    "            else:\n",
    "                # use a recurrent call to compute the channel and store it\n",
    "                # in the output\n",
    "                out_array[:, :, i] = tile_raster_images(\n",
    "                    X[i], img_shape, tile_shape, tile_spacing,\n",
    "                    scale_rows_to_unit_interval, output_pixel_vals)\n",
    "        return out_array\n",
    "\n",
    "    else:\n",
    "        # if we are dealing with only one channel\n",
    "        H, W = img_shape\n",
    "        Hs, Ws = tile_spacing\n",
    "\n",
    "        # generate a matrix to store the output\n",
    "        dt = X.dtype\n",
    "        if output_pixel_vals:\n",
    "            dt = 'uint8'\n",
    "        out_array = numpy.zeros(out_shape, dtype=dt)\n",
    "\n",
    "        for tile_row in range(tile_shape[0]):\n",
    "            for tile_col in range(tile_shape[1]):\n",
    "                if tile_row * tile_shape[1] + tile_col < X.shape[0]:\n",
    "                    this_x = X[tile_row * tile_shape[1] + tile_col]\n",
    "                    if scale_rows_to_unit_interval:\n",
    "                        # if we should scale values to be between 0 and 1\n",
    "                        # do this by calling the `scale_to_unit_interval`\n",
    "                        # function\n",
    "                        this_img = scale_to_unit_interval(\n",
    "                            this_x.reshape(img_shape))\n",
    "                    else:\n",
    "                        this_img = this_x.reshape(img_shape)\n",
    "                    # add the slice to the corresponding position in the\n",
    "                    # output array\n",
    "                    c = 1\n",
    "                    if output_pixel_vals:\n",
    "                        c = 255\n",
    "                    out_array[\n",
    "                        tile_row * (H + Hs): tile_row * (H + Hs) + H,\n",
    "                        tile_col * (W + Ws): tile_col * (W + Ws) + W\n",
    "                    ] = this_img * c\n",
    "        return out_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "def load_data(dataset):\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = pickle.load(f,encoding='latin1')\n",
    "    f.close()\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "datasets = load_data('mnist.pkl.gz')\n",
    "train_set_x, train_set_y = datasets[0]\n",
    "valid_set_x, valid_set_y = datasets[1]\n",
    "test_set_x,  test_set_y  = datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "import PIL.Image as Image\n",
    "\n",
    "# specific to training mnist from a zip file of the data\n",
    "def test_rbm(\n",
    "    X=None,\n",
    "    learning_rate=0.1, \n",
    "    training_epochs=15,\n",
    "    batch_size=20,\n",
    "    output_folder='rbm_plots',\n",
    "    n_hidden=500\n",
    "):\n",
    "    # init var\n",
    "    index = T.lscalar()    # index of [mini]batch\n",
    "    train = T.matrix('x')\n",
    "    x = train[index * batch_size : (index + 1) * batch_size] # batch, where each batch has batch_size number of rows\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "    # each row of the chain will store a hidden sample(layer)\n",
    "    persistent_chain = theano.shared(\n",
    "        numpy.zeros(\n",
    "            (batch_size, n_hidden),\n",
    "            dtype=theano.config.floatX\n",
    "        ),\n",
    "        borrow=True\n",
    "    )\n",
    "    rbm = RBM(\n",
    "        input=x, \n",
    "        n_visible=28 * 28, # dimensions of the mnist data\n",
    "        n_hidden=n_hidden, \n",
    "        numpy_rng=rng, \n",
    "        theano_rng=theano_rng\n",
    "    )\n",
    "    cost, updates = rbm.get_cost_updates(\n",
    "        lr=learning_rate,\n",
    "        persistent=persistent_chain, \n",
    "#         persistent=None, \n",
    "        k=15\n",
    "    )\n",
    "    \n",
    "    # go into folder to save plots\n",
    "#     if not os.path.isdir(output_folder):\n",
    "#         os.makedirs(output_folder)\n",
    "#     os.chdir(output_folder)\n",
    "    \n",
    "    # define theano function\n",
    "    train_rbm = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            train: X\n",
    "        },\n",
    "        name='train_rbm'\n",
    "    )\n",
    "    \n",
    "    # start training\n",
    "    plotting_time = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    n_train_batches = int(X.shape[0] / batch_size)\n",
    "    for epoch in range(training_epochs):\n",
    "        # go through the training set\n",
    "        mean_cost = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            mean_cost += [train_rbm(batch_index)]\n",
    "\n",
    "        print('Training epoch %d, cost is ' % epoch, numpy.mean(mean_cost))\n",
    "\n",
    "        # Plot filters after each training epoch\n",
    "        plotting_start = timeit.default_timer()\n",
    "#         # Construct image from the weight matrix\n",
    "#         image = Image.fromarray(\n",
    "#             tile_raster_images(\n",
    "#                 X=rbm.W.get_value(borrow=True).T,\n",
    "#                 img_shape=(28, 28),\n",
    "#                 tile_shape=(10, 10),\n",
    "#                 tile_spacing=(1, 1)\n",
    "#             )\n",
    "#         )\n",
    "#         image.save('filters_at_epoch_%i.png' % epoch)\n",
    "        plotting_stop = timeit.default_timer()\n",
    "        plotting_time += (plotting_stop - plotting_start)\n",
    "\n",
    "    # calculate time of execution\n",
    "    end_time = timeit.default_timer()\n",
    "    pretraining_time = (end_time - start_time) - plotting_time\n",
    "    print ('Training took %f minutes' % (pretraining_time / 60.))\n",
    "    \n",
    "    return rbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-5a4d18a06cfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# with (persistant) CD-k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_rbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-5ec2eb77b2df>\u001b[0m in \u001b[0;36mtest_rbm\u001b[0;34m(X, learning_rate, training_epochs, batch_size, output_folder, n_hidden)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mmean_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mmean_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_rbm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training epoch %d, cost is '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    961\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[1;32m    962\u001b[0m                  allow_gc=allow_gc):\n\u001b[0;32m--> 963\u001b[0;31m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mp\u001b[0;34m(node, args, outs)\u001b[0m\n\u001b[1;32m    950\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m                                                 self, node)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mscan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/tensor/raw_random.py\u001b[0m in \u001b[0;36mperform\u001b[0;34m(self, node, inputs, out_)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mrout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         if (not isinstance(rval, np.ndarray) or\n\u001b[1;32m    262\u001b[0m                 str(rval.dtype) != node.outputs[1].type.dtype):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# with (persistant) CD-k\n",
    "rbm = test_rbm(train_set_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(rbm, n_chains=20, n_samples=10):\n",
    "    #### sampling from the trained rbm\n",
    "    n_test = test_set_x.shape[0]\n",
    "    rng = numpy.random.RandomState(123)\n",
    "\n",
    "    # pick random test examples, with which to initialize the persistent chain\n",
    "    test_idx = rng.randint(n_test - n_chains)\n",
    "    persistent_vis_chain = theano.shared(\n",
    "        numpy.asarray(\n",
    "            test_set_x[test_idx:test_idx + n_chains],\n",
    "            dtype=theano.config.floatX\n",
    "        )\n",
    "    )\n",
    "    print(test_set_y[test_idx:test_idx + n_chains])\n",
    "\n",
    "    # pass back 1000 times\n",
    "    plot_every = 1000\n",
    "    (\n",
    "        [\n",
    "            presig_hids,\n",
    "            hid_mfs,\n",
    "            hid_samples,\n",
    "            presig_vis,\n",
    "            vis_mfs,\n",
    "            vis_samples\n",
    "        ],\n",
    "        updates\n",
    "    ) = theano.scan(\n",
    "        rbm.gibbs_vhv,\n",
    "        outputs_info=[None, None, None, None, None, persistent_vis_chain],\n",
    "        n_steps=plot_every\n",
    "    )\n",
    "\n",
    "    updates.update({persistent_vis_chain: vis_samples[-1]}) # so in the loop below, when scan is called, persistent_vis_chain will be updated to vis_samples[-1]\n",
    "    \n",
    "    # execute\n",
    "    sample_fn = theano.function(\n",
    "        [],\n",
    "        [\n",
    "            vis_mfs[-1],\n",
    "            vis_samples[-1]\n",
    "        ],\n",
    "        updates=updates,\n",
    "        name='sample_fn'\n",
    "    )\n",
    "\n",
    "    # create a space to store the image for plotting; 29 because x:(28,28) + 1 for separation\n",
    "    image_data = numpy.zeros(\n",
    "        (29 * n_samples + 1, 29 * n_chains - 1),\n",
    "        dtype='uint8'\n",
    "    )\n",
    "    for idx in range(n_samples):\n",
    "        # for every loop, sample_fn is called, passing the data through the rbm for 1000 more times\n",
    "        # only the last sample generated is plot\n",
    "        vis_mf, vis_sample = sample_fn()\n",
    "        print(' ... plotting sample ', idx) # note: each sample is a layer, not a row; only 20 rows are plotted\n",
    "        image_data[29 * idx:29 * idx + 28, :] = tile_raster_images(\n",
    "            X=vis_mf,\n",
    "            img_shape=(28, 28),\n",
    "            tile_shape=(1, n_chains),\n",
    "            tile_spacing=(1, 1)\n",
    "        )\n",
    "\n",
    "    image = Image.fromarray(image_data)\n",
    "    image.save('samples.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample(rbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(self, rng, input, n_in, n_out, W=None, b=None,activation=T.tanh): # if set actication=T.nnet.sigmoid, becomes logistic regresssion layer\n",
    "        self.input = input\n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, name='W', borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, name='b', borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        lin_output = T.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.input = input\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # predict_proba\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(self.input, self.W) + self.b) # softmax=normalized sigmoid\n",
    "        # predict\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "    # this is akin to cost = -1/m * sigma(ylog(wx) + (1-y)log(1-wx)) when y is binomial\n",
    "    # in the current case y has n-labels, and only the prediction of the right label is picked out\n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    # perc of wrong predictions\n",
    "    def errors(self, y):\n",
    "        return T.mean(T.neq(self.y_pred, y)) # T.neq(a,b) checks a != b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "class DBN:\n",
    "    def __init__(\n",
    "        self, \n",
    "        numpy_rng=None, \n",
    "        theano_rng=None, \n",
    "        n_ins=784,\n",
    "        hidden_layers_sizes=[500, 500],  # each elem represents on layer with n(value of elem) nodes\n",
    "        n_outs=10\n",
    "    ):\n",
    "        \n",
    "        self.sigmoid_layers = []\n",
    "        self.rbm_layers = []\n",
    "        self.params = [] # holds the shared/updatable vars\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "        assert self.n_layers > 0\n",
    "        self.theano_rng = theano_rng or RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "        self.index = T.lscalar('index')\n",
    "        self.batch_size = T.lscalar('index')\n",
    "        self.x = T.matrix('x')\n",
    "        self.x_batch = self.x[self.index * self.batch_size : (self.index + 1) * self.batch_size]\n",
    "        self.y = T.ivector('y') # the labels are presented as 1D vector of [int] labels\n",
    "        self.y_batch = self.y[self.index * self.batch_size : (self.index + 1) * self.batch_size]\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            output_size = hidden_layers_sizes[i]\n",
    "            if i == 0:\n",
    "                # first layer is to input\n",
    "                input_size = n_ins\n",
    "                layer_input = self.x_batch\n",
    "            else:\n",
    "                # subseq layers are RBMs, and the input is the prev layer\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "                layer_input = self.sigmoid_layers[-1].output # output is the final activation, i.e. softnet(X*W+b)\n",
    "            \n",
    "            # logistic regression layer\n",
    "            sigmoid_layer = HiddenLayer(\n",
    "                rng=numpy_rng,\n",
    "                input=layer_input,\n",
    "                n_in=input_size,\n",
    "                n_out=output_size,\n",
    "                activation=T.nnet.sigmoid\n",
    "            )\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "            self.params.extend(sigmoid_layer.params) # sigmoid_layer.params is [W,b]; Note: extend: [a] + [b]; append [a] << [b]\n",
    "            \n",
    "            # RBM layer, which pretrains the W and b that will be used by the MLP\n",
    "            rbm_layer = RBM(\n",
    "                numpy_rng=numpy_rng,\n",
    "                theano_rng=theano_rng,\n",
    "                input=layer_input,\n",
    "                n_visible=input_size,\n",
    "                n_hidden=output_size,\n",
    "                W=sigmoid_layer.W,\n",
    "                hbias=sigmoid_layer.b\n",
    "            )\n",
    "            self.rbm_layers.append(rbm_layer)\n",
    "            # note for this implementation, vbias of the RBMs are not treated as a param of the DBN (whereas W and hbias is already included in the DBN params)\n",
    "            \n",
    "        # note the sigmoid_layers do not generate a prediction or return the error of the model\n",
    "        # thus a LogisticRegression class that has those functions is added to the end of the sigmoid_layers\n",
    "        # the input is the activation of the final sigmoid_layer\n",
    "        # output is the actual prediction\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs)\n",
    "        self.params.extend(self.logLayer.params)\n",
    "        \n",
    "        # finetune_cost = cost of regression model\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y_batch)\n",
    "        # perc of wrong preds\n",
    "        self.errors = self.logLayer.errors(self.y_batch)\n",
    "        \n",
    "    def pretraining_functions(self, X, k, learning_rate, batch_size): \n",
    "        pretrain_fns = []\n",
    "        for rbm in self.rbm_layers:\n",
    "            # using CD-k here (persisent=None) for training each RBM.\n",
    "            cost, updates = rbm.get_cost_updates(\n",
    "                learning_rate,\n",
    "                persistent=None, \n",
    "                k=k # iters of hvh\n",
    "            )\n",
    "\n",
    "            fn = theano.function(\n",
    "                inputs=[self.index],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.batch_size: batch_size,\n",
    "                    self.x: X\n",
    "                }\n",
    "            )\n",
    "            # append theano function for each layer to output\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "    \n",
    "    def build_train_function(\n",
    "        self, \n",
    "        train_x, \n",
    "        train_y, \n",
    "        batch_size, \n",
    "        learning_rate\n",
    "    ):\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # gradients of MLP, computed by theano automatically\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # create updates list\n",
    "        updates = {}\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates[param] = param - gparam * T.cast(learning_rate, dtype=theano.config.floatX)\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[self.index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.batch_size: batch_size,\n",
    "                self.x: train_x,\n",
    "                self.y: train_y\n",
    "            }\n",
    "        )\n",
    "        return train_fn\n",
    "\n",
    "    def get_errors(self, X, y, batch_size):  \n",
    "        n_batches = int(X.shape[0] / batch_size)\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "        get_batch_error = theano.function(\n",
    "            inputs=[self.index],\n",
    "            outputs =self.errors, # perc of wrong preds\n",
    "            givens={\n",
    "                self.batch_size: batch_size,\n",
    "                self.x: X,\n",
    "                self.y: y\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        def score_func():\n",
    "            return [get_batch_error(i) for i in range(n_batches)]\n",
    "        \n",
    "        return score_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... building the model\n",
      "... pre-training the model\n",
      "Pre-training layer 0, epoch 0, mean cost: \n",
      "-98.58770587005615\n",
      "Pre-training layer 0, epoch 1, mean cost: \n",
      "-83.83068933868408\n",
      "Pre-training layer 0, epoch 2, mean cost: \n",
      "-80.71768848648071\n",
      "Pre-training layer 0, epoch 3, mean cost: \n",
      "-79.04617987442016\n",
      "Pre-training layer 0, epoch 4, mean cost: \n",
      "-77.91485452041626\n",
      "Pre-training layer 0, epoch 5, mean cost: \n",
      "-77.09489638519287\n",
      "Pre-training layer 0, epoch 6, mean cost: \n",
      "-76.36866627807618\n",
      "Pre-training layer 0, epoch 7, mean cost: \n",
      "-75.7995627105713\n",
      "Pre-training layer 0, epoch 8, mean cost: \n",
      "-75.33677360229493\n",
      "Pre-training layer 0, epoch 9, mean cost: \n",
      "-74.91954721298218\n",
      "Pre-training layer 0, epoch 10, mean cost: \n",
      "-74.56196344833374\n",
      "Pre-training layer 0, epoch 11, mean cost: \n",
      "-74.21666637954712\n",
      "Pre-training layer 0, epoch 12, mean cost: \n",
      "-73.92353014678955\n",
      "Pre-training layer 0, epoch 13, mean cost: \n",
      "-73.658787159729\n",
      "Pre-training layer 0, epoch 14, mean cost: \n",
      "-73.41774708709717\n",
      "Pre-training layer 0, epoch 15, mean cost: \n",
      "-73.17543847503661\n",
      "Pre-training layer 0, epoch 16, mean cost: \n",
      "-72.94021204833984\n",
      "Pre-training layer 0, epoch 17, mean cost: \n",
      "-72.76849327850341\n",
      "Pre-training layer 0, epoch 18, mean cost: \n",
      "-72.57377784957886\n",
      "Pre-training layer 0, epoch 19, mean cost: \n",
      "-72.38860248718262\n",
      "Pre-training layer 0, epoch 20, mean cost: \n",
      "-72.23766722106933\n",
      "Pre-training layer 0, epoch 21, mean cost: \n",
      "-72.08073511199952\n",
      "Pre-training layer 0, epoch 22, mean cost: \n",
      "-71.95222357635498\n",
      "Pre-training layer 0, epoch 23, mean cost: \n",
      "-71.79716955642701\n",
      "Pre-training layer 0, epoch 24, mean cost: \n",
      "-71.68222746887207\n",
      "Pre-training layer 0, epoch 25, mean cost: \n",
      "-71.53556893539428\n",
      "Pre-training layer 0, epoch 26, mean cost: \n",
      "-71.43600336914062\n",
      "Pre-training layer 0, epoch 27, mean cost: \n",
      "-71.30886045303345\n",
      "Pre-training layer 0, epoch 28, mean cost: \n",
      "-71.24304067687989\n",
      "Pre-training layer 0, epoch 29, mean cost: \n",
      "-71.09340059432984\n",
      "Pre-training layer 0, epoch 30, mean cost: \n",
      "-71.0221983833313\n",
      "Pre-training layer 0, epoch 31, mean cost: \n",
      "-70.9138223449707\n",
      "Pre-training layer 0, epoch 32, mean cost: \n",
      "-70.79893392562866\n",
      "Pre-training layer 0, epoch 33, mean cost: \n",
      "-70.73915310897827\n",
      "Pre-training layer 0, epoch 34, mean cost: \n",
      "-70.64481697311402\n",
      "Pre-training layer 0, epoch 35, mean cost: \n",
      "-70.5817158493042\n",
      "Pre-training layer 0, epoch 36, mean cost: \n",
      "-70.50427200088501\n",
      "Pre-training layer 0, epoch 37, mean cost: \n",
      "-70.40136917419433\n",
      "Pre-training layer 0, epoch 38, mean cost: \n",
      "-70.35978354644776\n",
      "Pre-training layer 0, epoch 39, mean cost: \n",
      "-70.25543015213013\n",
      "Pre-training layer 0, epoch 40, mean cost: \n",
      "-70.21135699768067\n",
      "Pre-training layer 0, epoch 41, mean cost: \n",
      "-70.16417630996705\n",
      "Pre-training layer 0, epoch 42, mean cost: \n",
      "-70.1153075126648\n",
      "Pre-training layer 0, epoch 43, mean cost: \n",
      "-70.04952685089111\n",
      "Pre-training layer 0, epoch 44, mean cost: \n",
      "-70.01882087554931\n",
      "Pre-training layer 0, epoch 45, mean cost: \n",
      "-69.91603784332275\n",
      "Pre-training layer 0, epoch 46, mean cost: \n",
      "-69.88429797286987\n",
      "Pre-training layer 0, epoch 47, mean cost: \n",
      "-69.83568030471801\n",
      "Pre-training layer 0, epoch 48, mean cost: \n",
      "-69.77965713119507\n",
      "Pre-training layer 0, epoch 49, mean cost: \n",
      "-69.73799373779296\n",
      "Pre-training layer 1, epoch 0, mean cost: \n",
      "-214.72808708190917\n",
      "Pre-training layer 1, epoch 1, mean cost: \n",
      "-188.8041744720459\n",
      "Pre-training layer 1, epoch 2, mean cost: \n",
      "-183.05622294464112\n",
      "Pre-training layer 1, epoch 3, mean cost: \n",
      "-179.92337172393798\n",
      "Pre-training layer 1, epoch 4, mean cost: \n",
      "-177.85160874023438\n",
      "Pre-training layer 1, epoch 5, mean cost: \n",
      "-176.31922327270507\n",
      "Pre-training layer 1, epoch 6, mean cost: \n",
      "-175.07420901641845\n",
      "Pre-training layer 1, epoch 7, mean cost: \n",
      "-174.0485997314453\n",
      "Pre-training layer 1, epoch 8, mean cost: \n",
      "-173.20979051971435\n",
      "Pre-training layer 1, epoch 9, mean cost: \n",
      "-172.4853423171997\n",
      "Pre-training layer 1, epoch 10, mean cost: \n",
      "-171.86492735290528\n",
      "Pre-training layer 1, epoch 11, mean cost: \n",
      "-171.29904056396484\n",
      "Pre-training layer 1, epoch 12, mean cost: \n",
      "-170.84122608795167\n",
      "Pre-training layer 1, epoch 13, mean cost: \n",
      "-170.42704435272216\n",
      "Pre-training layer 1, epoch 14, mean cost: \n",
      "-170.02576228637696\n",
      "Pre-training layer 1, epoch 15, mean cost: \n",
      "-169.66744013671874\n",
      "Pre-training layer 1, epoch 16, mean cost: \n",
      "-169.36655721740723\n",
      "Pre-training layer 1, epoch 17, mean cost: \n",
      "-169.0510109512329\n",
      "Pre-training layer 1, epoch 18, mean cost: \n",
      "-168.78794024963378\n",
      "Pre-training layer 1, epoch 19, mean cost: \n",
      "-168.54874654541015\n",
      "Pre-training layer 1, epoch 20, mean cost: \n",
      "-168.32410376281737\n",
      "Pre-training layer 1, epoch 21, mean cost: \n",
      "-168.11305158843993\n",
      "Pre-training layer 1, epoch 22, mean cost: \n",
      "-167.95332333984376\n",
      "Pre-training layer 1, epoch 23, mean cost: \n",
      "-167.7560734085083\n",
      "Pre-training layer 1, epoch 24, mean cost: \n",
      "-167.57986191864015\n",
      "Pre-training layer 1, epoch 25, mean cost: \n",
      "-167.4476672744751\n",
      "Pre-training layer 1, epoch 26, mean cost: \n",
      "-167.26429667510988\n",
      "Pre-training layer 1, epoch 27, mean cost: \n",
      "-167.14699211883544\n",
      "Pre-training layer 1, epoch 28, mean cost: \n",
      "-167.01859606170655\n",
      "Pre-training layer 1, epoch 29, mean cost: \n",
      "-166.90860153961182\n",
      "Pre-training layer 1, epoch 30, mean cost: \n",
      "-166.79203215026857\n",
      "Pre-training layer 1, epoch 31, mean cost: \n",
      "-166.72780986022948\n",
      "Pre-training layer 1, epoch 32, mean cost: \n",
      "-166.6179993774414\n",
      "Pre-training layer 1, epoch 33, mean cost: \n",
      "-166.52622886505128\n",
      "Pre-training layer 1, epoch 34, mean cost: \n",
      "-166.42949245300292\n",
      "Pre-training layer 1, epoch 35, mean cost: \n",
      "-166.37242518310546\n",
      "Pre-training layer 1, epoch 36, mean cost: \n",
      "-166.2992202835083\n",
      "Pre-training layer 1, epoch 37, mean cost: \n",
      "-166.22381999664307\n",
      "Pre-training layer 1, epoch 38, mean cost: \n",
      "-166.16114515838623\n",
      "Pre-training layer 1, epoch 39, mean cost: \n",
      "-166.08924996643066\n",
      "Pre-training layer 1, epoch 40, mean cost: \n",
      "-166.0339566833496\n",
      "Pre-training layer 1, epoch 41, mean cost: \n",
      "-165.9553503692627\n",
      "Pre-training layer 1, epoch 42, mean cost: \n",
      "-165.9330690765381\n",
      "Pre-training layer 1, epoch 43, mean cost: \n",
      "-165.88575777130126\n",
      "Pre-training layer 1, epoch 44, mean cost: \n",
      "-165.85857549591066\n",
      "Pre-training layer 1, epoch 45, mean cost: \n",
      "-165.7786752761841\n",
      "Pre-training layer 1, epoch 46, mean cost: \n",
      "-165.74411543273925\n",
      "Pre-training layer 1, epoch 47, mean cost: \n",
      "-165.69518148803712\n",
      "Pre-training layer 1, epoch 48, mean cost: \n",
      "-165.63612653808593\n",
      "Pre-training layer 1, epoch 49, mean cost: \n",
      "-165.61379446258545\n",
      "Pre-training layer 2, epoch 0, mean cost: \n",
      "-64.90090269165039\n",
      "Pre-training layer 2, epoch 1, mean cost: \n",
      "-52.07453692321777\n",
      "Pre-training layer 2, epoch 2, mean cost: \n",
      "-48.904152474212644\n",
      "Pre-training layer 2, epoch 3, mean cost: \n",
      "-47.20853685836792\n",
      "Pre-training layer 2, epoch 4, mean cost: \n",
      "-46.1752872467041\n",
      "Pre-training layer 2, epoch 5, mean cost: \n",
      "-45.391974770736695\n",
      "Pre-training layer 2, epoch 6, mean cost: \n",
      "-44.830072736740114\n",
      "Pre-training layer 2, epoch 7, mean cost: \n",
      "-44.399859553146364\n",
      "Pre-training layer 2, epoch 8, mean cost: \n",
      "-44.04093620872498\n",
      "Pre-training layer 2, epoch 9, mean cost: \n",
      "-43.75625210838318\n",
      "Pre-training layer 2, epoch 10, mean cost: \n",
      "-43.55494284667969\n",
      "Pre-training layer 2, epoch 11, mean cost: \n",
      "-43.34016231002808\n",
      "Pre-training layer 2, epoch 12, mean cost: \n",
      "-43.16074801864624\n",
      "Pre-training layer 2, epoch 13, mean cost: \n",
      "-43.00968446235657\n",
      "Pre-training layer 2, epoch 14, mean cost: \n",
      "-42.853954949188235\n",
      "Pre-training layer 2, epoch 15, mean cost: \n",
      "-42.716109319686886\n",
      "Pre-training layer 2, epoch 16, mean cost: \n",
      "-42.603521965408326\n",
      "Pre-training layer 2, epoch 17, mean cost: \n",
      "-42.50671601638794\n",
      "Pre-training layer 2, epoch 18, mean cost: \n",
      "-42.408665433120724\n",
      "Pre-training layer 2, epoch 19, mean cost: \n",
      "-42.30950103378296\n",
      "Pre-training layer 2, epoch 20, mean cost: \n",
      "-42.23433369216919\n",
      "Pre-training layer 2, epoch 21, mean cost: \n",
      "-42.15453893814087\n",
      "Pre-training layer 2, epoch 22, mean cost: \n",
      "-42.09622144966126\n",
      "Pre-training layer 2, epoch 23, mean cost: \n",
      "-42.007106688690186\n",
      "Pre-training layer 2, epoch 24, mean cost: \n",
      "-41.96309180221558\n",
      "Pre-training layer 2, epoch 25, mean cost: \n",
      "-41.86579760284424\n",
      "Pre-training layer 2, epoch 26, mean cost: \n",
      "-41.81325118942261\n",
      "Pre-training layer 2, epoch 27, mean cost: \n",
      "-41.75455167694092\n",
      "Pre-training layer 2, epoch 28, mean cost: \n",
      "-41.70412637290955\n",
      "Pre-training layer 2, epoch 29, mean cost: \n",
      "-41.614296977233884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training layer 2, epoch 30, mean cost: \n",
      "-41.55054725227356\n",
      "Pre-training layer 2, epoch 31, mean cost: \n",
      "-41.52927019996643\n",
      "Pre-training layer 2, epoch 32, mean cost: \n",
      "-41.465435191726684\n",
      "Pre-training layer 2, epoch 33, mean cost: \n",
      "-41.408279712677\n",
      "Pre-training layer 2, epoch 34, mean cost: \n",
      "-41.35971137046814\n",
      "Pre-training layer 2, epoch 35, mean cost: \n",
      "-41.304834408187865\n",
      "Pre-training layer 2, epoch 36, mean cost: \n",
      "-41.27505603408814\n",
      "Pre-training layer 2, epoch 37, mean cost: \n",
      "-41.22063816947937\n",
      "Pre-training layer 2, epoch 38, mean cost: \n",
      "-41.17922067718506\n",
      "Pre-training layer 2, epoch 39, mean cost: \n",
      "-41.129606490707395\n",
      "Pre-training layer 2, epoch 40, mean cost: \n",
      "-41.089771279907225\n",
      "Pre-training layer 2, epoch 41, mean cost: \n",
      "-41.05367492523193\n",
      "Pre-training layer 2, epoch 42, mean cost: \n",
      "-41.00482059059143\n",
      "Pre-training layer 2, epoch 43, mean cost: \n",
      "-40.96135161972046\n",
      "Pre-training layer 2, epoch 44, mean cost: \n",
      "-40.879485597991945\n",
      "Pre-training layer 2, epoch 45, mean cost: \n",
      "-40.88209765129089\n",
      "Pre-training layer 2, epoch 46, mean cost: \n",
      "-40.82970209274292\n",
      "Pre-training layer 2, epoch 47, mean cost: \n",
      "-40.77555294113159\n",
      "Pre-training layer 2, epoch 48, mean cost: \n",
      "-40.74614690322876\n",
      "Pre-training layer 2, epoch 49, mean cost: \n",
      "-40.71378244323731\n",
      "Training time: 1640.486915393005 s.\n"
     ]
    }
   ],
   "source": [
    "batch_size=10\n",
    "k = 1\n",
    "pretrain_lr=0.01\n",
    "pretraining_epochs=50\n",
    "\n",
    "# for testing\n",
    "# batch_size=10\n",
    "# k = 1\n",
    "# pretrain_lr=0.01\n",
    "# pretraining_epochs=1\n",
    "# train_set_x=train_set_x[0:1000,:]\n",
    "\n",
    "train_set_y = train_set_y.astype('int32')\n",
    "valid_set_y = valid_set_y.astype('int32')\n",
    "test_set_y = test_set_y.astype('int32')\n",
    "\n",
    "\n",
    "numpy_rng = numpy.random.RandomState(123)\n",
    "n_train_batches = int(train_set_x.shape[0] / batch_size)\n",
    "\n",
    "print('... building the model')\n",
    "dbn = DBN(\n",
    "    numpy_rng=numpy_rng, \n",
    "    n_ins=28 * 28,\n",
    "    hidden_layers_sizes=[1000, 250, 50],\n",
    "    n_outs=10\n",
    ")\n",
    "pretraining_fns = dbn.pretraining_functions(\n",
    "    X=train_set_x,\n",
    "    k=k,\n",
    "    learning_rate=pretrain_lr,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print('... pre-training the model')\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Pre-train layer-wise\n",
    "for i in range(dbn.n_layers):\n",
    "    for epoch in range(pretraining_epochs):\n",
    "        costs = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            costs.append(pretraining_fns[i](index=batch_index))\n",
    "        print(f'Pre-training layer {i}, epoch {epoch}, mean cost: ')\n",
    "        print(numpy.mean(costs, dtype='float64'))\n",
    "\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f'Training time: {end_time - start_time} s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... getting the finetuning functions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:127: UserWarning: The parameter 'updates' of theano.function() expects an OrderedDict, got <class 'dict'>. Using a standard dictionary here results in non-deterministic behavior. You should use an OrderedDict if you are using Python 2.7 (theano.compat.OrderedDict for older python), or use a list of (shared, update) pairs. Do not just convert your dictionary to this type before the call as the conversion will still be non-deterministic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... finetuning the model\n",
      "epoch 0, minibatch 5000/5000, validation error 4.8%\n",
      "     epoch 0, minibatch minibatch 5000/5000, test error of best model 5.109999999999999%\n",
      "epoch 1, minibatch 5000/5000, validation error 3.8799999999999994%\n",
      "     epoch 1, minibatch minibatch 5000/5000, test error of best model 4.01%\n",
      "epoch 2, minibatch 5000/5000, validation error 3.35%\n",
      "     epoch 2, minibatch minibatch 5000/5000, test error of best model 3.5999999999999996%\n",
      "epoch 3, minibatch 5000/5000, validation error 3.1199999999999997%\n",
      "     epoch 3, minibatch minibatch 5000/5000, test error of best model 3.2799999999999994%\n",
      "epoch 4, minibatch 5000/5000, validation error 2.9000000000000004%\n",
      "     epoch 4, minibatch minibatch 5000/5000, test error of best model 3.1300000000000003%\n",
      "epoch 5, minibatch 5000/5000, validation error 2.69%\n",
      "     epoch 5, minibatch minibatch 5000/5000, test error of best model 2.9400000000000004%\n",
      "epoch 6, minibatch 5000/5000, validation error 2.45%\n",
      "     epoch 6, minibatch minibatch 5000/5000, test error of best model 2.86%\n",
      "epoch 7, minibatch 5000/5000, validation error 2.3899999999999997%\n",
      "     epoch 7, minibatch minibatch 5000/5000, test error of best model 2.83%\n",
      "epoch 8, minibatch 5000/5000, validation error 2.39%\n",
      "epoch 9, minibatch 5000/5000, validation error 2.35%\n",
      "     epoch 9, minibatch minibatch 5000/5000, test error of best model 2.6599999999999997%\n",
      "epoch 10, minibatch 5000/5000, validation error 2.2800000000000002%\n",
      "     epoch 10, minibatch minibatch 5000/5000, test error of best model 2.6%\n",
      "epoch 11, minibatch 5000/5000, validation error 2.2800000000000002%\n",
      "epoch 12, minibatch 5000/5000, validation error 2.24%\n",
      "     epoch 12, minibatch minibatch 5000/5000, test error of best model 2.5100000000000002%\n",
      "epoch 13, minibatch 5000/5000, validation error 2.25%\n",
      "epoch 14, minibatch 5000/5000, validation error 2.22%\n",
      "     epoch 14, minibatch minibatch 5000/5000, test error of best model 2.3899999999999997%\n",
      "epoch 15, minibatch 5000/5000, validation error 2.23%\n",
      "epoch 16, minibatch 5000/5000, validation error 2.21%\n",
      "     epoch 16, minibatch minibatch 5000/5000, test error of best model 2.41%\n",
      "epoch 17, minibatch 5000/5000, validation error 2.2200000000000006%\n",
      "epoch 18, minibatch 5000/5000, validation error 2.19%\n",
      "     epoch 18, minibatch minibatch 5000/5000, test error of best model 2.33%\n",
      "epoch 19, minibatch 5000/5000, validation error 2.1900000000000004%\n",
      "epoch 20, minibatch 5000/5000, validation error 2.1900000000000004%\n",
      "epoch 21, minibatch 5000/5000, validation error 2.1900000000000004%\n",
      "epoch 22, minibatch 5000/5000, validation error 2.16%\n",
      "     epoch 22, minibatch minibatch 5000/5000, test error of best model 2.22%\n",
      "epoch 23, minibatch 5000/5000, validation error 2.14%\n",
      "     epoch 23, minibatch minibatch 5000/5000, test error of best model 2.1999999999999997%\n",
      "epoch 24, minibatch 5000/5000, validation error 2.15%\n",
      "epoch 25, minibatch 5000/5000, validation error 2.1500000000000004%\n",
      "epoch 26, minibatch 5000/5000, validation error 2.1500000000000004%\n",
      "epoch 27, minibatch 5000/5000, validation error 2.13%\n",
      "     epoch 27, minibatch minibatch 5000/5000, test error of best model 2.16%\n",
      "epoch 28, minibatch 5000/5000, validation error 2.1200000000000006%\n",
      "     epoch 28, minibatch minibatch 5000/5000, test error of best model 2.1399999999999997%\n",
      "epoch 29, minibatch 5000/5000, validation error 2.13%\n",
      "epoch 30, minibatch 5000/5000, validation error 2.14%\n",
      "epoch 31, minibatch 5000/5000, validation error 2.1399999999999997%\n",
      "epoch 32, minibatch 5000/5000, validation error 2.15%\n",
      "epoch 33, minibatch 5000/5000, validation error 2.16%\n",
      "epoch 34, minibatch 5000/5000, validation error 2.16%\n",
      "epoch 35, minibatch 5000/5000, validation error 2.13%\n",
      "epoch 36, minibatch 5000/5000, validation error 2.15%\n",
      "epoch 37, minibatch 5000/5000, validation error 2.1399999999999997%\n",
      "epoch 38, minibatch 5000/5000, validation error 2.13%\n",
      "epoch 39, minibatch 5000/5000, validation error 2.1200000000000006%\n",
      "epoch 40, minibatch 5000/5000, validation error 2.1%\n",
      "     epoch 40, minibatch minibatch 5000/5000, test error of best model 2.0300000000000002%\n",
      "epoch 41, minibatch 5000/5000, validation error 2.06%\n",
      "     epoch 41, minibatch minibatch 5000/5000, test error of best model 2.01%\n",
      "epoch 42, minibatch 5000/5000, validation error 2.06%\n",
      "epoch 43, minibatch 5000/5000, validation error 2.06%\n",
      "epoch 44, minibatch 5000/5000, validation error 2.0500000000000003%\n",
      "     epoch 44, minibatch minibatch 5000/5000, test error of best model 2.0300000000000002%\n",
      "epoch 45, minibatch 5000/5000, validation error 2.0500000000000003%\n",
      "epoch 46, minibatch 5000/5000, validation error 2.04%\n",
      "     epoch 46, minibatch minibatch 5000/5000, test error of best model 1.9899999999999998%\n",
      "epoch 47, minibatch 5000/5000, validation error 2.02%\n",
      "     epoch 47, minibatch minibatch 5000/5000, test error of best model 1.97%\n",
      "epoch 48, minibatch 5000/5000, validation error 2.01%\n",
      "     epoch 48, minibatch minibatch 5000/5000, test error of best model 1.95%\n",
      "epoch 49, minibatch 5000/5000, validation error 2.0%\n",
      "     epoch 49, minibatch minibatch 5000/5000, test error of best model 1.9399999999999997%\n",
      "epoch 50, minibatch 5000/5000, validation error 1.9899999999999998%\n",
      "     epoch 50, minibatch minibatch 5000/5000, test error of best model 1.92%\n",
      "epoch 51, minibatch 5000/5000, validation error 1.9600000000000002%\n",
      "     epoch 51, minibatch minibatch 5000/5000, test error of best model 1.92%\n",
      "epoch 52, minibatch 5000/5000, validation error 1.9700000000000002%\n",
      "epoch 53, minibatch 5000/5000, validation error 1.9700000000000002%\n",
      "epoch 54, minibatch 5000/5000, validation error 1.95%\n",
      "     epoch 54, minibatch minibatch 5000/5000, test error of best model 1.9100000000000001%\n",
      "epoch 55, minibatch 5000/5000, validation error 1.9600000000000002%\n",
      "epoch 56, minibatch 5000/5000, validation error 1.95%\n",
      "epoch 57, minibatch 5000/5000, validation error 1.94%\n",
      "     epoch 57, minibatch minibatch 5000/5000, test error of best model 1.92%\n",
      "epoch 58, minibatch 5000/5000, validation error 1.9300000000000002%\n",
      "     epoch 58, minibatch minibatch 5000/5000, test error of best model 1.9100000000000001%\n",
      "epoch 59, minibatch 5000/5000, validation error 1.9100000000000001%\n",
      "     epoch 59, minibatch minibatch 5000/5000, test error of best model 1.8900000000000001%\n",
      "epoch 60, minibatch 5000/5000, validation error 1.9200000000000002%\n",
      "epoch 61, minibatch 5000/5000, validation error 1.9300000000000002%\n",
      "epoch 62, minibatch 5000/5000, validation error 1.9300000000000002%\n",
      "epoch 63, minibatch 5000/5000, validation error 1.92%\n",
      "epoch 64, minibatch 5000/5000, validation error 1.9100000000000001%\n",
      "epoch 65, minibatch 5000/5000, validation error 1.9%\n",
      "     epoch 65, minibatch minibatch 5000/5000, test error of best model 1.9%\n",
      "epoch 66, minibatch 5000/5000, validation error 1.9100000000000001%\n",
      "epoch 67, minibatch 5000/5000, validation error 1.9100000000000001%\n",
      "epoch 68, minibatch 5000/5000, validation error 1.8900000000000003%\n",
      "     epoch 68, minibatch minibatch 5000/5000, test error of best model 1.92%\n",
      "epoch 69, minibatch 5000/5000, validation error 1.8699999999999999%\n",
      "     epoch 69, minibatch minibatch 5000/5000, test error of best model 1.9100000000000001%\n",
      "epoch 70, minibatch 5000/5000, validation error 1.8699999999999999%\n",
      "epoch 71, minibatch 5000/5000, validation error 1.86%\n",
      "     epoch 71, minibatch minibatch 5000/5000, test error of best model 1.9100000000000001%\n",
      "epoch 72, minibatch 5000/5000, validation error 1.86%\n",
      "epoch 73, minibatch 5000/5000, validation error 1.86%\n",
      "epoch 74, minibatch 5000/5000, validation error 1.86%\n",
      "epoch 75, minibatch 5000/5000, validation error 1.8699999999999999%\n",
      "epoch 76, minibatch 5000/5000, validation error 1.8499999999999999%\n",
      "     epoch 76, minibatch minibatch 5000/5000, test error of best model 1.9100000000000001%\n",
      "epoch 77, minibatch 5000/5000, validation error 1.8499999999999999%\n",
      "epoch 78, minibatch 5000/5000, validation error 1.8499999999999999%\n",
      "epoch 79, minibatch 5000/5000, validation error 1.8499999999999999%\n",
      "epoch 80, minibatch 5000/5000, validation error 1.8499999999999999%\n",
      "epoch 81, minibatch 5000/5000, validation error 1.8399999999999999%\n",
      "     epoch 81, minibatch minibatch 5000/5000, test error of best model 1.9100000000000001%\n",
      "epoch 82, minibatch 5000/5000, validation error 1.8399999999999999%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 83, minibatch 5000/5000, validation error 1.8499999999999999%\n",
      "epoch 84, minibatch 5000/5000, validation error 1.8499999999999999%\n",
      "epoch 85, minibatch 5000/5000, validation error 1.8499999999999999%\n",
      "epoch 86, minibatch 5000/5000, validation error 1.8499999999999999%\n",
      "epoch 87, minibatch 5000/5000, validation error 1.8399999999999999%\n",
      "epoch 88, minibatch 5000/5000, validation error 1.8399999999999999%\n",
      "epoch 89, minibatch 5000/5000, validation error 1.8399999999999999%\n",
      "epoch 90, minibatch 5000/5000, validation error 1.8399999999999999%\n",
      "epoch 91, minibatch 5000/5000, validation error 1.83%\n",
      "     epoch 91, minibatch minibatch 5000/5000, test error of best model 1.9%\n",
      "epoch 92, minibatch 5000/5000, validation error 1.83%\n",
      "epoch 93, minibatch 5000/5000, validation error 1.82%\n",
      "     epoch 93, minibatch minibatch 5000/5000, test error of best model 1.9%\n",
      "epoch 94, minibatch 5000/5000, validation error 1.82%\n",
      "epoch 95, minibatch 5000/5000, validation error 1.82%\n",
      "epoch 96, minibatch 5000/5000, validation error 1.82%\n",
      "epoch 97, minibatch 5000/5000, validation error 1.82%\n",
      "epoch 98, minibatch 5000/5000, validation error 1.81%\n",
      "     epoch 98, minibatch minibatch 5000/5000, test error of best model 1.8699999999999999%\n",
      "epoch 99, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 100, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 101, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 102, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 103, minibatch 5000/5000, validation error 1.79%\n",
      "     epoch 103, minibatch minibatch 5000/5000, test error of best model 1.8699999999999999%\n",
      "epoch 104, minibatch 5000/5000, validation error 1.79%\n",
      "epoch 105, minibatch 5000/5000, validation error 1.7999999999999998%\n",
      "epoch 106, minibatch 5000/5000, validation error 1.7999999999999998%\n",
      "epoch 107, minibatch 5000/5000, validation error 1.79%\n",
      "epoch 108, minibatch 5000/5000, validation error 1.79%\n",
      "epoch 109, minibatch 5000/5000, validation error 1.78%\n",
      "     epoch 109, minibatch minibatch 5000/5000, test error of best model 1.8699999999999999%\n",
      "epoch 110, minibatch 5000/5000, validation error 1.78%\n",
      "epoch 111, minibatch 5000/5000, validation error 1.78%\n",
      "epoch 112, minibatch 5000/5000, validation error 1.78%\n",
      "epoch 113, minibatch 5000/5000, validation error 1.7999999999999998%\n",
      "epoch 114, minibatch 5000/5000, validation error 1.7999999999999998%\n",
      "epoch 115, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 116, minibatch 5000/5000, validation error 1.7999999999999998%\n",
      "epoch 117, minibatch 5000/5000, validation error 1.7999999999999998%\n",
      "epoch 118, minibatch 5000/5000, validation error 1.7999999999999998%\n",
      "epoch 119, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 120, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 121, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 122, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 123, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 124, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 125, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 126, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 127, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 128, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 129, minibatch 5000/5000, validation error 1.81%\n",
      "epoch 130, minibatch 5000/5000, validation error 1.81%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-5b4226e9cb68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_validation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mbest_validation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-5b4226e9cb68>\u001b[0m in \u001b[0;36mtrain_mlp\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "finetune_lr = 0.1\n",
    "training_epochs=1000\n",
    "\n",
    "print('... getting the finetuning functions')\n",
    "train_fn = dbn.build_train_function(\n",
    "    train_x = train_set_x,\n",
    "    train_y = train_set_y,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=finetune_lr\n",
    ")\n",
    "get_validate_errors = dbn.get_errors(valid_set_x, valid_set_y, batch_size)\n",
    "get_test_errors = dbn.get_errors(test_set_x, test_set_y, batch_size)\n",
    "\n",
    "def train_mlp():\n",
    "    print('... finetuning the model')\n",
    "    \n",
    "    # early-stopping parameters\n",
    "    patience = 4 * n_train_batches # look as this many examples regardless, i.e. 4 epochs\n",
    "    patience_increase = 2. # loop for n times more when a new best is found\n",
    "    improvement_threshold = 0.995 # a relative improvement of this much is considered significant\n",
    "\n",
    "    # go through this many minibatches before checking the network on\n",
    "    # the validation set; in this case we check every epoch\n",
    "    validation_frequency = min(n_train_batches, patience / 2) # = n_train_batches\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            train_fn(minibatch_index)\n",
    "            iter = epoch * n_train_batches + minibatch_index\n",
    "\n",
    "            # for every 'validation_frequency' iters\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = get_validate_errors()\n",
    "                curr_mean_validation_loss = numpy.mean(validation_losses, dtype='float64')\n",
    "                print(f'epoch {epoch}, minibatch {minibatch_index + 1}/{n_train_batches}, validation error {curr_mean_validation_loss * 100.}%')\n",
    "\n",
    "                # if we got the least validation errors until now\n",
    "                if curr_mean_validation_loss < best_validation_loss:\n",
    "                    # improve patience if loss improvement is good enough; which will allow more training = double of the curr loop count\n",
    "                    if (curr_mean_validation_loss < best_validation_loss * improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = curr_mean_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = get_test_errors()\n",
    "                    test_score = numpy.mean(test_losses, dtype='float64')\n",
    "                    print(f'     epoch {epoch}, minibatch minibatch {minibatch_index + 1}/{n_train_batches}, test error of best model {test_score * 100.}%')\n",
    "\n",
    "            # if no improvement in validation score for the last 50% iters\n",
    "            if patience <= iter:\n",
    "                return best_validation_loss, best_iter, test_score\n",
    "    return best_validation_loss, best_iter, test_score\n",
    "\n",
    "best_validation_loss, best_iter, test_score = train_mlp()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f'training time: {end_time - start_time}s.')\n",
    "print(f'Optimization complete with best validation score of {best_validation_loss * 100.}%,\\n'\n",
    "    f'obtained at iteration {best_iter + 1},\\n'\n",
    "    f'with test performance {test_score * 100.}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
