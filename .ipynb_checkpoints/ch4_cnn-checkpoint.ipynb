{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config theano to use GPU, must be done before theano is imported\n",
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda,floatX=float32\"#,optimizer=None,exception_verbosity=high\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://deeplearning.net/tutorial/lenet.html\n",
    "    \n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "\n",
    "class LeNetConvPoolLayer():\n",
    "    def __init__(self, \n",
    "        rng, \n",
    "        input, # with 4 dimensions: rows of input(i.e. batch) * num of input maps per input * height per map * width per map; see: https://www.quora.com/Why-are-there-4-dimensions-to-convolve-over-the-Stanford-UFDL-example-in-convolutional-neural-networks\n",
    "        image_shape, # tuple/list of len 4 representing the 4 dims of input: (batch size, num input feature maps, image height, image width)\n",
    "        filter_shape, # tuple/list of len 4 representing: (number of filters, num input feature maps, filter height, filter width)\n",
    "        poolsize=(2, 2), # eg. downsample every 2x2 bits to 1 bit\n",
    "        activation=T.nnet.relu,\n",
    "        border_mode='valid', # no padding; see doc for more options\n",
    "        subsample=(1, 1) # unit stride        \n",
    "    ):\n",
    "        assert image_shape[1] == filter_shape[1] # num input feataure maps should be same for both arrs\n",
    "        \n",
    "        self.input = input\n",
    "        fan_in = numpy.prod(filter_shape[1:]) # (num input feature maps * filter height * filter width) input nodes to each hidden node/unit(?) \n",
    "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) / numpy.prod(poolsize)) # (num output feature maps * filter height * filter width /  pooling size) output nodes per layer\n",
    "        \n",
    "        # initialize weights with random weights\n",
    "        W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
    "        self.W = theano.shared(\n",
    "            numpy.asarray(\n",
    "                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX) # a 1D tensor(i.e. array) of len(num of filters) for adding a bias per output feature map\n",
    "        self.b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "        # convolve input feature maps with filters\n",
    "        # doc: http://deeplearning.net/software/theano/library/tensor/nnet/conv.html#theano.tensor.nnet.conv2d\n",
    "        # simply explained; conv2d iterates over every sample, apply the SAME weights on each 'cropped' input(map) to create each output map\n",
    "        # taking an analogy with MLP, each output feature map can be considered as one hidden node in an MLP, where each value in the map represents a different cropped position of the input image\n",
    "        # (programmatically, I believe it flattens the num of maps * input height * input width for every sample; apply the transformtion; and then reshape it back to the expected output dimensions)\n",
    "        conv_out = conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            input_shape=image_shape,\n",
    "            border_mode=border_mode,\n",
    "            subsample=subsample\n",
    "        )\n",
    "\n",
    "        # pool each feature map individually, using maxpooling\n",
    "        # doc: http://deeplearning.net/software/theano/library/tensor/signal/pool.html\n",
    "        # note: 'Max pooling will be done over the 2 last dimensions', i.e. height * width\n",
    "        pooled_out = pool.pool_2d(\n",
    "            input=conv_out,\n",
    "            ws=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "        # add the bias term. Since the bias is a vector (1D array), we reshape it to a tensor of shape (1, n_filters, 1, 1). \n",
    "        # since every output map uses one shared weight, only 1 bias is needed for each output map\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x')) # explanation of dimshuffle: https://stackoverflow.com/questions/42401420/how-the-function-dimshuffle-works-in-theano\n",
    "\n",
    "        # shared_params\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_train():\n",
    "    X = numpy.empty((0,3072), dtype=theano.config.floatX)\n",
    "    y = []\n",
    "    for i in range(5):\n",
    "        i += 1\n",
    "        unpacked = unpickle(f'cifar-10-batches-py/data_batch_{i}')\n",
    "        X = numpy.concatenate((X, unpacked[b'data']))\n",
    "        y = numpy.concatenate((y, unpacked[b'labels']))\n",
    "    return X,y.astype('int32')\n",
    "\n",
    "def load_cifar10_test():\n",
    "    unpacked = unpickle(f'cifar-10-batches-py/test_batch')\n",
    "    return numpy.array(unpacked[b'data'], dtype=theano.config.floatX), numpy.array(unpacked[b'labels'], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_cifar10_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set_x, valid_set_x, train_set_y, valid_set_y = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_x, test_set_y = load_cifar10_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "train_set_x = train_set_x[0:1000,:]\n",
    "valid_set_x = valid_set_x[0:1000,:]\n",
    "test_set_x = test_set_x[0:1000,:]\n",
    "\n",
    "train_set_y = train_set_y[0:1000]\n",
    "valid_set_y = valid_set_y[0:1000]\n",
    "test_set_y = test_set_y[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        rng, \n",
    "        input, \n",
    "        n_in, \n",
    "        n_out, \n",
    "        p_dropout=0.0, \n",
    "        W=None, \n",
    "        b=None, \n",
    "        activation=T.tanh # if set actication=T.nnet.sigmoid, becomes logistic regresssion layer\n",
    "    ): \n",
    "        self.input = input\n",
    "        self.theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "        \n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        dropout_input = self.get_corrupted_input(input, p_dropout)\n",
    "        lin_output = T.dot(dropout_input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    # add noise by setting corruption_level% of data to 0s\n",
    "    def get_corrupted_input(self, input, corruption_level):   \n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.input = input\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # predict_proba\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(self.input, self.W) + self.b) # softmax=normalized sigmoid\n",
    "        # predict\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "    # this is akin to cost = -1/m * sigma(ylog(wx) + (1-y)log(1-wx)) when y is binomial\n",
    "    # in the current case y has n-labels, and only the prediction of the right label is picked out\n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    # perc of wrong predictions\n",
    "    def errors(self, y):\n",
    "        return T.mean(T.neq(self.y_pred, y)) # T.neq(a,b) checks a != b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: output size calculated by formula:\n",
    "# (Width - Filter_size + 2*Padding / Stride) + 1\n",
    "# stride is usually 1 in implementations such as conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "\n",
    "learning_rate=0.1 \n",
    "nkerns=[20, 40] # num of feature maps per conv layer\n",
    "batch_size=500\n",
    "\n",
    "# init var\n",
    "rng = numpy.random.RandomState(23455)\n",
    "x = T.matrix('x')\n",
    "y = T.ivector('y')\n",
    "index = T.lscalar('index')\n",
    "x_batch = x[index * batch_size: (index + 1) * batch_size]\n",
    "y_batch = y[index * batch_size: (index + 1) * batch_size]\n",
    "\n",
    "layer0_input = x_batch.reshape((batch_size, 3, 32, 32)) # cifar data comes in RGB(3) channels in 32x32 size\n",
    "\n",
    "#layer0 output size before pooling = (32 - 5 +2*2) + 1 = 32; after pooling = 32/2 = 16\n",
    "layer0 = LeNetConvPoolLayer(\n",
    "    rng,\n",
    "    input=layer0_input,\n",
    "    image_shape=(batch_size, 3, 32, 32), # apparently, theano conv2d accepts a defined list/tuple as arg only, therefore this arg cannot be simplified in to input.shape (which would be a tensor vector)\n",
    "    filter_shape=(nkerns[0], 3, 5, 5),\n",
    "    poolsize=(2, 2),\n",
    "    border_mode=(2, 2)\n",
    ")\n",
    "\n",
    "# layer1 output size before pooling = (16 - 5 + 2*2) + 1 = 16; after pooling = 16/2 = 8\n",
    "layer1 = LeNetConvPoolLayer(\n",
    "    rng,\n",
    "    input=layer0.output,\n",
    "    image_shape=(batch_size, nkerns[0], 16, 16),\n",
    "    filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "    poolsize=(2, 2),\n",
    "    border_mode=(2, 2)\n",
    ")\n",
    "\n",
    "layer2_input = layer1.output.flatten(2) # theano function that flattens all dims after the first into a single dim, ie. row *(num_feature_maps * width * height)\n",
    "\n",
    "# mlp layer(s) after 2 conv layers\n",
    "layer2 = HiddenLayer(\n",
    "    rng,\n",
    "    input=layer2_input,\n",
    "    n_in=nkerns[1] * 8 * 8,\n",
    "#     n_in=T.shape(layer2_input)[1],\n",
    "    n_out=1000,\n",
    "    p_dropout=0.3,\n",
    "    activation=T.tanh\n",
    ")\n",
    "\n",
    "layer3 = HiddenLayer(\n",
    "    rng,\n",
    "    input=layer2.output,\n",
    "    n_in=1000,\n",
    "    n_out=1000,\n",
    "    p_dropout=0.3,\n",
    "    activation=T.tanh\n",
    ")\n",
    "\n",
    "# uses sigmoidal activation by default\n",
    "log_layer = LogisticRegression(\n",
    "    input=layer3.output, \n",
    "    n_in=1000, \n",
    "    n_out=10\n",
    ")\n",
    "\n",
    "cost = log_layer.negative_log_likelihood(y_batch)\n",
    "errors = log_layer.errors(y_batch)\n",
    "\n",
    "# create a list of all model parameters to be fit by gradient descent\n",
    "params = log_layer.params + layer3.params + layer2.params + layer1.params + layer0.params\n",
    "\n",
    "# create a list of gradients for all model parameters\n",
    "grads = T.grad(cost, params)\n",
    "\n",
    "updates = OrderedDict()\n",
    "for grad, param in zip(grads, params):\n",
    "    # make sure that the learning rate is of the right dtype\n",
    "    updates[param] = param - grad * T.cast(learning_rate, dtype=theano.config.floatX)\n",
    "\n",
    "train_fn = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_set_x,\n",
    "        y: train_set_y\n",
    "    }\n",
    ")\n",
    "\n",
    "def get_errors(X_, y_, batch_size):  \n",
    "    n_batches = int(X_.shape[0] / batch_size)\n",
    "    get_batch_error = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs = errors, # perc of wrong preds\n",
    "        givens={\n",
    "#             batch_size: batch_size,\n",
    "            x: X_,\n",
    "            y: y_\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def score_func():\n",
    "        return [get_batch_error(i) for i in range(n_batches)]\n",
    "\n",
    "    return score_func\n",
    "\n",
    "get_validate_errors = get_errors(valid_set_x, valid_set_y, batch_size)\n",
    "get_test_errors = get_errors(test_set_x, test_set_y, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training the model\n",
      "epoch 0, minibatch 2/2, validation error 88.8%\n",
      "     epoch 0, minibatch minibatch 2/2, test error of best model 88.0%\n",
      "epoch 1, minibatch 2/2, validation error 85.00000000000001%\n",
      "     epoch 1, minibatch minibatch 2/2, test error of best model 83.1%\n",
      "epoch 2, minibatch 2/2, validation error 83.5%\n",
      "     epoch 2, minibatch minibatch 2/2, test error of best model 82.30000000000001%\n",
      "epoch 3, minibatch 2/2, validation error 85.2%\n",
      "epoch 4, minibatch 2/2, validation error 88.5%\n",
      "epoch 5, minibatch 2/2, validation error 78.7%\n",
      "     epoch 5, minibatch minibatch 2/2, test error of best model 76.5%\n",
      "epoch 6, minibatch 2/2, validation error 91.0%\n",
      "epoch 7, minibatch 2/2, validation error 82.30000000000001%\n",
      "epoch 8, minibatch 2/2, validation error 77.8%\n",
      "     epoch 8, minibatch minibatch 2/2, test error of best model 76.8%\n",
      "epoch 9, minibatch 2/2, validation error 85.8%\n",
      "epoch 10, minibatch 2/2, validation error 82.4%\n",
      "epoch 11, minibatch 2/2, validation error 77.8%\n",
      "epoch 12, minibatch 2/2, validation error 88.1%\n",
      "epoch 13, minibatch 2/2, validation error 81.80000000000001%\n",
      "epoch 14, minibatch 2/2, validation error 76.7%\n",
      "     epoch 14, minibatch minibatch 2/2, test error of best model 73.9%\n",
      "epoch 15, minibatch 2/2, validation error 86.1%\n",
      "epoch 16, minibatch 2/2, validation error 77.8%\n",
      "epoch 17, minibatch 2/2, validation error 84.00000000000001%\n",
      "epoch 18, minibatch 2/2, validation error 85.0%\n",
      "epoch 19, minibatch 2/2, validation error 71.10000000000001%\n",
      "     epoch 19, minibatch minibatch 2/2, test error of best model 71.2%\n",
      "epoch 20, minibatch 2/2, validation error 79.7%\n",
      "epoch 21, minibatch 2/2, validation error 86.5%\n",
      "epoch 22, minibatch 2/2, validation error 87.2%\n",
      "epoch 23, minibatch 2/2, validation error 76.1%\n",
      "epoch 24, minibatch 2/2, validation error 79.10000000000001%\n",
      "epoch 25, minibatch 2/2, validation error 74.1%\n",
      "epoch 26, minibatch 2/2, validation error 80.4%\n",
      "epoch 27, minibatch 2/2, validation error 81.1%\n",
      "epoch 28, minibatch 2/2, validation error 79.5%\n",
      "epoch 29, minibatch 2/2, validation error 85.7%\n",
      "epoch 30, minibatch 2/2, validation error 72.8%\n",
      "epoch 31, minibatch 2/2, validation error 71.7%\n",
      "epoch 32, minibatch 2/2, validation error 74.3%\n",
      "epoch 33, minibatch 2/2, validation error 83.60000000000001%\n",
      "epoch 34, minibatch 2/2, validation error 80.7%\n",
      "epoch 35, minibatch 2/2, validation error 79.5%\n",
      "epoch 36, minibatch 2/2, validation error 77.3%\n",
      "epoch 37, minibatch 2/2, validation error 75.3%\n",
      "epoch 38, minibatch 2/2, validation error 76.7%\n",
      "epoch 39, minibatch 2/2, validation error 69.9%\n",
      "     epoch 39, minibatch minibatch 2/2, test error of best model 67.2%\n",
      "epoch 40, minibatch 2/2, validation error 74.7%\n",
      "epoch 41, minibatch 2/2, validation error 73.8%\n",
      "epoch 42, minibatch 2/2, validation error 77.4%\n",
      "epoch 43, minibatch 2/2, validation error 74.5%\n",
      "epoch 44, minibatch 2/2, validation error 69.9%\n",
      "epoch 45, minibatch 2/2, validation error 72.0%\n",
      "epoch 46, minibatch 2/2, validation error 71.0%\n",
      "epoch 47, minibatch 2/2, validation error 87.4%\n",
      "epoch 48, minibatch 2/2, validation error 75.4%\n",
      "epoch 49, minibatch 2/2, validation error 77.60000000000001%\n",
      "epoch 50, minibatch 2/2, validation error 78.10000000000001%\n",
      "epoch 51, minibatch 2/2, validation error 69.30000000000001%\n",
      "     epoch 51, minibatch minibatch 2/2, test error of best model 68.30000000000001%\n",
      "epoch 52, minibatch 2/2, validation error 73.4%\n",
      "epoch 53, minibatch 2/2, validation error 69.5%\n",
      "epoch 54, minibatch 2/2, validation error 75.3%\n",
      "epoch 55, minibatch 2/2, validation error 72.1%\n",
      "epoch 56, minibatch 2/2, validation error 70.9%\n",
      "epoch 57, minibatch 2/2, validation error 72.6%\n",
      "epoch 58, minibatch 2/2, validation error 74.6%\n",
      "epoch 59, minibatch 2/2, validation error 75.1%\n",
      "epoch 60, minibatch 2/2, validation error 74.9%\n",
      "epoch 61, minibatch 2/2, validation error 70.5%\n",
      "epoch 62, minibatch 2/2, validation error 73.0%\n",
      "epoch 63, minibatch 2/2, validation error 72.6%\n",
      "epoch 64, minibatch 2/2, validation error 78.60000000000001%\n",
      "epoch 65, minibatch 2/2, validation error 73.6%\n",
      "epoch 66, minibatch 2/2, validation error 70.30000000000001%\n",
      "epoch 67, minibatch 2/2, validation error 64.9%\n",
      "     epoch 67, minibatch minibatch 2/2, test error of best model 66.0%\n",
      "epoch 68, minibatch 2/2, validation error 74.5%\n",
      "epoch 69, minibatch 2/2, validation error 81.30000000000001%\n",
      "epoch 70, minibatch 2/2, validation error 70.7%\n",
      "epoch 71, minibatch 2/2, validation error 67.80000000000001%\n",
      "epoch 72, minibatch 2/2, validation error 69.30000000000001%\n",
      "epoch 73, minibatch 2/2, validation error 65.2%\n",
      "epoch 74, minibatch 2/2, validation error 71.9%\n",
      "epoch 75, minibatch 2/2, validation error 71.30000000000001%\n",
      "epoch 76, minibatch 2/2, validation error 73.7%\n",
      "epoch 77, minibatch 2/2, validation error 65.7%\n",
      "epoch 78, minibatch 2/2, validation error 65.4%\n",
      "epoch 79, minibatch 2/2, validation error 64.2%\n",
      "     epoch 79, minibatch minibatch 2/2, test error of best model 62.9%\n",
      "epoch 80, minibatch 2/2, validation error 67.30000000000001%\n",
      "epoch 81, minibatch 2/2, validation error 74.2%\n",
      "epoch 82, minibatch 2/2, validation error 64.8%\n",
      "epoch 83, minibatch 2/2, validation error 65.4%\n",
      "epoch 84, minibatch 2/2, validation error 68.10000000000001%\n",
      "epoch 85, minibatch 2/2, validation error 74.4%\n",
      "epoch 86, minibatch 2/2, validation error 64.1%\n",
      "     epoch 86, minibatch minibatch 2/2, test error of best model 65.5%\n",
      "epoch 87, minibatch 2/2, validation error 64.8%\n",
      "epoch 88, minibatch 2/2, validation error 67.7%\n",
      "epoch 89, minibatch 2/2, validation error 65.60000000000001%\n",
      "epoch 90, minibatch 2/2, validation error 68.0%\n",
      "epoch 91, minibatch 2/2, validation error 64.9%\n",
      "epoch 92, minibatch 2/2, validation error 70.5%\n",
      "epoch 93, minibatch 2/2, validation error 73.4%\n",
      "epoch 94, minibatch 2/2, validation error 69.5%\n",
      "epoch 95, minibatch 2/2, validation error 65.5%\n",
      "epoch 96, minibatch 2/2, validation error 66.60000000000001%\n",
      "epoch 97, minibatch 2/2, validation error 64.9%\n",
      "epoch 98, minibatch 2/2, validation error 69.6%\n",
      "epoch 99, minibatch 2/2, validation error 66.0%\n",
      "epoch 100, minibatch 2/2, validation error 76.5%\n",
      "epoch 101, minibatch 2/2, validation error 65.9%\n",
      "epoch 102, minibatch 2/2, validation error 63.9%\n",
      "     epoch 102, minibatch minibatch 2/2, test error of best model 65.10000000000001%\n",
      "epoch 103, minibatch 2/2, validation error 69.60000000000001%\n",
      "epoch 104, minibatch 2/2, validation error 65.3%\n",
      "epoch 105, minibatch 2/2, validation error 65.5%\n",
      "epoch 106, minibatch 2/2, validation error 66.9%\n",
      "epoch 107, minibatch 2/2, validation error 69.2%\n",
      "epoch 108, minibatch 2/2, validation error 65.8%\n",
      "epoch 109, minibatch 2/2, validation error 65.60000000000001%\n",
      "epoch 110, minibatch 2/2, validation error 64.4%\n",
      "epoch 111, minibatch 2/2, validation error 65.3%\n",
      "epoch 112, minibatch 2/2, validation error 66.0%\n",
      "epoch 113, minibatch 2/2, validation error 64.2%\n",
      "epoch 114, minibatch 2/2, validation error 65.7%\n",
      "epoch 115, minibatch 2/2, validation error 63.9%\n",
      "epoch 116, minibatch 2/2, validation error 67.5%\n",
      "epoch 117, minibatch 2/2, validation error 73.2%\n",
      "epoch 118, minibatch 2/2, validation error 67.80000000000001%\n",
      "epoch 119, minibatch 2/2, validation error 70.6%\n",
      "epoch 120, minibatch 2/2, validation error 64.0%\n",
      "epoch 121, minibatch 2/2, validation error 63.7%\n",
      "     epoch 121, minibatch minibatch 2/2, test error of best model 65.4%\n",
      "epoch 122, minibatch 2/2, validation error 64.5%\n",
      "epoch 123, minibatch 2/2, validation error 64.1%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-17b048eb6d3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mbest_validation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-17b048eb6d3e>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "n_train_batches = int(train_set_x.shape[0] / batch_size)\n",
    "training_epochs=1000\n",
    "\n",
    "def train_cnn():\n",
    "    print('... training the model')\n",
    "    \n",
    "    \n",
    "    # early-stopping parameters\n",
    "    patience = 10000 # look as this many examples regardless\n",
    "    patience_increase = 2. # loop for n times more when a new best is found\n",
    "    improvement_threshold = 0.995 # a relative improvement of this much is considered significant\n",
    "\n",
    "    # go through this many minibatches before checking the network on\n",
    "    # the validation set; in this case we check every epoch\n",
    "    validation_frequency = min(n_train_batches, patience / 2) # = n_train_batches\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            train_fn(minibatch_index)\n",
    "            iter = epoch * n_train_batches + minibatch_index\n",
    "\n",
    "            # for every 'validation_frequency' iters\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = get_validate_errors()\n",
    "                curr_mean_validation_loss = numpy.mean(validation_losses, dtype='float64')\n",
    "                print(f'epoch {epoch}, minibatch {minibatch_index + 1}/{n_train_batches}, validation error {curr_mean_validation_loss * 100.}%')\n",
    "\n",
    "                # if we got the least validation errors until now\n",
    "                if curr_mean_validation_loss < best_validation_loss:\n",
    "                    # improve patience if loss improvement is good enough; which will allow more training = double of the curr loop count\n",
    "                    if (curr_mean_validation_loss < best_validation_loss * improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = curr_mean_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = get_test_errors()\n",
    "                    test_score = numpy.mean(test_losses, dtype='float64')\n",
    "                    print(f'     epoch {epoch}, minibatch minibatch {minibatch_index + 1}/{n_train_batches}, test error of best model {test_score * 100.}%')\n",
    "\n",
    "            # if no improvement in validation score for the last 50% iters\n",
    "            if patience <= iter:\n",
    "                return best_validation_loss, best_iter, test_score\n",
    "    return best_validation_loss, best_iter, test_score\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "best_validation_loss, best_iter, test_score = train_cnn()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f'training time: {end_time - start_time}s.')\n",
    "print(f'Optimization complete with best validation score of {best_validation_loss * 100.}%,\\n'\n",
    "    f'obtained at iteration {best_iter + 1},\\n'\n",
    "    f'with test performance {test_score * 100.}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
