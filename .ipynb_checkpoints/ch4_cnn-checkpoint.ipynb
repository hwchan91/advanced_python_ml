{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config theano to use GPU, must be done before theano is imported\n",
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=cuda0,floatX=float32\"#,optimizer=None,exception_verbosity=high\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://deeplearning.net/tutorial/lenet.html\n",
    "    \n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "\n",
    "from theano.tensor.signal import pool\n",
    "from theano.tensor.nnet import conv2d\n",
    "\n",
    "class LeNetConvPoolLayer():\n",
    "    def __init__(self, \n",
    "        rng, \n",
    "        input, # with 4 dimensions: rows of input(i.e. batch) * num of input maps per input * height per map * width per map; see: https://www.quora.com/Why-are-there-4-dimensions-to-convolve-over-the-Stanford-UFDL-example-in-convolutional-neural-networks\n",
    "        image_shape, # tuple/list of len 4 representing the 4 dims of input: (batch size, num input feature maps, image height, image width)\n",
    "        filter_shape, # tuple/list of len 4 representing: (number of filters, num input feature maps, filter height, filter width)\n",
    "        poolsize=(2, 2), # eg. downsample every 2x2 bits to 1 bit\n",
    "        activation=T.nnet.relu,\n",
    "        border_mode='valid', # no padding; see doc for more options\n",
    "        subsample=(1, 1) # unit stride        \n",
    "    ):\n",
    "        assert image_shape[1] == filter_shape[1] # num input feataure maps should be same for both arrs\n",
    "        \n",
    "        self.input = input\n",
    "        fan_in = numpy.prod(filter_shape[1:]) # (num input feature maps * filter height * filter width) input nodes to each hidden node/unit(?) \n",
    "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) / numpy.prod(poolsize)) # (num output feature maps * filter height * filter width /  pooling size) output nodes per layer\n",
    "        \n",
    "        # initialize weights with random weights\n",
    "        W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
    "        self.W = theano.shared(\n",
    "            numpy.asarray(\n",
    "                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX) # a 1D tensor(i.e. array) of len(num of filters) for adding a bias per output feature map\n",
    "        self.b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "        # convolve input feature maps with filters\n",
    "        # doc: http://deeplearning.net/software/theano/library/tensor/nnet/conv.html#theano.tensor.nnet.conv2d\n",
    "        # simply explained; conv2d iterates over every sample, apply the SAME weights on each 'cropped' input(map) to create each output map\n",
    "        # taking an analogy with MLP, each output feature map can be considered as one hidden node in an MLP, where each value in the map represents a different cropped position of the input image\n",
    "        # (programmatically, I believe it flattens the num of maps * input height * input width for every sample; apply the transformtion; and then reshape it back to the expected output dimensions)\n",
    "        conv_out = conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            input_shape=image_shape,\n",
    "            border_mode=border_mode,\n",
    "            subsample=subsample\n",
    "        )\n",
    "\n",
    "        # pool each feature map individually, using maxpooling\n",
    "        # doc: http://deeplearning.net/software/theano/library/tensor/signal/pool.html\n",
    "        # note: 'Max pooling will be done over the 2 last dimensions', i.e. height * width\n",
    "        pooled_out = pool.pool_2d(\n",
    "            input=conv_out,\n",
    "            ws=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "        # add the bias term. Since the bias is a vector (1D array), we reshape it to a tensor of shape (1, n_filters, 1, 1). \n",
    "        # since every output map uses one shared weight, only 1 bias is needed for each output map\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x')) # explanation of dimshuffle: https://stackoverflow.com/questions/42401420/how-the-function-dimshuffle-works-in-theano\n",
    "\n",
    "        # shared_params\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_train():\n",
    "    X = numpy.empty((0,3072), dtype=theano.config.floatX)\n",
    "    y = []\n",
    "    for i in range(5):\n",
    "        i += 1\n",
    "        unpacked = unpickle(f'cifar-10-batches-py/data_batch_{i}')\n",
    "        X = numpy.concatenate((X, unpacked[b'data']))\n",
    "        y = numpy.concatenate((y, unpacked[b'labels']))\n",
    "    return X,y.astype('int32')\n",
    "\n",
    "def load_cifar10_test():\n",
    "    unpacked = unpickle(f'cifar-10-batches-py/test_batch')\n",
    "    return numpy.array(unpacked[b'data'], dtype=theano.config.floatX), numpy.array(unpacked[b'labels'], dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_cifar10_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set_x, valid_set_x, train_set_y, valid_set_y = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_x, test_set_y = load_cifar10_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "train_set_x = train_set_x[0:10000,:]\n",
    "valid_set_x = valid_set_x[0:1000,:]\n",
    "test_set_x = test_set_x[0:1000,:]\n",
    "\n",
    "train_set_y = train_set_y[0:10000]\n",
    "valid_set_y = valid_set_y[0:1000]\n",
    "test_set_y = test_set_y[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(object):\n",
    "    def __init__(\n",
    "        self, \n",
    "        rng, \n",
    "        input, \n",
    "        n_in, \n",
    "        n_out, \n",
    "        p_dropout=0.0, \n",
    "        W=None, \n",
    "        b=None, \n",
    "        activation=T.tanh # if set actication=T.nnet.sigmoid, becomes logistic regresssion layer\n",
    "    ): \n",
    "        self.input = input\n",
    "        self.theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "        \n",
    "        # `W` is initialized with `W_values` which is uniformely sampled\n",
    "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
    "        # for tanh activation function\n",
    "        # the output of uniform if converted using asarray to dtype\n",
    "        # theano.config.floatX so that the code is runable on GPU\n",
    "        # Note : optimal initialization of weights is dependent on the\n",
    "        #        activation function used (among other things).\n",
    "        #        For example, results presented in [Xavier10] suggest that you\n",
    "        #        should use 4 times larger initial weights for sigmoid\n",
    "        #        compared to tanh\n",
    "        #        We have no info for other function, so we use the same as\n",
    "        #        tanh.\n",
    "        if W is None:\n",
    "            W_values = numpy.asarray(\n",
    "                rng.uniform(\n",
    "                    low=-numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    high=numpy.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            if activation == theano.tensor.nnet.sigmoid:\n",
    "                W_values *= 4\n",
    "\n",
    "            W = theano.shared(value=W_values, borrow=True)\n",
    "\n",
    "        if b is None:\n",
    "            b_values = numpy.zeros((n_out,), dtype=theano.config.floatX)\n",
    "            b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        dropout_input = self.get_corrupted_input(input, p_dropout)\n",
    "        lin_output = T.dot(dropout_input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if activation is None\n",
    "            else activation(lin_output)\n",
    "        )\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    # add noise by setting corruption_level% of data to 0s\n",
    "    def get_corrupted_input(self, input, corruption_level):   \n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.input = input\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in, n_out),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # predict_proba\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(self.input, self.W) + self.b) # softmax=normalized sigmoid\n",
    "        # predict\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "    # this is akin to cost = -1/m * sigma(ylog(wx) + (1-y)log(1-wx)) when y is binomial\n",
    "    # in the current case y has n-labels, and only the prediction of the right label is picked out\n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    # perc of wrong predictions\n",
    "    def errors(self, y):\n",
    "        return T.mean(T.neq(self.y_pred, y)) # T.neq(a,b) checks a != b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: output size calculated by formula:\n",
    "# (Width - Filter_size + 2*Padding / Stride) + 1\n",
    "# stride is usually 1 in implementations such as conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "from collections import OrderedDict\n",
    "\n",
    "class CNN():\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng=numpy.random.RandomState(1234),\n",
    "        n_in_maps=3, \n",
    "        in_width=32,\n",
    "        in_height=32,\n",
    "        n_out=10,\n",
    "        conv_layer_sizes=[1000,1000],\n",
    "        conv_filter_sizes=[(5, 5), (5, 5)],\n",
    "        conv_padding_sizes=[(2, 2), (2, 2)],\n",
    "        conv_subsample_sizes=[(1, 1), (1, 1)],\n",
    "        conv_pooling_sizes=[(2, 2), (2, 2)],\n",
    "        fully_connected_layer_sizes=[100],\n",
    "        dropout_levels=[0.3],\n",
    "        batch_size=500\n",
    "    ):        \n",
    "        # init var\n",
    "        self.rng = numpy_rng or numpy.random.RandomState(23455)\n",
    "        self.x = T.matrix('x')\n",
    "        self.y = T.ivector('y')\n",
    "        self.index = T.lscalar('index')\n",
    "        self.x_batch = self.x[self.index * batch_size: (self.index + 1) * batch_size]\n",
    "        self.y_batch = self.y[self.index * batch_size: (self.index + 1) * batch_size]\n",
    "        self.conv_layers = []\n",
    "        self.fully_connected_layers = []\n",
    "        self.params = [] # holds the shared/updatable vars\n",
    "\n",
    "        input_shape = (batch_size, n_in_maps, in_width, in_height)\n",
    "        layer0_input = self.x_batch.reshape(input_shape) # cifar data comes in RGB(3) channels in 32x32 size\n",
    "\n",
    "        # conv layers\n",
    "        out_dims = [] # store (width, height) of each conv layers output\n",
    "        for i in range(len(conv_layer_sizes)):\n",
    "            out_n_maps = conv_layer_sizes[i]\n",
    "            filter_size = conv_filter_sizes[i]\n",
    "            border_mode = conv_padding_sizes[i]\n",
    "            subsample = conv_subsample_sizes[i]\n",
    "            poolsize = conv_pooling_sizes[i]\n",
    "            \n",
    "            if i == 0:\n",
    "                layer_input = layer0_input\n",
    "            else:\n",
    "                # the input is the prev layer\n",
    "                n_in_maps, in_width, in_height = out_dims[i-1]             \n",
    "                layer_input = self.conv_layers[i-1].output # output is the final activation\n",
    "            \n",
    "            out_width  = ((in_width - filter_size[0] + 2 * border_mode[0]) / subsample[0] + 1 ) / poolsize[0]\n",
    "            out_height = ((in_height - filter_size[1] + 2 * border_mode[1]) / subsample[1] + 1) / poolsize[1]\n",
    "            out_dims.append((out_n_maps, out_width, out_height))\n",
    "\n",
    "            conv_layer = LeNetConvPoolLayer(\n",
    "                self.rng,\n",
    "                input=layer_input,\n",
    "                image_shape=(batch_size, n_in_maps, in_width, in_height), # apparently, theano conv2d accepts a defined list/tuple as arg only, therefore this arg cannot be simplified in to input.shape (which would be a tensor vector)\n",
    "                filter_shape=(out_n_maps, n_in_maps, filter_size[0], filter_size[1]),\n",
    "                poolsize=poolsize,\n",
    "                border_mode=border_mode,\n",
    "                subsample=subsample\n",
    "            )\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            self.params.extend(conv_layer.params)\n",
    "        \n",
    "        final_conv_output = self.conv_layers[-1].output.flatten(2)\n",
    "        final_conv_output_features = int(numpy.prod(out_dims[-1]))\n",
    "        \n",
    "        # fully connected layers\n",
    "        for i in range(len(fully_connected_layer_sizes)):\n",
    "            if i == 0:\n",
    "                fully_connected_layer_input = self.conv_layers[-1].output.flatten(2)\n",
    "                fully_connected_layer_n_in = int(numpy.prod(out_dims[-1]))\n",
    "            else:\n",
    "                fully_connected_layer_input = self.fully_connected_layers[i-1].output\n",
    "                fully_connected_layer_n_in = fully_connected_layer_sizes[i-1]\n",
    "\n",
    "            hidden_layer = HiddenLayer(\n",
    "                self.rng,\n",
    "                input=fully_connected_layer_input,\n",
    "                n_in=fully_connected_layer_n_in,\n",
    "                n_out=fully_connected_layer_sizes[i],\n",
    "                p_dropout=dropout_levels[i],\n",
    "                activation=T.tanh\n",
    "            )\n",
    "            self.fully_connected_layers.append(hidden_layer)\n",
    "            self.params.extend(hidden_layer.params)\n",
    "\n",
    "        # sigmoidal LR output layer\n",
    "        log_layer = LogisticRegression(\n",
    "            input=self.fully_connected_layers[-1].output, \n",
    "            n_in=fully_connected_layer_sizes[-1], \n",
    "            n_out=n_out\n",
    "        )\n",
    "        self.params.extend(log_layer.params)\n",
    "    \n",
    "        self.cost = log_layer.negative_log_likelihood(self.y_batch)\n",
    "        self.errors = log_layer.errors(self.y_batch)\n",
    "\n",
    "    def build_train_fn(self, train_x, train_y, learning_rate):\n",
    "        # create a list of gradients for all model parameters\n",
    "        grads = T.grad(self.cost, self.params)\n",
    "        \n",
    "        updates = OrderedDict()\n",
    "        for grad, param in zip(grads, self.params):\n",
    "            # make sure that the learning rate is of the right dtype\n",
    "            updates[param] = param - grad * T.cast(learning_rate, dtype=theano.config.floatX)\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[self.index],\n",
    "            outputs=self.cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_x,\n",
    "                self.y: train_y\n",
    "            }\n",
    "        )\n",
    "        return train_fn\n",
    "\n",
    "    def get_errors(self, test_x, test_y, batch_size):  \n",
    "        n_batches = int(test_x.shape[0] / batch_size)\n",
    "        get_batch_error = theano.function(\n",
    "            inputs=[self.index],\n",
    "            outputs = self.errors, # perc of wrong preds\n",
    "            givens={\n",
    "                self.x: test_x,\n",
    "                self.y: test_y\n",
    "            }\n",
    "        )\n",
    "\n",
    "        def score_func():\n",
    "            return [get_batch_error(i) for i in range(n_batches)]\n",
    "\n",
    "        return score_func\n",
    "\n",
    "cnn = CNN()\n",
    "train_fn = cnn.build_train_fn(train_set_x, train_set_y, 0.1)\n",
    "get_validate_errors = cnn.get_errors(valid_set_x, valid_set_y, 500)\n",
    "get_test_errors = cnn.get_errors(test_set_x, test_set_y, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... training the model\n",
      "epoch 0, minibatch 20/20, validation error 84.7%\n",
      "     epoch 0, minibatch minibatch 20/20, test error of best model 84.8%\n",
      "epoch 1, minibatch 20/20, validation error 82.9%\n",
      "     epoch 1, minibatch minibatch 20/20, test error of best model 83.10000000000001%\n",
      "epoch 2, minibatch 20/20, validation error 78.60000000000001%\n",
      "     epoch 2, minibatch minibatch 20/20, test error of best model 77.9%\n",
      "epoch 3, minibatch 20/20, validation error 70.8%\n",
      "     epoch 3, minibatch minibatch 20/20, test error of best model 71.8%\n",
      "epoch 4, minibatch 20/20, validation error 67.30000000000001%\n",
      "     epoch 4, minibatch minibatch 20/20, test error of best model 65.8%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f3d05b9f2565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mbest_validation_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f3d05b9f2565>\u001b[0m in \u001b[0;36mtrain_cnn\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mminibatch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_train_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_train_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mminibatch_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "batch_size = 500\n",
    "n_train_batches = int(train_set_x.shape[0] / batch_size)\n",
    "training_epochs=1000\n",
    "\n",
    "\n",
    "def train_cnn():\n",
    "    print('... training the model')\n",
    "    \n",
    "    \n",
    "    # early-stopping parameters\n",
    "    patience = 10000 # look as this many examples regardless\n",
    "    patience_increase = 2. # loop for n times more when a new best is found\n",
    "    improvement_threshold = 0.995 # a relative improvement of this much is considered significant\n",
    "\n",
    "    # go through this many minibatches before checking the network on\n",
    "    # the validation set; in this case we check every epoch\n",
    "    validation_frequency = min(n_train_batches, patience / 2) # = n_train_batches\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            train_fn(minibatch_index)\n",
    "            iter = epoch * n_train_batches + minibatch_index\n",
    "\n",
    "            # for every 'validation_frequency' iters\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = get_validate_errors()\n",
    "                curr_mean_validation_loss = numpy.mean(validation_losses, dtype='float64')\n",
    "                print(f'epoch {epoch}, minibatch {minibatch_index + 1}/{n_train_batches}, validation error {curr_mean_validation_loss * 100.}%')\n",
    "\n",
    "                # if we got the least validation errors until now\n",
    "                if curr_mean_validation_loss < best_validation_loss:\n",
    "                    # improve patience if loss improvement is good enough; which will allow more training = double of the curr loop count\n",
    "                    if (curr_mean_validation_loss < best_validation_loss * improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = curr_mean_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = get_test_errors()\n",
    "                    test_score = numpy.mean(test_losses, dtype='float64')\n",
    "                    print(f'     epoch {epoch}, minibatch minibatch {minibatch_index + 1}/{n_train_batches}, test error of best model {test_score * 100.}%')\n",
    "\n",
    "            # if no improvement in validation score for the last 50% iters\n",
    "            if patience <= iter:\n",
    "                return best_validation_loss, best_iter, test_score\n",
    "    return best_validation_loss, best_iter, test_score\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "best_validation_loss, best_iter, test_score = train_cnn()\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f'training time: {end_time - start_time}s.')\n",
    "print(f'Optimization complete with best validation score of {best_validation_loss * 100.}%,\\n'\n",
    "    f'obtained at iteration {best_iter + 1},\\n'\n",
    "    f'with test performance {test_score * 100.}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
