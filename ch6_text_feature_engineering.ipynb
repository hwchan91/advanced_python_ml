{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('testtrolls.csv')\n",
    "training = pd.read_csv('trainingtrolls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Insult</th>\n",
       "      <th>Date</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"THE DRUDGE REPORT\\\\n\\\\n\\\\n\\\\nYou won't see th...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>20120618222256Z</td>\n",
       "      <td>\"@ian21\\xa0\"Roger Clemens is the fucking man, ...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>20120618213617Z</td>\n",
       "      <td>\"Agree with Alan you are an extremest idiot.  ...</td>\n",
       "      <td>PublicTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Really?\\\\n\\\\nI see Marc Lamont Hill on variou...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>20120620003825Z</td>\n",
       "      <td>\"Really suck isn't the word, when many of our ...</td>\n",
       "      <td>PrivateTest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Insult             Date                                            Comment  \\\n",
       "0       0              NaN  \"THE DRUDGE REPORT\\\\n\\\\n\\\\n\\\\nYou won't see th...   \n",
       "1       0  20120618222256Z  \"@ian21\\xa0\"Roger Clemens is the fucking man, ...   \n",
       "2       1  20120618213617Z  \"Agree with Alan you are an extremest idiot.  ...   \n",
       "3       0              NaN  \"Really?\\\\n\\\\nI see Marc Lamont Hill on variou...   \n",
       "4       0  20120620003825Z  \"Really suck isn't the word, when many of our ...   \n",
       "\n",
       "         Usage  \n",
       "0   PublicTest  \n",
       "1   PublicTest  \n",
       "2   PublicTest  \n",
       "3  PrivateTest  \n",
       "4  PrivateTest  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse 'some degree' of html text, like removing quotes, double \\\\ etc.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "training['Comment'] = training['Comment'].apply(lambda text: BeautifulSoup(text, 'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test _EM 123'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "EMAIL_REGEX = r'[\\w\\-\\.\\+]+\\@[a-zA-Z0-9\\.\\-]+\\.[a-zA-z0-9]{2,4}'\n",
    "re.sub(EMAIL_REGEX, '_EM', \"test anc@mail.com 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test _U _U 123'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL_REGEX = r'http(s)?:\\/\\/\\S+' # only works for ones prepended by http(s)://\n",
    "re.sub(URL_REGEX, '_U', \"test http://mail.com https://mail.com 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test '"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format whitespaces, line breaks and quotes\n",
    "\n",
    "line = \" \\\" \\n \\\\n test - _ \\ '\"\n",
    "\n",
    "line = line.replace('\"', ' ')\n",
    "line = line.replace('_', ' ')\n",
    "line = line.replace('-', ' ')\n",
    "line = line.replace('\\n', ' ')\n",
    "line = line.replace('\\\\n', ' ')\n",
    "line = line.replace('\\'', ' ')\n",
    "line = line.replace('\\\\', ' ')\n",
    "line = re.sub(' +',' ', line)\n",
    "\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'... test what _BX\\n you _BQ\\n   _SS\\n test aagh _EL abcdef help _Q\\n me _X\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manage punctuation\n",
    "# ([^!\\?]) matches not ! or ?\n",
    "# (\\Z|[^!\\?]) matches end of line or not (! or ?)\n",
    "# r'\\1 _BQ\\n\\3' replaces the middle matching part with _BQ\n",
    "line = \"... test what?!? you????  ... test aaaaaaagh abc.def help? me!\"\n",
    "\n",
    "line = re.sub(r'([^!\\?])(\\?{2,})(\\Z|[^!\\?])', r'\\1 _BQ\\n\\3', line) # replace what??? to what _BQ\\n\n",
    "line = re.sub(r'([^\\.])(\\.{2,})', r'\\1 _SS\\n', line) # replace  uh... to uh _SS\\n\n",
    "line = re.sub(r'([^!\\?])(\\?|!){2,}(\\Z|[^!\\?])', r'\\1 _BX\\n\\3', line) # replace what!? to what _BX\\n \n",
    "line = re.sub(r'([^!\\?])\\?(\\Z|[^!\\?])', r'\\1 _Q\\n\\2', line) # replace is it? with is it _Q\\n\n",
    "line = re.sub(r'([^!\\?])!(\\Z|[^!\\?])', r'\\1 _X\\n\\2', line) # replace great! with great _X\\n\n",
    "line = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2 _EL', line) # replace aaaaaaagh(at least 3 repeating letters) to aagh _EL\n",
    "line = re.sub(r'(\\w+)\\.(\\w+)', r'\\1\\2', line) # replace abc.def with abcdef\n",
    "\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey #%**&$ _SW you'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hey #%**&$ you\"\n",
    "\n",
    "# swearing\n",
    "text = re.sub(r'([#%&\\*\\$]{2,})(\\w*)', r'\\1\\2 _SW', text) # add _SW after sequence of #%$&* (assuming ! and ? are already removed, and not including @)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi _BS _S  _S _BF _F'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hi x-}} => <3 =(( :(\"\n",
    "\n",
    "# emotes\n",
    "# note: (?:xxxx){n,} meaning any of which with count n+\n",
    "# head: any of 8 x ; : =\n",
    "# nose(optional): -\n",
    "# mouth(happy): any of ) ] } >\n",
    "# mouth(sad):   any of ( [ | \\ / { < \n",
    "\n",
    "text = re.sub(r' [8x;:=]-?(?:\\)|\\}|\\]|>){2,}', r' _BS', text) # with 2+ mouth symbols\n",
    "text = re.sub(r' ([8x;:=]-?[\\)\\}\\]|>])|(?:<3)', r' _S', text) # with 1 mouth OR heart symbol, ie.<3\n",
    "text = re.sub(r' [8x;:=]-?(?:\\(|\\[|\\||\\\\|/|\\{|<){2,}', r' _BF', text) # wih 2+ mouth symbols\n",
    "text = re.sub(r' [8x;:=]-?[\\(\\[\\(|\\[|\\||\\\\|/|\\{|<]', r' _F', text) # with 1 mouth\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is '"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove number and percentages\n",
    "line = \"it is 100%\"\n",
    "\n",
    "line = re.sub('[1|2|3|4|5|6|7|8|9|0]', '', line)\n",
    "line = re.sub('[%]', '', line)\n",
    "\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi'],\n",
       " ['welcome'],\n",
       " ['hello'],\n",
       " ['See', 'this'],\n",
       " ['amazing', 'sentence'],\n",
       " ['don', 't', 'you', 'agree'],\n",
       " ['100%', 'wholesome&healthy']]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into phrases and words\n",
    "line = \"Hi; welcome \\n hello. See this: amazing sentence (don't you agree) 100% wholesome&healthy! .  . \"\n",
    "\n",
    "phrases = re.split(r'[;:\\.()\\n]', line)\n",
    "phrases = [re.findall(r'[\\w%\\*&#]+', ph) for ph in phrases] # select words (words may include symbols except ! and ?)\n",
    "phrases = [ph for ph in phrases if ph] # remove empty arrays inside phrases; note: empty array is falsey\n",
    "\n",
    "phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'welcome',\n",
       " 'hello',\n",
       " 'See',\n",
       " 'this',\n",
       " 'amazing',\n",
       " 'sentence',\n",
       " 'don',\n",
       " 't',\n",
       " 'you',\n",
       " 'agree',\n",
       " '100%',\n",
       " 'wholesome&healthy']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten phrases into a single list\n",
    "\n",
    "words = []\n",
    "[words.extend(ph) for ph in phrases]\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stringing consecutive single letters together\n",
    "words = [\"h\", \"e\", \"l\", \"l\", \"o\", \"world\"]\n",
    "\n",
    "tmp = words\n",
    "words = []\n",
    "new_word = ''\n",
    "for word in tmp:\n",
    "    if len(word) == 1: # keep adding consecutive single letters to new_word until the next word is not a single letter\n",
    "        new_word = new_word + word\n",
    "    else:\n",
    "        if new_word:\n",
    "            words.append(new_word)\n",
    "            new_word = ''\n",
    "        words.append(word)\n",
    "        \n",
    "words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rain', 'Madrid', 'Spain', 'makes', 'elated', 'boy']"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "words = re.findall(r'[\\w%\\*&#]+', \"the rain in Madrid of Spain makes him a elated boy\")\n",
    "\n",
    "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram tagger of word's part of speech\n",
    "import nltk\n",
    "from nltk import NgramTagger\n",
    "# nltk.download('brown')\n",
    "\n",
    "# Backoff tagging\n",
    "brown_a = nltk.corpus.brown.tagged_sents()\n",
    "tagger = None\n",
    "# backoff sets the fallback if fails to tag; \n",
    "# since this is inside a loop, \n",
    "# the 4-gram will fall back on the 3-gram; \n",
    "# 3 on 2 and so on, until 1-gram will fallback to None\n",
    "for n in range(1,4):\n",
    "    tagger = NgramTagger(n, brown_a, backoff = tagger) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('silly', 'JJ'),\n",
       " ('dogs', 'NNS'),\n",
       " ('happily', 'RB'),\n",
       " ('jumped', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'AT'),\n",
       " ('reddish', 'JJ'),\n",
       " ('foxes', None)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['silly', 'dogs', 'happily', 'jumped', 'over', 'the', 'reddish', 'foxes']\n",
    "words = tagger.tag(words)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('silly', 'r'),\n",
       " ('dogs', 'n'),\n",
       " ('happily', 'r'),\n",
       " ('jumped', 'v'),\n",
       " ('over', None),\n",
       " ('the', None),\n",
       " ('reddish', 'a'),\n",
       " ('foxes', 'n')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "words = ['silly', 'dogs', 'happily', 'jumped', 'over', 'the', 'reddish', 'foxes']\n",
    "\n",
    "words = pos_tag(words)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag == None:\n",
    "        return None\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "words = list(map(lambda word_pos: (word_pos[0], get_wordnet_pos(word_pos[1])), words))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['silli', 'dog', 'jump', 'over', 'the', 'reddish', 'fox']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = ['silly', 'dogs', 'jumped', 'over', 'the', 'reddish', 'foxes']\n",
    "list(map(lambda word: stemmer.stem(word), words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['silly', 'dog', 'happily', 'jump', 'over', 'the', 'reddish', 'fox']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = []\n",
    "for word, pos in words: \n",
    "    if pos:\n",
    "        word = lemmatizer.lemmatize(word, pos = pos)\n",
    "    lemmatized.append(word)\n",
    "        \n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# together\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_line(line):\n",
    "#     line = str(BeautifulSoup(str(line), \"html.parser\"))\n",
    "    line = BeautifulSoup(line).get_text()\n",
    "    line = line.lower()\n",
    "\n",
    "    EMAIL_REGEX = r'[\\w\\-\\.\\+]+\\@[a-zA-Z0-9\\.\\-]+\\.[a-zA-z0-9]{2,4}'\n",
    "    line = re.sub(EMAIL_REGEX, '_EM', line)\n",
    "\n",
    "    URL_REGEX = r'http(s)?:\\/\\/\\S+' # only works for ones prepended by http(s)://\n",
    "    line = re.sub(URL_REGEX, '_U', line)\n",
    "\n",
    "    # format whitespaces, line breaks and quotes\n",
    "    line = line.replace('\"', ' ')\n",
    "    line = line.replace('_', ' ')\n",
    "    line = line.replace('-', ' ')\n",
    "    line = line.replace('\\n', ' ')\n",
    "    line = line.replace('\\\\n', ' ')\n",
    "    line = line.replace('\\'', ' ')\n",
    "    line = re.sub(' +',' ', line)\n",
    "    \n",
    "    print(line)\n",
    "    # manage punctuation\n",
    "    line = re.sub(r'([^!\\?])(\\?{2,})(\\Z|[^!\\?])', r'\\1 _BQ\\n\\3', line) # replace what??? to what _BQ\\n\n",
    "    line = re.sub(r'([^\\.])(\\.{2,})', r'\\1 _SS\\n', line) # replace  uh... to uh _SS\\n\n",
    "    line = re.sub(r'([^!\\?])(\\?|!){2,}(\\Z|[^!\\?])', r'\\1 _BX\\n\\3', line) # replace what!? to what _BX\\n \n",
    "    line = re.sub(r'([^!\\?])\\?(\\Z|[^!\\?])', r'\\1 _Q\\n\\2', line) # replace is it? with is it _Q\\n\n",
    "    line = re.sub(r'([^!\\?])!(\\Z|[^!\\?])', r'\\1 _X\\n\\2', line) # replace great! with great _X\\n\n",
    "    line = re.sub(r'([a-zA-Z])\\1\\1+(\\w*)', r'\\1\\1\\2 _EL', line) # replace aaaaaaagh(at least 3 repeating letters) to aagh _EL\n",
    "    line = re.sub(r'(\\w+)\\.(\\w+)', r'\\1\\2', line) # replace abc.def with abcdef\n",
    "\n",
    "    # swearing\n",
    "    line = re.sub(r'([#%&\\*\\$]{2,})(\\w*)', r'\\1\\2 _SW', line) # add _SW after sequence of #%$&* (assuming ! and ? are already removed, and not including @)\n",
    "\n",
    "    # emotes\n",
    "    # note: (?:xxxx){n,} meaning any of which with count n+\n",
    "    # head: any of 8 x ; : =\n",
    "    # nose(optional): -\n",
    "    # mouth(happy): any of ) ] } >\n",
    "    # mouth(sad):   any of ( [ | \\ / { < \n",
    "    line = re.sub(r' [8x;:=]-?(?:\\)|\\}|\\]|>){2,}', r' _BS', line) # with 2+ mouth symbols\n",
    "    line = re.sub(r' ([8x;:=]-?[\\)\\}\\]|>])|(?:<3)', r' _S', line) # with 1 mouth OR heart symbol, ie.<3\n",
    "    line = re.sub(r' [8x;:=]-?(?:\\(|\\[|\\||\\\\|/|\\{|<){2,}', r' _BF', line) # wih 2+ mouth symbols\n",
    "    line = re.sub(r' [8x;:=]-?[\\(\\[\\(|\\[|\\||\\\\|/|\\{|<]', r' _F', line) # with 1 mouth\n",
    "\n",
    "    # remove number and percentages; and '\\'\n",
    "    line = re.sub('[1|2|3|4|5|6|7|8|9|0]', '', line)\n",
    "    line = re.sub('[%]', '', line)\n",
    "    line = line.replace('\\\\', ' ')\n",
    "\n",
    "    # split into phrases and words\n",
    "    phrases = re.split(r'[;:\\.()\\n]', line)\n",
    "    phrases = [re.findall(r'[\\w%\\*&#]+', ph) for ph in phrases] # select words (words may include symbols except ! and ?)\n",
    "    phrases = [ph for ph in phrases if ph] # remove empty arrays inside phrases; note: empty array is falsey\n",
    "\n",
    "    # flatten phrases into a single list\n",
    "    words = []\n",
    "    [words.extend(ph) for ph in phrases]\n",
    "\n",
    "    # stringing consecutive single letters together\n",
    "    tmp = words\n",
    "    words = []\n",
    "    new_word = ''\n",
    "    for word in tmp:\n",
    "        if len(word) == 1: # keep adding consecutive single letters to new_word until the next word is not a single letter\n",
    "            new_word = new_word + word\n",
    "        else:\n",
    "            if new_word:\n",
    "                words.append(new_word)\n",
    "                new_word = ''\n",
    "            words.append(word)\n",
    "\n",
    "    # remove common words, defined in stopwords by NLTK\n",
    "    words = [w for w in words if not w in stopwords.words(\"english\")]\n",
    "    \n",
    "    # tag part-of-speech\n",
    "    words = pos_tag(words)\n",
    "    words = list(map(lambda word_pos: (word_pos[0], get_wordnet_pos(word_pos[1])), words))\n",
    "\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = []\n",
    "    for word, pos in words: \n",
    "        if pos:\n",
    "            word = lemmatizer.lemmatize(word, pos = pos)\n",
    "        lemmatized.append(word)\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag == None:\n",
    "        return None\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i am 100% sure that this is a scam!!!! ;( \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sure', 'scam', '_BX', '_F']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_line(\"'<b>I am 100% sure that this is a scam!!!!</b> ;('\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the dogs lazily lay on the green fields :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dog', 'lazily', 'lay', 'green', 'field', '_S']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_line(\"the dogs lazily lay on the green fields :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = training['Comment'].apply(clean_line)\n",
    "test_x = test['Comment'].apply(clean_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [drudge, report, see, story, foxfag, forum, su...\n",
      "1    [ian, xa, roger, clemens, fuck, man, never, fu...\n",
      "2    [agree, alan, extreme, idiot, american, native...\n",
      "3    [really, _Q, see, marc, lamont, hill, various,...\n",
      "4    [really, suck, word, many, nuclear, power, pla...\n",
      "Name: Comment, dtype: object 0    [gallup, daily, may, u, update, daily, pm, et,...\n",
      "1    [someone, whose, self, importance, get, best, ...\n",
      "2                                       [stand, porch]\n",
      "3             [camp, get, come, get, guy, earn, money]\n",
      "4    [could, wrong, american, tea, party, stand, li...\n",
      "Name: Comment, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0:5], test_x[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# stop words are already removed\n",
    "# swds = stopwords.words('english')\n",
    "vect = TfidfVectorizer(\n",
    "    analyzer = \"word\",\n",
    "    ngram_range = (1,3), \n",
    "    min_df = 0, \n",
    "    stop_words = None, #swds, \n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "docs_new = [\" \".join(x) for x in train_x]\n",
    "tf = vect.fit_transform(docs_new).toarray() # create bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('train_x.csv', tf, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = [\" \".join(x) for x in test_x]\n",
    "tf = vect.fit_transform(docs_new).toarray() # create bag of words model\n",
    "np.savetxt('test_x.csv', tf, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.genfromtxt('train_x.csv', delimiter = ',')\n",
    "test_x = np.genfromtxt('test_x.csv', delimiter = ',')\n",
    "# for y \n",
    "training = pd.read_csv('trainingtrolls.csv')\n",
    "test = pd.read_csv('testtrolls.csv')\n",
    "train_y = training[\"Insult\"]\n",
    "test_y = test[\"Insult\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators = 2000, max_depth = 5, max_features = 1000)\n",
    "\n",
    "rf.fit(train_x, train_y)\n",
    "y_submission = rf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_submission_norm = (y_submission - y_submission.min())/(y_submission.max() - y_submission.min())\n",
    "y_submission_bool = (y_submission >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest benchmark AUC, 1000 estimators\n",
      "0.7737747391969321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_submission_bool, test_y)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Random Forest benchmark AUC, 1000 estimators\")\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Precision: 0.926\n",
      "Recall: 0.254\n",
      "F1: 0.399\n",
      "\n",
      "\n",
      "Testing\n",
      "Precision: 0.803\n",
      "Recall: 0.057\n",
      "F1: 0.106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_test = train_y\n",
    "y_pred = (rf.predict(train_x) >= 0.5).astype(int)\n",
    "print('Training')\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "y_test = test_y\n",
    "y_pred = (rf.predict(test_x) >= 0.5).astype(int)\n",
    "print('Testing')\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest benchmark score, 1000 estimators\n",
      "0.012422375995523207\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest benchmark score, 1000 estimators\")\n",
    "print(rf.score(test_x, test_y))\n",
    "# very bad correlation, most likely because of the low recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using SVC as classifier\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(C=1000, max_iter=50000)\n",
    "clf.fit(train_x, train_y)\n",
    "y_submission = clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest benchmark AUC, 1000 estimators\n",
      "0.5134523561892516\n",
      "Training\n",
      "Precision: 0.926\n",
      "Recall: 0.254\n",
      "F1: 0.399\n",
      "\n",
      "\n",
      "Testing\n",
      "Precision: 0.803\n",
      "Recall: 0.057\n",
      "F1: 0.106\n",
      "Random Forest benchmark score, 1000 estimators\n",
      "0.012422375995523207\n"
     ]
    }
   ],
   "source": [
    "# y_submission_norm = (y_submission - y_submission.min())/(y_submission.max() - y_submission.min())\n",
    "y_submission_bool = (y_submission >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, _ = roc_curve(y_submission_bool, test_y)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Random Forest benchmark AUC, 1000 estimators\")\n",
    "print(roc_auc)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_test = train_y\n",
    "y_pred = (rf.predict(train_x) >= 0.5).astype(int)\n",
    "print('Training')\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "y_test = test_y\n",
    "y_pred = (rf.predict(test_x) >= 0.5).astype(int)\n",
    "print('Testing')\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "print('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "print(\"Random Forest benchmark score, 1000 estimators\")\n",
    "print(rf.score(test_x, test_y))\n",
    "# very bad correlation, most likely because of the low recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
